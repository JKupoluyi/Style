{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcusdaly/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/marcusdaly/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/marcusdaly/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/marcusdaly/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/marcusdaly/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/marcusdaly/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/marcusdaly/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/marcusdaly/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/marcusdaly/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/marcusdaly/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/marcusdaly/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/marcusdaly/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Dense, LSTM, Activation, Bidirectional, Dropout, Input, concatenate\n",
    "from keras.models import Sequential\n",
    "from keras import Model\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_dims = 50 # len(n)\n",
    "z_dims = 50 # len(Z)\n",
    "total_latent_dims = n_dims+z_dims # len(N) + len(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squareError(xTrue, xPred):\n",
    "    return K.square(xTrue - xPred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstructionLoss(sample, encoder, decoder, f_w, weight): # (L_1 from the paper)\n",
    "    return K.mean(squareError(sample, decoder(tf.concat(encoder(sample), axis=1))) + \n",
    "                  weight*K.log(f_w(tf.concat(encoder(sample), axis=1))), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divergenceLoss(f_w, encoder, sample, z_j, n_j): # Mean of log f_w(E_theta_i(x_j)) + log (1-f_w(z_j, n_j)) from the paper (L_2).\n",
    "    return K.mean(K.log(f_w(tf.concat(encoder(sample), axis=1))) + \n",
    "                  K.log(1 - f_w(tf.concat([z_j, n_j], axis=1))), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently just doing a restriction to the last z variables, might want to do a matrix multiplication?\n",
    "# pi_Z from the paper. projects a latent distribution in (z, n) to z\n",
    "def projectZ(encoded):\n",
    "    return encoded[0] # take zs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in two inputs, n and z, and outputs samples.\n",
    "def createDecoder(z_dims, n_dims, output_dims):\n",
    "    # TODO MAYBE: Add in more regularization or different than dropout?\n",
    "\n",
    "    z_inputs = Input(shape=(z_dims))\n",
    "    n_inputs = Input(shape=(n_dims))\n",
    "    inputs = concatenate([z_inputs, n_inputs])\n",
    "    # 150 is arbitrary rn...\n",
    "    dense = Dense(150)(inputs)\n",
    "    bilstm = Bidirectional(LSTM(32, activation='tanh', return_sequences=True))(dense)\n",
    "    bilstm = Dropout(0.2)(bilstm)\n",
    "    outputs = Bidirectional(LSTM(32, activation='tanh', dropout=0.2, return_sequences=True))(inputs)\n",
    "    \n",
    "    model = Model(inputs=[z_inputs, n_inputs], outputs=outputs)\n",
    "    \n",
    "    model.compile()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEncoder(input_num, z_dims, n_dims):\n",
    "    # TODO MAYBE: Add in more regularization or different than dropout?\n",
    "    inputs = Input(shape=(time_steps, features))\n",
    "    bilstm = Bidirectional(LSTM(32, activation='tanh', return_sequences=True))(bilstm)\n",
    "    bilstm = Dropout(0.2)(bilstm)\n",
    "    dense = Bidirectional(LSTM(32, activation='tanh', dropout=0.2, return_sequences=False))(bilstm)\n",
    "    dense = Dropout(0.5)(dense)\n",
    "    z_output = Dense(z_dims)(dense)\n",
    "    n_output = Dense(n_dims)(dense)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=[z_output, n_output])\n",
    "    \n",
    "    model.compile()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDiscriminator(z_dims, n_dims):\n",
    "    z_inputs = Input(shape=(z_dims))\n",
    "    n_inputs = Input(shape=(n_dims))\n",
    "    inputs = concatenate([z_inputs, n_inputs])\n",
    "    \n",
    "    # 150, 100 is arbitrary rn...\n",
    "    dense = Dense(150)(inputs)\n",
    "    dense = Dense(100)(dense)\n",
    "    output = Dense(1)\n",
    "    \n",
    "    model = Model(inputs=[z_inputs, n_inputs], outputs=output)\n",
    "    \n",
    "    model.compile()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_optimizer = tf.keras.optimizers.Adam(5e-4)\n",
    "dec_optimizer = tf.keras.optimizers.Adam(5e-4)\n",
    "disc_optimizer = tf.keras.optimizers.Adam(5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When $P_Z$ is known... (Writing this out for clarityâ€”will not be used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# k is num of domains.\n",
    "# encoders is a list of encoders.\n",
    "# decoders is list of decoders.\n",
    "# samples is a K x N array of samples, where the first index is the domain,\n",
    "# the second index is the # of the sample in that domain.\n",
    "\n",
    "# TODO IMPORTANT: Currently assuming P_Z is known, but it is NOT. Must alter algorithm as in (3.2) to support unknown P_Z.\n",
    "def trainAutoencoders(k, encoders, decoders, samples, discriminator, weight=1.0):\n",
    "    N = samples.shape[0]\n",
    "    \n",
    "    for i in range(k):\n",
    "        encoder = encoders[i]\n",
    "        decoder = decoders[i]\n",
    "        while(not isConverged(encoder, decoder)):\n",
    "            p_Xi_samples = samples[i,:]\n",
    "            p_Z_samples = projectZ(encoder(samples[]))\n",
    "            p_Ni_samples = None # TODO Something!\n",
    "            \n",
    "            with tf.GradientTape() as enc_tape, tf.GradientTape() as dec_tape, tf.GradientTape() as disc_tape:\n",
    "                \n",
    "                reconstruction_loss = reconstructionLoss(p_Xi_samples, encoder, decoder, f_w, weight)\n",
    "                \n",
    "                # negative b/c gradient ascent.\n",
    "                divergence_loss = -divergenceLoss(f_w, encoder, p_Xi_samples, p_Z_samples, p_Ni_samples)\n",
    "                \n",
    "            gradients_of_encoder = enc_tape.gradient(reconstruction_loss, encoder.trainable_variables)\n",
    "            gradients_of_decoder = dec_tape.gradient(reconstruction_loss, decoder.trainable_variables)\n",
    "            gradients_of_discriminator = disc_tape.gradient(divergence_loss, discriminator.trainable_variables)\n",
    "            \n",
    "\n",
    "            enc_optimizer.apply_gradients(zip(gradients_of_encoder, encoder.trainable_variables))\n",
    "            dec_optimizer.apply_gradients(zip(gradients_of_decoder, decoder.trainable_variables))\n",
    "            disc_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When $P_Z$ is unknown...\n",
    "\"A straight-forward approach for learning the latent distribution PZ is to train a regularized autoencoder on data from a\n",
    "single representative domain. However, such a representation could potentially capture variability that is specific to\n",
    "that one domain. To learn a more invariant latent representation, we propose the following extension of our autoencoder\n",
    "framework. The basic idea is to alternate between training\n",
    "multiple autoencoders until they agree on a latent representation that is effective for their respective domains. This is\n",
    "particularly relevant for applications to biology; for example, often one is interested in learning a latent representation\n",
    "that integrates all of the data modalities.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# k is num of domains.\n",
    "# encoders is a list of encoders.\n",
    "# decoders is list of decoders.\n",
    "# samples is a K x N array of samples, where the first index is the domain,\n",
    "# the second index is the # of the sample in that domain.\n",
    "\n",
    "# TODO IMPORTANT: Currently assuming P_Z is known, but it is NOT. Must alter algorithm as in (3.2) to support unknown P_Z.\n",
    "def trainAutoencoders(k, p_Z_hats, encoders, decoders, samples, discriminator, weight=1.0):\n",
    "    N = samples.shape[0]\n",
    "    \n",
    "    for i in range(k):\n",
    "        encoder = encoders[i]\n",
    "        decoder = decoders[i]\n",
    "        p_Z_hat = p_Z_hats[i]\n",
    "        while(not isConverged(encoder, decoder)):\n",
    "            p_Xi_samples = samples[i,:]\n",
    "            p_Zi_samples = projectZ(encoder(samples[i,:]))\n",
    "            p_Ni_samples = None # TODO Something!\n",
    "            \n",
    "            with tf.GradientTape() as enc_tape, tf.GradientTape() as dec_tape, tf.GradientTape() as disc_tape:\n",
    "                \n",
    "                reconstruction_loss = reconstructionLoss(p_Xi_samples, encoder, decoder, f_w, weight)\n",
    "                \n",
    "                # negative b/c gradient ascent.\n",
    "                divergence_loss = -divergenceLoss(f_w, encoder, p_Xi_samples, p_Z_samples, p_Ni_samples)\n",
    "                \n",
    "            gradients_of_encoder = enc_tape.gradient(reconstruction_loss, encoder.trainable_variables)\n",
    "            gradients_of_decoder = dec_tape.gradient(reconstruction_loss, decoder.trainable_variables)\n",
    "            gradients_of_discriminator = disc_tape.gradient(divergence_loss, discriminator.trainable_variables)\n",
    "            \n",
    "\n",
    "            enc_optimizer.apply_gradients(zip(gradients_of_encoder, encoder.trainable_variables))\n",
    "            dec_optimizer.apply_gradients(zip(gradients_of_decoder, decoder.trainable_variables))\n",
    "            disc_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateOnArticles(articles, encoder, decoder, evaluator):\n",
    "    translated = decoder(encoder(articles))\n",
    "    scores = evaluator.get_scores(translated, original)\n",
    "    \n",
    "    for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
    "        print('\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * results['p'], 'R', 100.0 * results['r'], 'F1', 100.0 * results['f']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(articles, encoders, decoders):\n",
    "    evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
    "                        max_n=4,\n",
    "                        limit_length=True,\n",
    "                        length_limit=100,\n",
    "                        length_limit_type='words',\n",
    "                        apply_avg=False,\n",
    "                        apply_best=True,\n",
    "                        alpha=0.5, # Default F1_score\n",
    "                        weight_factor=1.2,\n",
    "                        stemming=True)\n",
    "    \n",
    "    #TODO Decide which articles are being tested with which styles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
