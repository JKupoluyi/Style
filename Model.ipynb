{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import pandas\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "import rouge\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LSTM, Activation, Bidirectional, Dropout, Input, concatenate, Reshape, TimeDistributed, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow.keras.backend as K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>publication</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[washington, —, congressional, republicans, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[after, the, bullet, shells, get, the, south, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[when, walt, disney, ’, s, but, what, they, di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[death, may, be, the, great, equalizer, ,, but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[seoul, ,, south, korea, —, although, north, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47220</th>\n",
       "      <td>47220</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[bt, is, introducing, two, initiatives, from, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47221</th>\n",
       "      <td>47221</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[computer, users, across, the, world, more, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47222</th>\n",
       "      <td>47222</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[a, new, european, directive, could, if, it, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47223</th>\n",
       "      <td>47223</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[the, man, making, sure, us, amit, yoran, was,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47224</th>\n",
       "      <td>47224</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[online, role, playing, games, are, time-consu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47225 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0     publication  \\\n",
       "0               0  New York Times   \n",
       "1               1  New York Times   \n",
       "2               2  New York Times   \n",
       "3               3  New York Times   \n",
       "4               4  New York Times   \n",
       "...           ...             ...   \n",
       "47220       47220        BBC_tech   \n",
       "47221       47221        BBC_tech   \n",
       "47222       47222        BBC_tech   \n",
       "47223       47223        BBC_tech   \n",
       "47224       47224        BBC_tech   \n",
       "\n",
       "                                                 content  \n",
       "0      [washington, —, congressional, republicans, ha...  \n",
       "1      [after, the, bullet, shells, get, the, south, ...  \n",
       "2      [when, walt, disney, ’, s, but, what, they, di...  \n",
       "3      [death, may, be, the, great, equalizer, ,, but...  \n",
       "4      [seoul, ,, south, korea, —, although, north, k...  \n",
       "...                                                  ...  \n",
       "47220  [bt, is, introducing, two, initiatives, from, ...  \n",
       "47221  [computer, users, across, the, world, more, th...  \n",
       "47222  [a, new, european, directive, could, if, it, g...  \n",
       "47223  [the, man, making, sure, us, amit, yoran, was,...  \n",
       "47224  [online, role, playing, games, are, time-consu...  \n",
       "\n",
       "[47225 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('data/tokenized.pkl')\n",
    "eval_df = pd.read_pickle('data/evaluation.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = list(data['content'])\n",
    "all_sentences.extend(list(eval_df['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Relevant publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_publications = [\n",
    "#  'Breitbart',\n",
    "#  'CNN',\n",
    "#  'New York Times',\n",
    "#  'NPR',\n",
    "#  'Fox News',\n",
    "#  'Reuters']\n",
    "selected_publications = [\n",
    " 'Breitbart',\n",
    " 'CNN',\n",
    " 'New York Times']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Atlantic',\n",
       " 'New York Times',\n",
       " 'Vox',\n",
       " 'Reuters',\n",
       " 'Buzzfeed News',\n",
       " 'Business Insider',\n",
       " 'Talking Points Memo',\n",
       " 'New York Post',\n",
       " 'Washington Post',\n",
       " 'BBC_business',\n",
       " 'BBC_politics',\n",
       " 'BBC_tech',\n",
       " 'BBC_sport',\n",
       " 'Breitbart',\n",
       " 'Fox News',\n",
       " 'Guardian',\n",
       " 'NPR',\n",
       " 'CNN',\n",
       " 'National Review',\n",
       " 'BBC_entertainment']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_publications = list(set(data['publication']))\n",
    "all_publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Breitbart', 'CNN', 'New York Times']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take only the contents from publications with >= 3000 samples.\n",
    "publications = [pub for pub in selected_publications if pub in all_publications and len(data[data['publication'] == pub]) >= 3000]\n",
    "publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = []\n",
    "for pub in publications:\n",
    "    contents.append(np.asarray(data[data['publication'] == pub]['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding with special Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = max([len(seq) for content in contents for seq in content])\n",
    "max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_token = '~?@_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for content in contents:\n",
    "    for seq in content:\n",
    "        seq.extend([end_token] * (max_seq_length - len(seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = max([len(seq) for content in contents for seq in content])\n",
    "max_seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word2vec = gensim.models.Word2Vec(all_sentences, min_count = 1,  \n",
    "                              size = word_dim, window = 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'congress' and 'senate' - CBOW :  0.7559486\n",
      "Cosine similarity between 'congress' and 'house' - CBOW :  0.6252873\n"
     ]
    }
   ],
   "source": [
    "print(\"Cosine similarity between 'congress' \" + \n",
    "               \"and 'senate' - CBOW : \", \n",
    "    word2vec.wv.similarity('congress', 'senate')) \n",
    "      \n",
    "print(\"Cosine similarity between 'congress' \" +\n",
    "                 \"and 'house' - CBOW : \", \n",
    "    word2vec.wv.similarity('congress', 'house')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.29592454, -0.8773146 ,  0.76711816, -0.05720106, -0.03461413,\n",
       "       -0.40311787,  0.22946852,  1.0140978 ,  0.8975274 , -0.7267992 ,\n",
       "        1.2324462 , -0.43481615,  0.8528818 , -0.7095517 , -0.07889917,\n",
       "       -0.52104896,  0.2608844 , -0.58594555, -0.59863317, -0.31064552,\n",
       "       -0.11377191, -0.50565964,  0.15798195,  0.26478073,  1.4427081 ,\n",
       "        0.55394846,  0.17723931,  0.7052398 , -0.88249487, -0.5539005 ,\n",
       "        0.2363067 ,  0.06038764, -0.86185366, -0.19701053,  0.14048052,\n",
       "       -0.45728892,  0.2533323 , -0.26327336, -0.8601514 ,  1.0089282 ,\n",
       "        0.14991663, -0.44243005, -1.3007863 , -1.164767  , -1.2460694 ,\n",
       "        0.24592596, -0.501519  , -0.23789279,  0.24771175,  0.13797134,\n",
       "       -0.20122194,  1.2545592 ,  1.3626865 , -0.5414245 ,  0.9415389 ,\n",
       "       -0.27835897, -0.4958755 , -0.01836712,  0.8055421 , -0.56852084,\n",
       "       -0.9491626 , -1.0129623 , -0.07664649, -0.00478025, -1.3793099 ,\n",
       "        0.02242324, -0.07649595, -0.25851354,  0.30401504,  0.07324145,\n",
       "        0.53251505,  0.6323623 ,  0.02162529,  0.14522254, -0.9740372 ,\n",
       "       -0.41615644, -0.21245232,  0.48022053, -0.6016198 ,  0.6512501 ,\n",
       "       -0.30599976,  0.3995207 ,  0.1611771 ,  0.36789933,  0.85954314,\n",
       "        0.0623347 , -0.14163163, -0.5173472 ,  0.6012967 , -0.30832508,\n",
       "        0.5945276 ,  0.5229075 ,  0.15898941,  0.26670974,  0.04463593,\n",
       "        0.69880843, -0.0644099 ,  0.3105393 ,  0.15631242,  0.6915079 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv['congressional']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('congressional', 0.9999998807907104),\n",
       " ('senate', 0.8712809085845947),\n",
       " ('gop', 0.8363369703292847),\n",
       " ('committee', 0.8346565961837769),\n",
       " ('judiciary', 0.8318742513656616),\n",
       " ('lawmakers', 0.8250969052314758),\n",
       " ('chambers', 0.8131620287895203),\n",
       " ('voted', 0.8018012046813965),\n",
       " ('liberal', 0.7954562306404114),\n",
       " ('conservative', 0.7942031621932983)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.similar_by_vector(word2vec.wv['congressional'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = np.asarray(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.zeros(shape=(contents.shape[0], contents.shape[1], max_seq_length, word_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(contents.shape[0]):\n",
    "    for j in range(contents.shape[1]):\n",
    "        for k in range(max_seq_length):\n",
    "            samples[i, j, k, :] = word2vec.wv[contents[i, j][k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO use closest cosine distance to find output word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciton Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squareError(xTrue, xPred):\n",
    "    return K.square(xTrue - xPred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstructionLoss(sample, encoder, decoder, f_w, weight): # (L_1 from the paper)\n",
    "    return K.mean(squareError(sample, decoder(encoder(sample)))) + K.mean(weight*K.log(f_w(encoder(sample))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divergenceLoss(f_w, encoder, sample, z_j, n_j): # Mean of log f_w(E_theta_i(x_j)) + log (1-f_w(z_j, n_j)) from the paper (L_2).\n",
    "    return K.mean(K.log(f_w(encoder(sample)))) + K.mean(K.log(1 - f_w([z_j, n_j])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(data, domain, num_samples):\n",
    "    N = data.shape[1]\n",
    "    return tf.convert_to_tensor(data[domain, np.random.choice(N, num_samples, replace=True),:,:], dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently just doing a restriction to the last z variables, might want to do a matrix multiplication?\n",
    "# pi_Z from the paper. projects a latent distribution in (z, n) to z\n",
    "def projectZ(encoded):\n",
    "    return encoded[0] # take zs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projectN(encoded):\n",
    "    return encoded[1] # taek Ns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in two inputs, n and z, and outputs samples.\n",
    "def createDecoder(z_dims, n_dims, time_steps, output_dims):\n",
    "    # TODO MAYBE: Add in more regularization or different than dropout?\n",
    "\n",
    "    z_inputs = Input(shape=(z_dims,))\n",
    "    n_inputs = Input(shape=(n_dims,))\n",
    "    inputs = concatenate([z_inputs, n_inputs])\n",
    "#     # 150 is arbitrary rn...\n",
    "#     dense = Dense(150)(inputs)\n",
    "    dense = Dense(time_steps*output_dims)(inputs)\n",
    "    reshape = Reshape((time_steps, output_dims))(dense)\n",
    "    # TODO Reshape to enforce time_steps?\n",
    "    bilstm = Bidirectional(LSTM(64, activation='tanh', return_sequences=True))(reshape)\n",
    "    bilstm = Dropout(0.2)(bilstm)\n",
    "    bilstm = Bidirectional(LSTM(64, activation='tanh', return_sequences=False))(bilstm)\n",
    "    bilstm = Dropout(0.2)(bilstm)\n",
    "    \n",
    "    dense = Dense(time_steps*output_dims, activation='linear')(bilstm)\n",
    "    outputs = Reshape((time_steps, output_dims))(dense)\n",
    "    \n",
    "    model = Model(inputs=[z_inputs, n_inputs], outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEncoder(time_steps, input_num, z_dims, n_dims):\n",
    "    # TODO MAYBE: Add in more regularization or different than dropout?\n",
    "    inputs = Input(shape=(time_steps, input_num,))\n",
    "    bilstm = Bidirectional(LSTM(64, activation='tanh', return_sequences=True))(inputs)\n",
    "    bilstm = Dropout(0.2)(bilstm)\n",
    "    dense = Bidirectional(LSTM(64, activation='tanh', return_sequences=False))(bilstm)\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    z_output = Dense(z_dims, activation='linear')(dense)\n",
    "    n_output = Dense(n_dims, activation='linear')(dense)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=[z_output, n_output])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDiscriminator(z_dims, n_dims):\n",
    "    z_inputs = Input(shape=(z_dims,))\n",
    "    n_inputs = Input(shape=(n_dims,))\n",
    "    inputs = concatenate([z_inputs, n_inputs])\n",
    "    \n",
    "    # 150, 100 is arbitrary rn...\n",
    "    dense = Dense(150, activation='relu')(inputs)\n",
    "    dense = Dense(100, activation='relu')(dense)\n",
    "    output = Dense(1, activation='sigmoid')(dense)\n",
    "    \n",
    "    model = Model(inputs=[z_inputs, n_inputs], outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-4\n",
    "enc_optimizer = tf.keras.optimizers.Adam(lr)\n",
    "dec_optimizer = tf.keras.optimizers.Adam(lr)\n",
    "disc_optimizer = tf.keras.optimizers.Adam(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When $P_Z$ is known... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# k is num of domains.\n",
    "# encoders is a list of encoders.\n",
    "# decoders is list of decoders.\n",
    "# samples is a K x N x Timesteps x dim, array of samples, where the 0th index is the domain,\n",
    "# the 1th index is the # of the sample in that domain, 2th index is the # timesteps per sequence, 3th index is the #\n",
    "# of dimensions at each timestep\n",
    "# original_domains is a list of the original domains P_z was derived from.\n",
    "\n",
    "# Currently assuming P_Z is known. Must approximate P_Z first.\n",
    "def trainAutoencodersWithPz(samples, encoders, decoders, discriminator, num_samples, original_domains, epochs=10, weight=1.0):\n",
    "    N = samples.shape[1]\n",
    "    k = samples.shape[0]\n",
    "        \n",
    "    \n",
    "    for i in range(k):\n",
    "        if i not in original_domains:\n",
    "            original_domain = np.random.choice(original_domains)\n",
    "            encoder = encoders[i]\n",
    "            decoder = decoders[i]\n",
    "            original_encoder = encoders[original_domain]\n",
    "            epoch = 0\n",
    "            while(epoch < epochs): # TOOD: could also do until some convergence criteria.\n",
    "                p_Xi_samples = sample(samples, i, num_samples)\n",
    "                p_Z_samples = projectZ(original_encoder(sample(samples, original_domain, num_samples)))\n",
    "                p_Ni_samples = projectN(encoder(sample(samples, i, num_samples)))\n",
    "\n",
    "                with tf.GradientTape() as enc_tape, tf.GradientTape() as dec_tape, tf.GradientTape() as disc_tape:\n",
    "\n",
    "                    reconstruction_loss = reconstructionLoss(p_Xi_samples, encoder, decoder, discriminator, weight)\n",
    "\n",
    "                    # negative b/c gradient ascent.\n",
    "                    divergence_loss = -1 * divergenceLoss(discriminator, encoder, p_Xi_samples, p_Z_samples, p_Ni_samples)\n",
    "\n",
    "                gradients_of_encoder = enc_tape.gradient(reconstruction_loss, encoder.trainable_variables)\n",
    "                gradients_of_decoder = dec_tape.gradient(reconstruction_loss, decoder.trainable_variables)\n",
    "                gradients_of_discriminator = disc_tape.gradient(divergence_loss, discriminator.trainable_variables)\n",
    "\n",
    "\n",
    "                enc_optimizer.apply_gradients(zip(gradients_of_encoder, encoder.trainable_variables))\n",
    "                dec_optimizer.apply_gradients(zip(gradients_of_decoder, decoder.trainable_variables))\n",
    "                disc_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "                \n",
    "                print('Domain {}, Epoch {}:\\n\\tReconstruction Loss: {}\\n\\tDivergence Loss: {}'.format(i, epoch+1, reconstruction_loss, divergence_loss))\n",
    "                epoch+=1\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When $P_Z$ is unknown...\n",
    "\"A straight-forward approach for learning the latent distribution PZ is to train a regularized autoencoder on data from a\n",
    "single representative domain. However, such a representation could potentially capture variability that is specific to\n",
    "that one domain. To learn a more invariant latent representation, we propose the following extension of our autoencoder\n",
    "framework. The basic idea is to alternate between training\n",
    "multiple autoencoders until they agree on a latent representation that is effective for their respective domains. This is\n",
    "particularly relevant for applications to biology; for example, often one is interested in learning a latent representation\n",
    "that integrates all of the data modalities.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# k is num of domains.\n",
    "# encoders is a list of encoders.\n",
    "# decoders is list of decoders.\n",
    "# samples is a K x N x Timesteps x dim, array of samples, where the 0th index is the domain,\n",
    "# the 1th index is the # of the sample in that domain, 2th index is the # timesteps per sequence, 3th index is the #\n",
    "# of dimensions at each timestep\n",
    "# domains is a list of the domains we are currently training over.\n",
    "\n",
    "def trainAutoencodersInitial(samples, encoders, decoders, discriminator, num_samples, domains, epochs=10, weight=1.0):\n",
    "    N = samples.shape[1]\n",
    "    k = samples.shape[0]\n",
    "    \n",
    "    for i in domains:\n",
    "        encoder = encoders[i]\n",
    "        decoder = decoders[i]\n",
    "        for j in domains:\n",
    "            if i != j:\n",
    "                j_encoder = encoders[j]\n",
    "                epoch = 0\n",
    "                while(epoch < epochs): # TOOD: could also do until some convergence criteria.\n",
    "                    p_Xi_samples = sample(samples, i, num_samples)\n",
    "                    p_Zj_samples = projectZ(j_encoder(sample(samples, j, num_samples)))\n",
    "                    p_Ni_samples = projectN(encoder(sample(samples, i, num_samples)))\n",
    "\n",
    "                    with tf.GradientTape() as enc_tape, tf.GradientTape() as dec_tape, tf.GradientTape() as disc_tape:\n",
    "\n",
    "                        reconstruction_loss = reconstructionLoss(p_Xi_samples, encoder, decoder, discriminator, weight)\n",
    "#                         print(p_Xi_samples)\n",
    "\n",
    "                        # negative b/c gradient ascent.\n",
    "                        divergence_loss = -1 * divergenceLoss(discriminator, encoder, p_Xi_samples, p_Zj_samples, p_Ni_samples)\n",
    "#                         print(p_Zj_samples)\n",
    "#                         print(p_Ni_samples)\n",
    "                        \n",
    "                    gradients_of_encoder = enc_tape.gradient(reconstruction_loss, encoder.trainable_variables)\n",
    "                    gradients_of_decoder = dec_tape.gradient(reconstruction_loss, decoder.trainable_variables)\n",
    "                    gradients_of_discriminator = disc_tape.gradient(divergence_loss, discriminator.trainable_variables)\n",
    "            \n",
    "\n",
    "                    enc_optimizer.apply_gradients(zip(gradients_of_encoder, encoder.trainable_variables))\n",
    "                    dec_optimizer.apply_gradients(zip(gradients_of_decoder, decoder.trainable_variables))\n",
    "                    disc_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "                    \n",
    "                    print('Domain {}, Epoch {}:\\n\\tReconstruction Loss: {}\\n\\tDivergence Loss: {}'.format(i, epoch+1, reconstruction_loss, divergence_loss))\n",
    "                    epoch+=1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples is a K x N x Timesteps x dim, array of samples, where the 0th index is the domain,\n",
    "# the 1th index is the # of the sample in that domain, 2th index is the # timesteps per sequence, 3th index is the #\n",
    "# of dimensions at each timestep\n",
    "\n",
    "def initModel(samples, z_dims, n_dims):\n",
    "    \n",
    "    k = samples.shape[0]\n",
    "    N = samples.shape[1]\n",
    "    time_steps = samples.shape[2]\n",
    "    dim = samples.shape[3]\n",
    "    \n",
    "    \n",
    "    \n",
    "    discriminator = createDiscriminator(z_dims, n_dims)\n",
    "    \n",
    "    encoders = []\n",
    "    decoders = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        encoders.append(createEncoder(time_steps, dim, z_dims, n_dims))\n",
    "        decoders.append(createDecoder(z_dims, n_dims, time_steps, dim))\n",
    "    \n",
    "    return encoders, decoders, discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(start_sequences, samples, encoders, decoders, start_domain, end_domain):\n",
    "    N = samples.shape[1]\n",
    "    print(start_sequences.shape)\n",
    "    num_samples = start_sequences.shape[0]\n",
    "    \n",
    "    start_encoder = encoders[start_domain]\n",
    "    end_encoder = encoders[end_domain]\n",
    "    end_decoder = decoders[end_domain]\n",
    "    \n",
    "    z = projectZ(start_encoder(start_sequences))\n",
    "    n = projectN(end_encoder(sample(samples, end_domain, num_samples)))\n",
    "    \n",
    "    end_sequences = end_decoder([z, n])\n",
    "    return end_sequences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vecSeqToSentence(sequence):\n",
    "    sequence = K.eval(sequence)\n",
    "    sentence = []\n",
    "    for i in range(sequence.shape[0]):\n",
    "        word = sequence[i,:]\n",
    "#         print(word)\n",
    "#         print(word2vec.wv.similar_by_vector(word))\n",
    "        sentence.append(word2vec.wv.similar_by_vector(word)[0][0])\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dims = 60 # len(n)\n",
    "z_dims = 240 # len(Z)\n",
    "\n",
    "num_epochs = 100\n",
    "num_samples = 128\n",
    "\n",
    "weight = 1\n",
    "\n",
    "original_domains = [0, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = tf.convert_to_tensor(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders, decoders, discriminator = initModel(samples, z_dims, n_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original First Sentence from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'on tuesday ’ s broadcast ” zeleny said , “ and she ’ s having a difficult time in federal prison , no question . ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(contents[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 100)\n"
     ]
    }
   ],
   "source": [
    "seq = tf.convert_to_tensor(np.asarray([samples[0, 0, :, :]]), dtype=tf.float32)\n",
    "translation = translate(seq, samples, encoders, decoders, original_domains[0], original_domains[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original First Sentence from 0 translated to 1 before Training (Random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yasser', 'dominguez', 'fronds', \"'can\", 'fewest', 'insignia', '151', 'unyielding', 'affectless', 'corden', 'brawls', 'improvisational', 'sikhs', 'barracks', 'affronted', 'exposes', 'downstairs', 'guinean', 'munger', 'overdrive', 'ayer', 'throng', 'kokomo', 'prothero', 'obscurities', 'americas', 'tedious', 'transients', 'winship', 'autolink', 'artthrob', 'ruckus', 'helpfully', 'hunh', 'biographies', 'turkeys', 'viridiana', 'soapbox', 'baahubali', 'welty', 'maples', 'solstice', 'pelletz', 'dissolution', 'cornwell', 'punishments', 'epitaph', 'inhofe', 'chatterjee', 'aloha']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vecSeqToSentence(translation[0,:,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain 0, Epoch 1:\n",
      "\tReconstruction Loss: 0.18974536657333374\n",
      "\tDivergence Loss: 1.4089030027389526\n",
      "Domain 0, Epoch 2:\n",
      "\tReconstruction Loss: 0.13334870338439941\n",
      "\tDivergence Loss: 1.4225118160247803\n",
      "Domain 0, Epoch 3:\n",
      "\tReconstruction Loss: 0.07649147510528564\n",
      "\tDivergence Loss: 1.440427541732788\n",
      "Domain 0, Epoch 4:\n",
      "\tReconstruction Loss: 0.0489310622215271\n",
      "\tDivergence Loss: 1.4419782161712646\n",
      "Domain 0, Epoch 5:\n",
      "\tReconstruction Loss: -0.009554445743560791\n",
      "\tDivergence Loss: 1.462437391281128\n",
      "Domain 0, Epoch 6:\n",
      "\tReconstruction Loss: -0.10720270872116089\n",
      "\tDivergence Loss: 1.4892263412475586\n",
      "Domain 0, Epoch 7:\n",
      "\tReconstruction Loss: -0.17957699298858643\n",
      "\tDivergence Loss: 1.5167958736419678\n",
      "Domain 0, Epoch 8:\n",
      "\tReconstruction Loss: -0.249464213848114\n",
      "\tDivergence Loss: 1.5463027954101562\n",
      "Domain 0, Epoch 9:\n",
      "\tReconstruction Loss: -0.333532452583313\n",
      "\tDivergence Loss: 1.5867674350738525\n",
      "Domain 0, Epoch 10:\n",
      "\tReconstruction Loss: -0.41586029529571533\n",
      "\tDivergence Loss: 1.635570764541626\n",
      "Domain 0, Epoch 11:\n",
      "\tReconstruction Loss: -0.4915159344673157\n",
      "\tDivergence Loss: 1.6549357175827026\n",
      "Domain 0, Epoch 12:\n",
      "\tReconstruction Loss: -0.5572577714920044\n",
      "\tDivergence Loss: 1.6779568195343018\n",
      "Domain 0, Epoch 13:\n",
      "\tReconstruction Loss: -0.6160790920257568\n",
      "\tDivergence Loss: 1.665010690689087\n",
      "Domain 0, Epoch 14:\n",
      "\tReconstruction Loss: -0.6393353343009949\n",
      "\tDivergence Loss: 1.648604154586792\n",
      "Domain 0, Epoch 15:\n",
      "\tReconstruction Loss: -0.6616048812866211\n",
      "\tDivergence Loss: 1.632699966430664\n",
      "Domain 0, Epoch 16:\n",
      "\tReconstruction Loss: -0.6568147540092468\n",
      "\tDivergence Loss: 1.6127591133117676\n",
      "Domain 0, Epoch 17:\n",
      "\tReconstruction Loss: -0.6193704009056091\n",
      "\tDivergence Loss: 1.5377230644226074\n",
      "Domain 0, Epoch 18:\n",
      "\tReconstruction Loss: -0.5751550793647766\n",
      "\tDivergence Loss: 1.4644358158111572\n",
      "Domain 0, Epoch 19:\n",
      "\tReconstruction Loss: -0.46215373277664185\n",
      "\tDivergence Loss: 1.3818475008010864\n",
      "Domain 0, Epoch 20:\n",
      "\tReconstruction Loss: -0.41573238372802734\n",
      "\tDivergence Loss: 1.2865943908691406\n",
      "Domain 0, Epoch 21:\n",
      "\tReconstruction Loss: -0.28587836027145386\n",
      "\tDivergence Loss: 1.1795347929000854\n",
      "Domain 0, Epoch 22:\n",
      "\tReconstruction Loss: -0.23751521110534668\n",
      "\tDivergence Loss: 1.0973262786865234\n",
      "Domain 0, Epoch 23:\n",
      "\tReconstruction Loss: -0.13922125101089478\n",
      "\tDivergence Loss: 1.011486291885376\n",
      "Domain 0, Epoch 24:\n",
      "\tReconstruction Loss: -0.056204140186309814\n",
      "\tDivergence Loss: 0.9408872127532959\n",
      "Domain 0, Epoch 25:\n",
      "\tReconstruction Loss: 0.01576375961303711\n",
      "\tDivergence Loss: 0.8611948490142822\n",
      "Domain 0, Epoch 26:\n",
      "\tReconstruction Loss: 0.10577595233917236\n",
      "\tDivergence Loss: 0.7896072864532471\n",
      "Domain 0, Epoch 27:\n",
      "\tReconstruction Loss: 0.136214017868042\n",
      "\tDivergence Loss: 0.7274183630943298\n",
      "Domain 0, Epoch 28:\n",
      "\tReconstruction Loss: 0.18530505895614624\n",
      "\tDivergence Loss: 0.6767858266830444\n",
      "Domain 0, Epoch 29:\n",
      "\tReconstruction Loss: 0.25212666392326355\n",
      "\tDivergence Loss: 0.6395089626312256\n",
      "Domain 0, Epoch 30:\n",
      "\tReconstruction Loss: 0.2532859444618225\n",
      "\tDivergence Loss: 0.6044604778289795\n",
      "Domain 0, Epoch 31:\n",
      "\tReconstruction Loss: 0.2863107919692993\n",
      "\tDivergence Loss: 0.5715742111206055\n",
      "Domain 0, Epoch 32:\n",
      "\tReconstruction Loss: 0.2696070075035095\n",
      "\tDivergence Loss: 0.545846700668335\n",
      "Domain 0, Epoch 33:\n",
      "\tReconstruction Loss: 0.28061801195144653\n",
      "\tDivergence Loss: 0.5151796340942383\n",
      "Domain 0, Epoch 34:\n",
      "\tReconstruction Loss: 0.2959027588367462\n",
      "\tDivergence Loss: 0.5049251317977905\n",
      "Domain 0, Epoch 35:\n",
      "\tReconstruction Loss: 0.3016415536403656\n",
      "\tDivergence Loss: 0.47617802023887634\n",
      "Domain 0, Epoch 36:\n",
      "\tReconstruction Loss: 0.290546715259552\n",
      "\tDivergence Loss: 0.4670904874801636\n",
      "Domain 0, Epoch 37:\n",
      "\tReconstruction Loss: 0.3162161111831665\n",
      "\tDivergence Loss: 0.44335803389549255\n",
      "Domain 0, Epoch 38:\n",
      "\tReconstruction Loss: 0.3002587556838989\n",
      "\tDivergence Loss: 0.4101340174674988\n",
      "Domain 0, Epoch 39:\n",
      "\tReconstruction Loss: 0.33207035064697266\n",
      "\tDivergence Loss: 0.4016890227794647\n",
      "Domain 0, Epoch 40:\n",
      "\tReconstruction Loss: 0.34704339504241943\n",
      "\tDivergence Loss: 0.3986377716064453\n",
      "Domain 0, Epoch 41:\n",
      "\tReconstruction Loss: 0.309930682182312\n",
      "\tDivergence Loss: 0.3723134398460388\n",
      "Domain 0, Epoch 42:\n",
      "\tReconstruction Loss: 0.3387940227985382\n",
      "\tDivergence Loss: 0.36057329177856445\n",
      "Domain 0, Epoch 43:\n",
      "\tReconstruction Loss: 0.3368441164493561\n",
      "\tDivergence Loss: 0.341555118560791\n",
      "Domain 0, Epoch 44:\n",
      "\tReconstruction Loss: 0.30903345346450806\n",
      "\tDivergence Loss: 0.35473042726516724\n",
      "Domain 0, Epoch 45:\n",
      "\tReconstruction Loss: 0.3145477771759033\n",
      "\tDivergence Loss: 0.35442498326301575\n",
      "Domain 0, Epoch 46:\n",
      "\tReconstruction Loss: 0.34817585349082947\n",
      "\tDivergence Loss: 0.35538622736930847\n",
      "Domain 0, Epoch 47:\n",
      "\tReconstruction Loss: 0.36481916904449463\n",
      "\tDivergence Loss: 0.34631019830703735\n",
      "Domain 0, Epoch 48:\n",
      "\tReconstruction Loss: 0.3321866989135742\n",
      "\tDivergence Loss: 0.3548630177974701\n",
      "Domain 0, Epoch 49:\n",
      "\tReconstruction Loss: 0.34472930431365967\n",
      "\tDivergence Loss: 0.3459717035293579\n",
      "Domain 0, Epoch 50:\n",
      "\tReconstruction Loss: 0.3459477424621582\n",
      "\tDivergence Loss: 0.3576440215110779\n",
      "Domain 0, Epoch 51:\n",
      "\tReconstruction Loss: 0.34644338488578796\n",
      "\tDivergence Loss: 0.3379606306552887\n",
      "Domain 0, Epoch 52:\n",
      "\tReconstruction Loss: 0.3582785427570343\n",
      "\tDivergence Loss: 0.34389644861221313\n",
      "Domain 0, Epoch 53:\n",
      "\tReconstruction Loss: 0.3517150282859802\n",
      "\tDivergence Loss: 0.35334086418151855\n",
      "Domain 0, Epoch 54:\n",
      "\tReconstruction Loss: 0.3263675570487976\n",
      "\tDivergence Loss: 0.3768441677093506\n",
      "Domain 0, Epoch 55:\n",
      "\tReconstruction Loss: 0.3615913987159729\n",
      "\tDivergence Loss: 0.36213067173957825\n",
      "Domain 0, Epoch 56:\n",
      "\tReconstruction Loss: 0.3239535093307495\n",
      "\tDivergence Loss: 0.38940244913101196\n",
      "Domain 0, Epoch 57:\n",
      "\tReconstruction Loss: 0.300944060087204\n",
      "\tDivergence Loss: 0.43616294860839844\n",
      "Domain 0, Epoch 58:\n",
      "\tReconstruction Loss: 0.2528289258480072\n",
      "\tDivergence Loss: 0.5214830636978149\n",
      "Domain 0, Epoch 59:\n",
      "\tReconstruction Loss: 0.2192825973033905\n",
      "\tDivergence Loss: 0.5651872158050537\n",
      "Domain 0, Epoch 60:\n",
      "\tReconstruction Loss: 0.15365302562713623\n",
      "\tDivergence Loss: 0.6758490204811096\n",
      "Domain 0, Epoch 61:\n",
      "\tReconstruction Loss: 0.09405645728111267\n",
      "\tDivergence Loss: 0.7224560976028442\n",
      "Domain 0, Epoch 62:\n",
      "\tReconstruction Loss: 0.02561849355697632\n",
      "\tDivergence Loss: 0.8394639492034912\n",
      "Domain 0, Epoch 63:\n",
      "\tReconstruction Loss: -0.09134185314178467\n",
      "\tDivergence Loss: 1.0308246612548828\n",
      "Domain 0, Epoch 64:\n",
      "\tReconstruction Loss: -0.2049562931060791\n",
      "\tDivergence Loss: 1.2186822891235352\n",
      "Domain 0, Epoch 65:\n",
      "\tReconstruction Loss: -0.2101033329963684\n",
      "\tDivergence Loss: 1.2580877542495728\n",
      "Domain 0, Epoch 66:\n",
      "\tReconstruction Loss: -0.19131064414978027\n",
      "\tDivergence Loss: 1.3327562808990479\n",
      "Domain 0, Epoch 67:\n",
      "\tReconstruction Loss: -0.19360435009002686\n",
      "\tDivergence Loss: 1.33761465549469\n",
      "Domain 0, Epoch 68:\n",
      "\tReconstruction Loss: -0.14468568563461304\n",
      "\tDivergence Loss: 1.309798002243042\n",
      "Domain 0, Epoch 69:\n",
      "\tReconstruction Loss: -0.07129848003387451\n",
      "\tDivergence Loss: 1.2570502758026123\n",
      "Domain 0, Epoch 70:\n",
      "\tReconstruction Loss: 0.004587233066558838\n",
      "\tDivergence Loss: 1.1755342483520508\n",
      "Domain 0, Epoch 71:\n",
      "\tReconstruction Loss: 0.11077713966369629\n",
      "\tDivergence Loss: 1.0551848411560059\n",
      "Domain 0, Epoch 72:\n",
      "\tReconstruction Loss: 0.1812109351158142\n",
      "\tDivergence Loss: 0.94227135181427\n",
      "Domain 0, Epoch 73:\n",
      "\tReconstruction Loss: 0.24317455291748047\n",
      "\tDivergence Loss: 0.811505138874054\n",
      "Domain 0, Epoch 74:\n",
      "\tReconstruction Loss: 0.30946436524391174\n",
      "\tDivergence Loss: 0.7241371870040894\n",
      "Domain 0, Epoch 75:\n",
      "\tReconstruction Loss: 0.3703399896621704\n",
      "\tDivergence Loss: 0.6112057566642761\n",
      "Domain 0, Epoch 76:\n",
      "\tReconstruction Loss: 0.4279780983924866\n",
      "\tDivergence Loss: 0.5333422422409058\n",
      "Domain 0, Epoch 77:\n",
      "\tReconstruction Loss: 0.41463541984558105\n",
      "\tDivergence Loss: 0.45848625898361206\n",
      "Domain 0, Epoch 78:\n",
      "\tReconstruction Loss: 0.4620046615600586\n",
      "\tDivergence Loss: 0.417651504278183\n",
      "Domain 0, Epoch 79:\n",
      "\tReconstruction Loss: 0.4472503066062927\n",
      "\tDivergence Loss: 0.36616113781929016\n",
      "Domain 0, Epoch 80:\n",
      "\tReconstruction Loss: 0.4836270213127136\n",
      "\tDivergence Loss: 0.34339356422424316\n",
      "Domain 0, Epoch 81:\n",
      "\tReconstruction Loss: 0.4728706479072571\n",
      "\tDivergence Loss: 0.3191298246383667\n",
      "Domain 0, Epoch 82:\n",
      "\tReconstruction Loss: 0.5082187056541443\n",
      "\tDivergence Loss: 0.28474974632263184\n",
      "Domain 0, Epoch 83:\n",
      "\tReconstruction Loss: 0.45074862241744995\n",
      "\tDivergence Loss: 0.26835551857948303\n",
      "Domain 0, Epoch 84:\n",
      "\tReconstruction Loss: 0.4618394374847412\n",
      "\tDivergence Loss: 0.23791871964931488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain 0, Epoch 85:\n",
      "\tReconstruction Loss: 0.48573654890060425\n",
      "\tDivergence Loss: 0.22616234421730042\n",
      "Domain 0, Epoch 86:\n",
      "\tReconstruction Loss: 0.45798158645629883\n",
      "\tDivergence Loss: 0.21308661997318268\n",
      "Domain 0, Epoch 87:\n",
      "\tReconstruction Loss: 0.4479249119758606\n",
      "\tDivergence Loss: 0.21275237202644348\n",
      "Domain 0, Epoch 88:\n",
      "\tReconstruction Loss: 0.4343830347061157\n",
      "\tDivergence Loss: 0.2037571370601654\n",
      "Domain 0, Epoch 89:\n",
      "\tReconstruction Loss: 0.4366544485092163\n",
      "\tDivergence Loss: 0.20952829718589783\n",
      "Domain 0, Epoch 90:\n",
      "\tReconstruction Loss: 0.4278284013271332\n",
      "\tDivergence Loss: 0.2173108160495758\n",
      "Domain 0, Epoch 91:\n",
      "\tReconstruction Loss: 0.3864607512950897\n",
      "\tDivergence Loss: 0.23116466403007507\n",
      "Domain 0, Epoch 92:\n",
      "\tReconstruction Loss: 0.3799397051334381\n",
      "\tDivergence Loss: 0.23971311748027802\n",
      "Domain 0, Epoch 93:\n",
      "\tReconstruction Loss: 0.36212873458862305\n",
      "\tDivergence Loss: 0.27255532145500183\n",
      "Domain 0, Epoch 94:\n",
      "\tReconstruction Loss: 0.35209590196609497\n",
      "\tDivergence Loss: 0.29946786165237427\n",
      "Domain 0, Epoch 95:\n",
      "\tReconstruction Loss: 0.31325864791870117\n",
      "\tDivergence Loss: 0.352943480014801\n",
      "Domain 0, Epoch 96:\n",
      "\tReconstruction Loss: 0.2709289789199829\n",
      "\tDivergence Loss: 0.3856537938117981\n",
      "Domain 0, Epoch 97:\n",
      "\tReconstruction Loss: 0.26010334491729736\n",
      "\tDivergence Loss: 0.4272489547729492\n",
      "Domain 0, Epoch 98:\n",
      "\tReconstruction Loss: 0.22794103622436523\n",
      "\tDivergence Loss: 0.4515046775341034\n",
      "Domain 0, Epoch 99:\n",
      "\tReconstruction Loss: 0.23182672262191772\n",
      "\tDivergence Loss: 0.5109443068504333\n",
      "Domain 0, Epoch 100:\n",
      "\tReconstruction Loss: 0.19009777903556824\n",
      "\tDivergence Loss: 0.5579718947410583\n",
      "Domain 1, Epoch 1:\n",
      "\tReconstruction Loss: 0.4109182357788086\n",
      "\tDivergence Loss: 2.2918548583984375\n",
      "Domain 1, Epoch 2:\n",
      "\tReconstruction Loss: 0.015458941459655762\n",
      "\tDivergence Loss: 2.7319846153259277\n",
      "Domain 1, Epoch 3:\n",
      "\tReconstruction Loss: -0.3600959777832031\n",
      "\tDivergence Loss: 2.9456892013549805\n",
      "Domain 1, Epoch 4:\n",
      "\tReconstruction Loss: -0.8142997026443481\n",
      "\tDivergence Loss: 3.0615668296813965\n",
      "Domain 1, Epoch 5:\n",
      "\tReconstruction Loss: -1.321532964706421\n",
      "\tDivergence Loss: 3.1975650787353516\n",
      "Domain 1, Epoch 6:\n",
      "\tReconstruction Loss: -1.9593243598937988\n",
      "\tDivergence Loss: 3.4620814323425293\n",
      "Domain 1, Epoch 7:\n",
      "\tReconstruction Loss: -2.8007709980010986\n",
      "\tDivergence Loss: 4.008481979370117\n",
      "Domain 1, Epoch 8:\n",
      "\tReconstruction Loss: -3.5706470012664795\n",
      "\tDivergence Loss: 4.609941005706787\n",
      "Domain 1, Epoch 9:\n",
      "\tReconstruction Loss: -4.343780040740967\n",
      "\tDivergence Loss: 5.242826461791992\n",
      "Domain 1, Epoch 10:\n",
      "\tReconstruction Loss: -4.887067794799805\n",
      "\tDivergence Loss: 5.70871639251709\n",
      "Domain 1, Epoch 11:\n",
      "\tReconstruction Loss: -5.252819061279297\n",
      "\tDivergence Loss: 6.068919658660889\n",
      "Domain 1, Epoch 12:\n",
      "\tReconstruction Loss: -5.675773620605469\n",
      "\tDivergence Loss: 6.412871360778809\n",
      "Domain 1, Epoch 13:\n",
      "\tReconstruction Loss: -5.686821937561035\n",
      "\tDivergence Loss: 6.442335605621338\n",
      "Domain 1, Epoch 14:\n",
      "\tReconstruction Loss: -5.625298500061035\n",
      "\tDivergence Loss: 6.309249401092529\n",
      "Domain 1, Epoch 15:\n",
      "\tReconstruction Loss: -5.212271690368652\n",
      "\tDivergence Loss: 5.966371059417725\n",
      "Domain 1, Epoch 16:\n",
      "\tReconstruction Loss: -4.556792259216309\n",
      "\tDivergence Loss: 5.353572845458984\n",
      "Domain 1, Epoch 17:\n",
      "\tReconstruction Loss: -3.594654083251953\n",
      "\tDivergence Loss: 4.5381879806518555\n",
      "Domain 1, Epoch 18:\n",
      "\tReconstruction Loss: -2.467801809310913\n",
      "\tDivergence Loss: 3.5529239177703857\n",
      "Domain 1, Epoch 19:\n",
      "\tReconstruction Loss: -1.1297229528427124\n",
      "\tDivergence Loss: 2.3693084716796875\n",
      "Domain 1, Epoch 20:\n",
      "\tReconstruction Loss: -0.09647244215011597\n",
      "\tDivergence Loss: 1.453514814376831\n",
      "Domain 1, Epoch 21:\n",
      "\tReconstruction Loss: 0.3901911973953247\n",
      "\tDivergence Loss: 1.1050995588302612\n",
      "Domain 1, Epoch 22:\n",
      "\tReconstruction Loss: 0.5112677812576294\n",
      "\tDivergence Loss: 1.055476427078247\n",
      "Domain 1, Epoch 23:\n",
      "\tReconstruction Loss: 0.591917872428894\n",
      "\tDivergence Loss: 1.0221327543258667\n",
      "Domain 1, Epoch 24:\n",
      "\tReconstruction Loss: 0.5182986259460449\n",
      "\tDivergence Loss: 0.921958863735199\n",
      "Domain 1, Epoch 25:\n",
      "\tReconstruction Loss: 0.5865671038627625\n",
      "\tDivergence Loss: 0.7750737071037292\n",
      "Domain 1, Epoch 26:\n",
      "\tReconstruction Loss: 0.5524728298187256\n",
      "\tDivergence Loss: 0.5994850397109985\n",
      "Domain 1, Epoch 27:\n",
      "\tReconstruction Loss: 0.5466766953468323\n",
      "\tDivergence Loss: 0.4341072142124176\n",
      "Domain 1, Epoch 28:\n",
      "\tReconstruction Loss: 0.5367626547813416\n",
      "\tDivergence Loss: 0.2954184412956238\n",
      "Domain 1, Epoch 29:\n",
      "\tReconstruction Loss: 0.5707001686096191\n",
      "\tDivergence Loss: 0.1908818781375885\n",
      "Domain 1, Epoch 30:\n",
      "\tReconstruction Loss: 0.5695452690124512\n",
      "\tDivergence Loss: 0.12237575650215149\n",
      "Domain 1, Epoch 31:\n",
      "\tReconstruction Loss: 0.5442805886268616\n",
      "\tDivergence Loss: 0.07920674234628677\n",
      "Domain 1, Epoch 32:\n",
      "\tReconstruction Loss: 0.5783421993255615\n",
      "\tDivergence Loss: 0.052423764020204544\n",
      "Domain 1, Epoch 33:\n",
      "\tReconstruction Loss: 0.5540675520896912\n",
      "\tDivergence Loss: 0.03476628288626671\n",
      "Domain 1, Epoch 34:\n",
      "\tReconstruction Loss: 0.5373751521110535\n",
      "\tDivergence Loss: 0.025499418377876282\n",
      "Domain 1, Epoch 35:\n",
      "\tReconstruction Loss: 0.6063758730888367\n",
      "\tDivergence Loss: 0.02091316506266594\n",
      "Domain 1, Epoch 36:\n",
      "\tReconstruction Loss: 0.5832957625389099\n",
      "\tDivergence Loss: 0.017251398414373398\n",
      "Domain 1, Epoch 37:\n",
      "\tReconstruction Loss: 0.5812374949455261\n",
      "\tDivergence Loss: 0.014859134331345558\n",
      "Domain 1, Epoch 38:\n",
      "\tReconstruction Loss: 0.5738016963005066\n",
      "\tDivergence Loss: 0.012832525186240673\n",
      "Domain 1, Epoch 39:\n",
      "\tReconstruction Loss: 0.5530288815498352\n",
      "\tDivergence Loss: 0.01145165041089058\n",
      "Domain 1, Epoch 40:\n",
      "\tReconstruction Loss: 0.5403069257736206\n",
      "\tDivergence Loss: 0.010283833369612694\n",
      "Domain 1, Epoch 41:\n",
      "\tReconstruction Loss: 0.5364943742752075\n",
      "\tDivergence Loss: 0.009516074322164059\n",
      "Domain 1, Epoch 42:\n",
      "\tReconstruction Loss: 0.55363929271698\n",
      "\tDivergence Loss: 0.008760293945670128\n",
      "Domain 1, Epoch 43:\n",
      "\tReconstruction Loss: 0.5570446848869324\n",
      "\tDivergence Loss: 0.008218254894018173\n",
      "Domain 1, Epoch 44:\n",
      "\tReconstruction Loss: 0.5906123518943787\n",
      "\tDivergence Loss: 0.007829622365534306\n",
      "Domain 1, Epoch 45:\n",
      "\tReconstruction Loss: 0.5439077615737915\n",
      "\tDivergence Loss: 0.007565552834421396\n",
      "Domain 1, Epoch 46:\n",
      "\tReconstruction Loss: 0.5589059591293335\n",
      "\tDivergence Loss: 0.007211103104054928\n",
      "Domain 1, Epoch 47:\n",
      "\tReconstruction Loss: 0.5742340087890625\n",
      "\tDivergence Loss: 0.006916883867233992\n",
      "Domain 1, Epoch 48:\n",
      "\tReconstruction Loss: 0.5834051966667175\n",
      "\tDivergence Loss: 0.006670916453003883\n",
      "Domain 1, Epoch 49:\n",
      "\tReconstruction Loss: 0.5539259314537048\n",
      "\tDivergence Loss: 0.006500135641545057\n",
      "Domain 1, Epoch 50:\n",
      "\tReconstruction Loss: 0.5195545554161072\n",
      "\tDivergence Loss: 0.006257045082747936\n",
      "Domain 1, Epoch 51:\n",
      "\tReconstruction Loss: 0.5919608473777771\n",
      "\tDivergence Loss: 0.006031753495335579\n",
      "Domain 1, Epoch 52:\n",
      "\tReconstruction Loss: 0.5660466551780701\n",
      "\tDivergence Loss: 0.005750695243477821\n",
      "Domain 1, Epoch 53:\n",
      "\tReconstruction Loss: 0.5542349815368652\n",
      "\tDivergence Loss: 0.005562505219131708\n",
      "Domain 1, Epoch 54:\n",
      "\tReconstruction Loss: 0.5378946661949158\n",
      "\tDivergence Loss: 0.005351362284272909\n",
      "Domain 1, Epoch 55:\n",
      "\tReconstruction Loss: 0.5194730162620544\n",
      "\tDivergence Loss: 0.005034231580793858\n",
      "Domain 1, Epoch 56:\n",
      "\tReconstruction Loss: 0.5817511677742004\n",
      "\tDivergence Loss: 0.004849810153245926\n",
      "Domain 1, Epoch 57:\n",
      "\tReconstruction Loss: 0.566889226436615\n",
      "\tDivergence Loss: 0.004647334571927786\n",
      "Domain 1, Epoch 58:\n",
      "\tReconstruction Loss: 0.5966704487800598\n",
      "\tDivergence Loss: 0.004444313235580921\n",
      "Domain 1, Epoch 59:\n",
      "\tReconstruction Loss: 0.5789622664451599\n",
      "\tDivergence Loss: 0.004258435219526291\n",
      "Domain 1, Epoch 60:\n",
      "\tReconstruction Loss: 0.5474022030830383\n",
      "\tDivergence Loss: 0.004105004481971264\n",
      "Domain 1, Epoch 61:\n",
      "\tReconstruction Loss: 0.5734586715698242\n",
      "\tDivergence Loss: 0.003868700237944722\n",
      "Domain 1, Epoch 62:\n",
      "\tReconstruction Loss: 0.5363055467605591\n",
      "\tDivergence Loss: 0.003699346911162138\n",
      "Domain 1, Epoch 63:\n",
      "\tReconstruction Loss: 0.5829482674598694\n",
      "\tDivergence Loss: 0.0035984197165817022\n",
      "Domain 1, Epoch 64:\n",
      "\tReconstruction Loss: 0.5765818357467651\n",
      "\tDivergence Loss: 0.003361207665875554\n",
      "Domain 1, Epoch 65:\n",
      "\tReconstruction Loss: 0.565300703048706\n",
      "\tDivergence Loss: 0.0033022076822817326\n",
      "Domain 1, Epoch 66:\n",
      "\tReconstruction Loss: 0.5550047159194946\n",
      "\tDivergence Loss: 0.003185026813298464\n",
      "Domain 1, Epoch 67:\n",
      "\tReconstruction Loss: 0.5801864862442017\n",
      "\tDivergence Loss: 0.0030332955066114664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain 1, Epoch 68:\n",
      "\tReconstruction Loss: 0.5511907935142517\n",
      "\tDivergence Loss: 0.0029586083255708218\n",
      "Domain 1, Epoch 69:\n",
      "\tReconstruction Loss: 0.5698443651199341\n",
      "\tDivergence Loss: 0.0028308588080108166\n",
      "Domain 1, Epoch 70:\n",
      "\tReconstruction Loss: 0.566948413848877\n",
      "\tDivergence Loss: 0.0027440800331532955\n",
      "Domain 1, Epoch 71:\n",
      "\tReconstruction Loss: 0.5435666441917419\n",
      "\tDivergence Loss: 0.0027027197647839785\n",
      "Domain 1, Epoch 72:\n",
      "\tReconstruction Loss: 0.5555870532989502\n",
      "\tDivergence Loss: 0.0026355076115578413\n",
      "Domain 1, Epoch 73:\n",
      "\tReconstruction Loss: 0.5775895714759827\n",
      "\tDivergence Loss: 0.00252106343396008\n",
      "Domain 1, Epoch 74:\n",
      "\tReconstruction Loss: 0.5342122912406921\n",
      "\tDivergence Loss: 0.002468107733875513\n",
      "Domain 1, Epoch 75:\n",
      "\tReconstruction Loss: 0.53425532579422\n",
      "\tDivergence Loss: 0.002456900430843234\n",
      "Domain 1, Epoch 76:\n",
      "\tReconstruction Loss: 0.5065349340438843\n",
      "\tDivergence Loss: 0.002361276187002659\n",
      "Domain 1, Epoch 77:\n",
      "\tReconstruction Loss: 0.5537890791893005\n",
      "\tDivergence Loss: 0.002361312275752425\n",
      "Domain 1, Epoch 78:\n",
      "\tReconstruction Loss: 0.5793305039405823\n",
      "\tDivergence Loss: 0.0023111822083592415\n",
      "Domain 1, Epoch 79:\n",
      "\tReconstruction Loss: 0.5800984501838684\n",
      "\tDivergence Loss: 0.00227774353697896\n",
      "Domain 1, Epoch 80:\n",
      "\tReconstruction Loss: 0.5487589836120605\n",
      "\tDivergence Loss: 0.002219351241365075\n",
      "Domain 1, Epoch 81:\n",
      "\tReconstruction Loss: 0.5698596239089966\n",
      "\tDivergence Loss: 0.0021585712675005198\n",
      "Domain 1, Epoch 82:\n",
      "\tReconstruction Loss: 0.5582947134971619\n",
      "\tDivergence Loss: 0.002103055128827691\n",
      "Domain 1, Epoch 83:\n",
      "\tReconstruction Loss: 0.494088351726532\n",
      "\tDivergence Loss: 0.0020373035222291946\n",
      "Domain 1, Epoch 84:\n",
      "\tReconstruction Loss: 0.5466232299804688\n",
      "\tDivergence Loss: 0.00205297046341002\n",
      "Domain 1, Epoch 85:\n",
      "\tReconstruction Loss: 0.5235589742660522\n",
      "\tDivergence Loss: 0.0019713875371962786\n",
      "Domain 1, Epoch 86:\n",
      "\tReconstruction Loss: 0.5608566403388977\n",
      "\tDivergence Loss: 0.001940885093063116\n",
      "Domain 1, Epoch 87:\n",
      "\tReconstruction Loss: 0.5742533206939697\n",
      "\tDivergence Loss: 0.0019418522715568542\n",
      "Domain 1, Epoch 88:\n",
      "\tReconstruction Loss: 0.5193182229995728\n",
      "\tDivergence Loss: 0.0019335561664775014\n",
      "Domain 1, Epoch 89:\n",
      "\tReconstruction Loss: 0.5846104621887207\n",
      "\tDivergence Loss: 0.0018957795109599829\n",
      "Domain 1, Epoch 90:\n",
      "\tReconstruction Loss: 0.6010295748710632\n",
      "\tDivergence Loss: 0.0018751840107142925\n",
      "Domain 1, Epoch 91:\n",
      "\tReconstruction Loss: 0.5766283869743347\n",
      "\tDivergence Loss: 0.0018232788424938917\n",
      "Domain 1, Epoch 92:\n",
      "\tReconstruction Loss: 0.5777878165245056\n",
      "\tDivergence Loss: 0.001789964735507965\n",
      "Domain 1, Epoch 93:\n",
      "\tReconstruction Loss: 0.5452452898025513\n",
      "\tDivergence Loss: 0.001816140254959464\n",
      "Domain 1, Epoch 94:\n",
      "\tReconstruction Loss: 0.5447126626968384\n",
      "\tDivergence Loss: 0.0017453692853450775\n",
      "Domain 1, Epoch 95:\n",
      "\tReconstruction Loss: 0.556218683719635\n",
      "\tDivergence Loss: 0.00174777302891016\n",
      "Domain 1, Epoch 96:\n",
      "\tReconstruction Loss: 0.5694375038146973\n",
      "\tDivergence Loss: 0.0017052984330803156\n",
      "Domain 1, Epoch 97:\n",
      "\tReconstruction Loss: 0.5820490121841431\n",
      "\tDivergence Loss: 0.0017175634857267141\n",
      "Domain 1, Epoch 98:\n",
      "\tReconstruction Loss: 0.5790043473243713\n",
      "\tDivergence Loss: 0.0016549329739063978\n",
      "Domain 1, Epoch 99:\n",
      "\tReconstruction Loss: 0.5524624586105347\n",
      "\tDivergence Loss: 0.0016743564046919346\n",
      "Domain 1, Epoch 100:\n",
      "\tReconstruction Loss: 0.5636640191078186\n",
      "\tDivergence Loss: 0.0016465630615130067\n"
     ]
    }
   ],
   "source": [
    "trainAutoencodersInitial(samples, encoders, decoders, discriminator, num_samples, original_domains, epochs=num_epochs, weight=weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original First Sentence from 0 translated to 1 after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 100)\n",
      "['(', 'cnn', ')', ')', 'although', 'the', 'although', 'although', 'although', 'although', '”', 'word', 'although', 'short', 'short', 'once', 'short', 'short', 'short', 'trouble', 'trouble', 'trouble', 'frustration', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_']\n"
     ]
    }
   ],
   "source": [
    "translation = translate(seq, samples, encoders, decoders, original_domains[0], original_domains[1])\n",
    "vecSeqToSentence(translation[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain 2, Epoch 1:\n",
      "\tReconstruction Loss: 0.534590482711792\n",
      "\tDivergence Loss: 14.22704029083252\n",
      "Domain 2, Epoch 2:\n",
      "\tReconstruction Loss: 0.3670617938041687\n",
      "\tDivergence Loss: 12.272100448608398\n",
      "Domain 2, Epoch 3:\n",
      "\tReconstruction Loss: 0.13843506574630737\n",
      "\tDivergence Loss: 9.682933807373047\n",
      "Domain 2, Epoch 4:\n",
      "\tReconstruction Loss: -0.10037904977798462\n",
      "\tDivergence Loss: 6.9949564933776855\n",
      "Domain 2, Epoch 5:\n",
      "\tReconstruction Loss: -0.36392509937286377\n",
      "\tDivergence Loss: 4.375701904296875\n",
      "Domain 2, Epoch 6:\n",
      "\tReconstruction Loss: -0.8513364195823669\n",
      "\tDivergence Loss: 2.4454500675201416\n",
      "Domain 2, Epoch 7:\n",
      "\tReconstruction Loss: -1.432175874710083\n",
      "\tDivergence Loss: 2.2602298259735107\n",
      "Domain 2, Epoch 8:\n",
      "\tReconstruction Loss: -2.2230114936828613\n",
      "\tDivergence Loss: 2.9392647743225098\n",
      "Domain 2, Epoch 9:\n",
      "\tReconstruction Loss: -3.078900098800659\n",
      "\tDivergence Loss: 3.732088804244995\n",
      "Domain 2, Epoch 10:\n",
      "\tReconstruction Loss: -3.9518747329711914\n",
      "\tDivergence Loss: 4.559657573699951\n",
      "Domain 2, Epoch 11:\n",
      "\tReconstruction Loss: -4.722859859466553\n",
      "\tDivergence Loss: 5.337340354919434\n",
      "Domain 2, Epoch 12:\n",
      "\tReconstruction Loss: -5.482304573059082\n",
      "\tDivergence Loss: 6.053117275238037\n",
      "Domain 2, Epoch 13:\n",
      "\tReconstruction Loss: -5.9693427085876465\n",
      "\tDivergence Loss: 6.5674920082092285\n",
      "Domain 2, Epoch 14:\n",
      "\tReconstruction Loss: -6.363968849182129\n",
      "\tDivergence Loss: 6.959214687347412\n",
      "Domain 2, Epoch 15:\n",
      "\tReconstruction Loss: -6.630781173706055\n",
      "\tDivergence Loss: 7.14930534362793\n",
      "Domain 2, Epoch 16:\n",
      "\tReconstruction Loss: -6.4291534423828125\n",
      "\tDivergence Loss: 7.013162612915039\n",
      "Domain 2, Epoch 17:\n",
      "\tReconstruction Loss: -6.127137184143066\n",
      "\tDivergence Loss: 6.6821608543396\n",
      "Domain 2, Epoch 18:\n",
      "\tReconstruction Loss: -5.37764310836792\n",
      "\tDivergence Loss: 5.958710670471191\n",
      "Domain 2, Epoch 19:\n",
      "\tReconstruction Loss: -4.359457969665527\n",
      "\tDivergence Loss: 4.908965110778809\n",
      "Domain 2, Epoch 20:\n",
      "\tReconstruction Loss: -3.003417491912842\n",
      "\tDivergence Loss: 3.574096918106079\n",
      "Domain 2, Epoch 21:\n",
      "\tReconstruction Loss: -1.5290822982788086\n",
      "\tDivergence Loss: 2.1106879711151123\n",
      "Domain 2, Epoch 22:\n",
      "\tReconstruction Loss: -0.4124682545661926\n",
      "\tDivergence Loss: 0.9506831765174866\n",
      "Domain 2, Epoch 23:\n",
      "\tReconstruction Loss: 0.27609872817993164\n",
      "\tDivergence Loss: 0.32740041613578796\n",
      "Domain 2, Epoch 24:\n",
      "\tReconstruction Loss: 0.5618183612823486\n",
      "\tDivergence Loss: 0.2119763195514679\n",
      "Domain 2, Epoch 25:\n",
      "\tReconstruction Loss: 0.505070686340332\n",
      "\tDivergence Loss: 0.35817399621009827\n",
      "Domain 2, Epoch 26:\n",
      "\tReconstruction Loss: 0.5617510080337524\n",
      "\tDivergence Loss: 0.5269582271575928\n",
      "Domain 2, Epoch 27:\n",
      "\tReconstruction Loss: 0.5263632535934448\n",
      "\tDivergence Loss: 0.5777574777603149\n",
      "Domain 2, Epoch 28:\n",
      "\tReconstruction Loss: 0.567788302898407\n",
      "\tDivergence Loss: 0.4755750596523285\n",
      "Domain 2, Epoch 29:\n",
      "\tReconstruction Loss: 0.5376949310302734\n",
      "\tDivergence Loss: 0.31168943643569946\n",
      "Domain 2, Epoch 30:\n",
      "\tReconstruction Loss: 0.5490999817848206\n",
      "\tDivergence Loss: 0.1774337738752365\n",
      "Domain 2, Epoch 31:\n",
      "\tReconstruction Loss: 0.5489848852157593\n",
      "\tDivergence Loss: 0.10151291638612747\n",
      "Domain 2, Epoch 32:\n",
      "\tReconstruction Loss: 0.553984522819519\n",
      "\tDivergence Loss: 0.055832866579294205\n",
      "Domain 2, Epoch 33:\n",
      "\tReconstruction Loss: 0.525256872177124\n",
      "\tDivergence Loss: 0.0337294340133667\n",
      "Domain 2, Epoch 34:\n",
      "\tReconstruction Loss: 0.5403267741203308\n",
      "\tDivergence Loss: 0.021723423153162003\n",
      "Domain 2, Epoch 35:\n",
      "\tReconstruction Loss: 0.5462486147880554\n",
      "\tDivergence Loss: 0.014408670365810394\n",
      "Domain 2, Epoch 36:\n",
      "\tReconstruction Loss: 0.5467427968978882\n",
      "\tDivergence Loss: 0.009797170758247375\n",
      "Domain 2, Epoch 37:\n",
      "\tReconstruction Loss: 0.5740109086036682\n",
      "\tDivergence Loss: 0.007013543508946896\n",
      "Domain 2, Epoch 38:\n",
      "\tReconstruction Loss: 0.6144751310348511\n",
      "\tDivergence Loss: 0.00519880186766386\n",
      "Domain 2, Epoch 39:\n",
      "\tReconstruction Loss: 0.549992024898529\n",
      "\tDivergence Loss: 0.004155827686190605\n",
      "Domain 2, Epoch 40:\n",
      "\tReconstruction Loss: 0.5391892194747925\n",
      "\tDivergence Loss: 0.0033318910282105207\n",
      "Domain 2, Epoch 41:\n",
      "\tReconstruction Loss: 0.5699892640113831\n",
      "\tDivergence Loss: 0.0027636270970106125\n",
      "Domain 2, Epoch 42:\n",
      "\tReconstruction Loss: 0.5438384413719177\n",
      "\tDivergence Loss: 0.0023478465154767036\n",
      "Domain 2, Epoch 43:\n",
      "\tReconstruction Loss: 0.5455016493797302\n",
      "\tDivergence Loss: 0.002074716379866004\n",
      "Domain 2, Epoch 44:\n",
      "\tReconstruction Loss: 0.5467223525047302\n",
      "\tDivergence Loss: 0.0018139251042157412\n",
      "Domain 2, Epoch 45:\n",
      "\tReconstruction Loss: 0.5763189792633057\n",
      "\tDivergence Loss: 0.0016080359928309917\n",
      "Domain 2, Epoch 46:\n",
      "\tReconstruction Loss: 0.5636014342308044\n",
      "\tDivergence Loss: 0.0014782152138650417\n",
      "Domain 2, Epoch 47:\n",
      "\tReconstruction Loss: 0.5516091585159302\n",
      "\tDivergence Loss: 0.0013480328489094973\n",
      "Domain 2, Epoch 48:\n",
      "\tReconstruction Loss: 0.5233362913131714\n",
      "\tDivergence Loss: 0.0012763220584020019\n",
      "Domain 2, Epoch 49:\n",
      "\tReconstruction Loss: 0.5302972197532654\n",
      "\tDivergence Loss: 0.0011797851184383035\n",
      "Domain 2, Epoch 50:\n",
      "\tReconstruction Loss: 0.6012293696403503\n",
      "\tDivergence Loss: 0.00110756722278893\n",
      "Domain 2, Epoch 51:\n",
      "\tReconstruction Loss: 0.5751151442527771\n",
      "\tDivergence Loss: 0.0010560345835983753\n",
      "Domain 2, Epoch 52:\n",
      "\tReconstruction Loss: 0.5301663875579834\n",
      "\tDivergence Loss: 0.0010058390907943249\n",
      "Domain 2, Epoch 53:\n",
      "\tReconstruction Loss: 0.5721408724784851\n",
      "\tDivergence Loss: 0.0009592825081199408\n",
      "Domain 2, Epoch 54:\n",
      "\tReconstruction Loss: 0.5653480291366577\n",
      "\tDivergence Loss: 0.0009284340776503086\n",
      "Domain 2, Epoch 55:\n",
      "\tReconstruction Loss: 0.5039846897125244\n",
      "\tDivergence Loss: 0.000875369063578546\n",
      "Domain 2, Epoch 56:\n",
      "\tReconstruction Loss: 0.532987117767334\n",
      "\tDivergence Loss: 0.0008453628979623318\n",
      "Domain 2, Epoch 57:\n",
      "\tReconstruction Loss: 0.5441808700561523\n",
      "\tDivergence Loss: 0.0008223869954235852\n",
      "Domain 2, Epoch 58:\n",
      "\tReconstruction Loss: 0.5241892337799072\n",
      "\tDivergence Loss: 0.0008106215973384678\n",
      "Domain 2, Epoch 59:\n",
      "\tReconstruction Loss: 0.534972071647644\n",
      "\tDivergence Loss: 0.0007837722660042346\n",
      "Domain 2, Epoch 60:\n",
      "\tReconstruction Loss: 0.5475029349327087\n",
      "\tDivergence Loss: 0.0007682342547923326\n",
      "Domain 2, Epoch 61:\n",
      "\tReconstruction Loss: 0.5385157465934753\n",
      "\tDivergence Loss: 0.0007366472273133695\n",
      "Domain 2, Epoch 62:\n",
      "\tReconstruction Loss: 0.5774662494659424\n",
      "\tDivergence Loss: 0.0007315024267882109\n",
      "Domain 2, Epoch 63:\n",
      "\tReconstruction Loss: 0.5912479758262634\n",
      "\tDivergence Loss: 0.0007148174336180091\n",
      "Domain 2, Epoch 64:\n",
      "\tReconstruction Loss: 0.5065162181854248\n",
      "\tDivergence Loss: 0.0006961303297430277\n",
      "Domain 2, Epoch 65:\n",
      "\tReconstruction Loss: 0.5205060243606567\n",
      "\tDivergence Loss: 0.0006874373648315668\n",
      "Domain 2, Epoch 66:\n",
      "\tReconstruction Loss: 0.5401540398597717\n",
      "\tDivergence Loss: 0.0006855595274828374\n",
      "Domain 2, Epoch 67:\n",
      "\tReconstruction Loss: 0.5385199785232544\n",
      "\tDivergence Loss: 0.0006736868526786566\n",
      "Domain 2, Epoch 68:\n",
      "\tReconstruction Loss: 0.5501441955566406\n",
      "\tDivergence Loss: 0.000667258573230356\n",
      "Domain 2, Epoch 69:\n",
      "\tReconstruction Loss: 0.5733720660209656\n",
      "\tDivergence Loss: 0.0006562619237229228\n",
      "Domain 2, Epoch 70:\n",
      "\tReconstruction Loss: 0.5939380526542664\n",
      "\tDivergence Loss: 0.0006506602512672544\n",
      "Domain 2, Epoch 71:\n",
      "\tReconstruction Loss: 0.4945107102394104\n",
      "\tDivergence Loss: 0.0006336683873087168\n",
      "Domain 2, Epoch 72:\n",
      "\tReconstruction Loss: 0.5303440690040588\n",
      "\tDivergence Loss: 0.0006313556805253029\n",
      "Domain 2, Epoch 73:\n",
      "\tReconstruction Loss: 0.5167128443717957\n",
      "\tDivergence Loss: 0.0006294662598520517\n",
      "Domain 2, Epoch 74:\n",
      "\tReconstruction Loss: 0.5386761426925659\n",
      "\tDivergence Loss: 0.0006153195863589644\n",
      "Domain 2, Epoch 75:\n",
      "\tReconstruction Loss: 0.5328798294067383\n",
      "\tDivergence Loss: 0.0006149349501356483\n",
      "Domain 2, Epoch 76:\n",
      "\tReconstruction Loss: 0.5163205862045288\n",
      "\tDivergence Loss: 0.0006163191865198314\n",
      "Domain 2, Epoch 77:\n",
      "\tReconstruction Loss: 0.5255129933357239\n",
      "\tDivergence Loss: 0.0006029417272657156\n",
      "Domain 2, Epoch 78:\n",
      "\tReconstruction Loss: 0.553003191947937\n",
      "\tDivergence Loss: 0.0006000706925988197\n",
      "Domain 2, Epoch 79:\n",
      "\tReconstruction Loss: 0.5027690529823303\n",
      "\tDivergence Loss: 0.0005957824760116637\n",
      "Domain 2, Epoch 80:\n",
      "\tReconstruction Loss: 0.5548573136329651\n",
      "\tDivergence Loss: 0.0005926934536546469\n",
      "Domain 2, Epoch 81:\n",
      "\tReconstruction Loss: 0.5567765831947327\n",
      "\tDivergence Loss: 0.0005897666560485959\n",
      "Domain 2, Epoch 82:\n",
      "\tReconstruction Loss: 0.560582160949707\n",
      "\tDivergence Loss: 0.0005790533032268286\n",
      "Domain 2, Epoch 83:\n",
      "\tReconstruction Loss: 0.5296868681907654\n",
      "\tDivergence Loss: 0.0005841440870426595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain 2, Epoch 84:\n",
      "\tReconstruction Loss: 0.576836884021759\n",
      "\tDivergence Loss: 0.0005801574443466961\n",
      "Domain 2, Epoch 85:\n",
      "\tReconstruction Loss: 0.5370258688926697\n",
      "\tDivergence Loss: 0.0005668889498338103\n",
      "Domain 2, Epoch 86:\n",
      "\tReconstruction Loss: 0.5871444344520569\n",
      "\tDivergence Loss: 0.0005667031509801745\n",
      "Domain 2, Epoch 87:\n",
      "\tReconstruction Loss: 0.532485842704773\n",
      "\tDivergence Loss: 0.0005634379922412336\n",
      "Domain 2, Epoch 88:\n",
      "\tReconstruction Loss: 0.5096260905265808\n",
      "\tDivergence Loss: 0.0005592532106675208\n",
      "Domain 2, Epoch 89:\n",
      "\tReconstruction Loss: 0.5665640830993652\n",
      "\tDivergence Loss: 0.000562068191356957\n",
      "Domain 2, Epoch 90:\n",
      "\tReconstruction Loss: 0.5254238843917847\n",
      "\tDivergence Loss: 0.0005510277114808559\n",
      "Domain 2, Epoch 91:\n",
      "\tReconstruction Loss: 0.5253952741622925\n",
      "\tDivergence Loss: 0.0005468283197842538\n",
      "Domain 2, Epoch 92:\n",
      "\tReconstruction Loss: 0.5448578596115112\n",
      "\tDivergence Loss: 0.0005548103363253176\n",
      "Domain 2, Epoch 93:\n",
      "\tReconstruction Loss: 0.5414652228355408\n",
      "\tDivergence Loss: 0.0005565705359913409\n",
      "Domain 2, Epoch 94:\n",
      "\tReconstruction Loss: 0.5297790169715881\n",
      "\tDivergence Loss: 0.0005353162996470928\n",
      "Domain 2, Epoch 95:\n",
      "\tReconstruction Loss: 0.5082710981369019\n",
      "\tDivergence Loss: 0.000539000378921628\n",
      "Domain 2, Epoch 96:\n",
      "\tReconstruction Loss: 0.5700076222419739\n",
      "\tDivergence Loss: 0.0005396339111030102\n",
      "Domain 2, Epoch 97:\n",
      "\tReconstruction Loss: 0.5312889814376831\n",
      "\tDivergence Loss: 0.0005362860392779112\n",
      "Domain 2, Epoch 98:\n",
      "\tReconstruction Loss: 0.5219486355781555\n",
      "\tDivergence Loss: 0.0005288100219331682\n",
      "Domain 2, Epoch 99:\n",
      "\tReconstruction Loss: 0.5371231436729431\n",
      "\tDivergence Loss: 0.000521513749845326\n",
      "Domain 2, Epoch 100:\n",
      "\tReconstruction Loss: 0.5397422909736633\n",
      "\tDivergence Loss: 0.0005280881887301803\n"
     ]
    }
   ],
   "source": [
    "trainAutoencodersWithPz(samples, encoders, decoders, discriminator, num_samples, original_domains, epochs=num_epochs, weight=weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original First Sentence from 0 translated to 2 after Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 100)\n",
      "['washington', '—', 'although', 'although', 'although', 'the', 'discussion', 'once', 'short', 'short', 'because', 'short', 'short', 'short', 'short', 'short', 'short', 'short', 'short', '~?@_', 'trouble', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_', '~?@_']\n"
     ]
    }
   ],
   "source": [
    "translation = translate(seq, samples, encoders, decoders, 0, 2)\n",
    "vecSeqToSentence(translation[0,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = []\n",
    "for pub in publications:\n",
    "    contents.append(np.asarray(eval_df[eval_df['publication'] == pub]['content']))\n",
    "    \n",
    "for content in contents:\n",
    "    for seq in content:\n",
    "        seq.extend([end_token] * (max_seq_length - len(seq)))\n",
    "    \n",
    "contents = np.asarray(contents)\n",
    "samples = np.zeros(shape=(contents.shape[0], contents.shape[1], max_seq_length, word_dim))\n",
    "\n",
    "for i in range(contents.shape[0]):\n",
    "    for j in range(contents.shape[1]):\n",
    "        for k in range(max_seq_length):\n",
    "            samples[i, j, k, :] = word2vec.wv[contents[i, j][k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detok = TreebankWordDetokenizer()\n",
    "\n",
    "evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
    "                        max_n=4,\n",
    "                        limit_length=True,\n",
    "                        length_limit=100,\n",
    "                        length_limit_type='words',\n",
    "                        apply_avg=False,\n",
    "                        apply_best=True,\n",
    "                        alpha=0.5, # Default F1_score\n",
    "                        weight_factor=1.2,\n",
    "                        stemming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateOnArticles(articles, encoder, decoder):\n",
    "    translated = decoder(encoder(tf.convert_to_tensor(articles, dtype=tf.float32)))\n",
    "       \n",
    "    original_sentences = [vecSeqToSentence(tokens) for tokens in articles]\n",
    "    \n",
    "    translated_sentences = [vecSeqToSentence(tokens) for tokens in translated]\n",
    "    \n",
    "    scores = evaluator.get_scores(translated_sentences, original_sentences)\n",
    "    \n",
    "    for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
    "        print('\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * results['p'], 'R', 100.0 * results['r'], 'F1', 100.0 * results['f']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(articles, encoders, decoders):\n",
    "    \n",
    "    for i in range(len(selected_publications)):\n",
    "        for j in range(len(selected_publications)):\n",
    "            if (i != j):\n",
    "                pub1=publications[i]\n",
    "                pub2=publications[j]\n",
    "                #source_articles = articles_df.loc[articles_df['publication'] == pub1]['content'].tolist()\n",
    "                source_articles = articles[i]\n",
    "                \n",
    "                print(pub1,\"to\",pub2)\n",
    "                evaluateOnArticles(source_articles, encoders[i], decoders[j])\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(samples, encoders, decoders)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
