{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import pandas\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "import rouge\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LSTM, Activation, Bidirectional, Dropout, Input, concatenate, Reshape, TimeDistributed, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow.keras.backend as K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>publication</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[washington, —, congressional, republicans, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[after, the, bullet, shells, get, counted, ,, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[when, walt, disney, ’, s, “, bambi, ”, opened...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[death, may, be, the, great, equalizer, ,, but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[seoul, ,, south, korea, —, north, korea, ’, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47220</th>\n",
       "      <td>47220</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[bt, is, introducing, two, initiatives, to, he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47221</th>\n",
       "      <td>47221</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[computer, users, across, the, world, continue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47222</th>\n",
       "      <td>47222</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[a, new, european, directive, could, put, soft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47223</th>\n",
       "      <td>47223</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[the, man, making, sure, us, computer, network...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47224</th>\n",
       "      <td>47224</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[online, role, playing, games, are, time-consu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47225 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0     publication  \\\n",
       "0               0  New York Times   \n",
       "1               1  New York Times   \n",
       "2               2  New York Times   \n",
       "3               3  New York Times   \n",
       "4               4  New York Times   \n",
       "...           ...             ...   \n",
       "47220       47220        BBC_tech   \n",
       "47221       47221        BBC_tech   \n",
       "47222       47222        BBC_tech   \n",
       "47223       47223        BBC_tech   \n",
       "47224       47224        BBC_tech   \n",
       "\n",
       "                                                 content  \n",
       "0      [washington, —, congressional, republicans, ha...  \n",
       "1      [after, the, bullet, shells, get, counted, ,, ...  \n",
       "2      [when, walt, disney, ’, s, “, bambi, ”, opened...  \n",
       "3      [death, may, be, the, great, equalizer, ,, but...  \n",
       "4      [seoul, ,, south, korea, —, north, korea, ’, s...  \n",
       "...                                                  ...  \n",
       "47220  [bt, is, introducing, two, initiatives, to, he...  \n",
       "47221  [computer, users, across, the, world, continue...  \n",
       "47222  [a, new, european, directive, could, put, soft...  \n",
       "47223  [the, man, making, sure, us, computer, network...  \n",
       "47224  [online, role, playing, games, are, time-consu...  \n",
       "\n",
       "[47225 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('data/tokenized.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = list(data['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Relevant publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_publications = [\n",
    " 'Breitbart',\n",
    " 'CNN',\n",
    " 'New York Times',\n",
    " 'NPR',\n",
    " 'Fox News',\n",
    " 'Reuters']\n",
    "# selected_publications = [\n",
    "#  'Breitbart',\n",
    "#  'CNN',\n",
    "#  'New York Times']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fox News',\n",
       " 'Talking Points Memo',\n",
       " 'NPR',\n",
       " 'Washington Post',\n",
       " 'Vox',\n",
       " 'Breitbart',\n",
       " 'BBC_business',\n",
       " 'BBC_sport',\n",
       " 'New York Post',\n",
       " 'Guardian',\n",
       " 'CNN',\n",
       " 'BBC_entertainment',\n",
       " 'New York Times',\n",
       " 'BBC_politics',\n",
       " 'Buzzfeed News',\n",
       " 'Reuters',\n",
       " 'BBC_tech',\n",
       " 'Atlantic',\n",
       " 'National Review',\n",
       " 'Business Insider']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_publications = list(set(data['publication']))\n",
    "all_publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Breitbart', 'CNN', 'New York Times', 'NPR', 'Fox News', 'Reuters']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take only the contents from publications with >= 3000 samples.\n",
    "publications = [pub for pub in selected_publications if pub in all_publications and len(data[data['publication'] == pub]) >= 3000]\n",
    "publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = []\n",
    "for pub in publications:\n",
    "    contents.append(np.asarray(data[data['publication'] == pub]['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding with special Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = max([len(seq) for content in contents for seq in content])\n",
    "max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_token = '~?@_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for content in contents:\n",
    "    for seq in content:\n",
    "        seq.extend([end_token] * (max_seq_length - len(seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = max([len(seq) for content in contents for seq in content])\n",
    "max_seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word2vec = gensim.models.Word2Vec(all_sentences, min_count = 1,  \n",
    "                              size = word_dim, window = 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'congress' and 'senate' - CBOW :  0.7014481\n",
      "Cosine similarity between 'congress' and 'house' - CBOW :  0.53328997\n"
     ]
    }
   ],
   "source": [
    "print(\"Cosine similarity between 'congress' \" + \n",
    "               \"and 'senate' - CBOW : \", \n",
    "    word2vec.wv.similarity('congress', 'senate')) \n",
    "      \n",
    "print(\"Cosine similarity between 'congress' \" +\n",
    "                 \"and 'house' - CBOW : \", \n",
    "    word2vec.wv.similarity('congress', 'house')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01908938,  2.2382617 ,  1.5679946 ,  0.18174295,  1.4411974 ,\n",
       "        0.05849129, -0.16806208, -0.05064933,  1.5836372 ,  0.19780707,\n",
       "        1.9312146 , -1.169705  , -0.4192394 ,  3.2565627 ,  2.9242406 ,\n",
       "        1.0648195 , -0.96829355, -0.38669196,  0.09931379,  0.24896295,\n",
       "        0.806823  , -0.23498604,  0.7084418 ,  0.53460824,  0.95937777,\n",
       "        1.8593422 , -0.06212355, -3.3417654 ,  1.1501628 ,  1.3510945 ,\n",
       "       -1.2199992 , -0.02709086,  1.235247  , -2.4158645 , -1.2816494 ,\n",
       "        0.1912856 ,  0.78216887,  0.22785416,  0.44435647, -0.7260348 ,\n",
       "       -1.0604091 , -2.9260032 , -1.012452  ,  0.12891795,  1.9401742 ,\n",
       "        1.0403886 , -0.05906727, -1.0269846 ,  0.95364034, -1.3797822 ,\n",
       "        0.5287092 , -0.1904405 ,  0.2672017 ,  0.5391164 ,  0.31580284,\n",
       "       -2.7189655 ,  3.5047731 , -0.39237607,  0.30070394,  2.6292272 ,\n",
       "       -1.2809072 ,  2.7227366 , -2.3651323 , -1.4861227 , -0.8131642 ,\n",
       "        3.4957328 ,  1.1951219 , -1.7762244 ,  1.2734178 , -0.29470754,\n",
       "       -0.5446405 , -0.8678007 , -2.3366451 ,  0.57110566,  1.812025  ,\n",
       "       -0.5787527 , -0.7913567 ,  0.41737783,  1.2975111 , -0.7573825 ,\n",
       "        1.6732817 ,  0.6869955 ,  0.07467324,  1.1108347 ,  1.4290955 ,\n",
       "        0.23581176,  2.8212512 ,  4.3923264 ,  0.8690267 ,  3.1280222 ,\n",
       "       -0.10109463,  2.0582438 ,  2.3886378 ,  1.3456098 ,  0.8326411 ,\n",
       "       -0.5305554 ,  1.6210487 , -1.5034293 ,  1.2073183 ,  0.14315599],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv['congressional']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('congressional', 1.0000001192092896),\n",
       " ('congress', 0.6512422561645508),\n",
       " ('house', 0.6509404182434082),\n",
       " ('senate', 0.639055073261261),\n",
       " ('legislative', 0.6334035396575928),\n",
       " ('bipartisan', 0.6009743213653564),\n",
       " ('lawmakers', 0.5876467227935791),\n",
       " ('gop', 0.5824518799781799),\n",
       " ('judiciary', 0.5693702697753906),\n",
       " ('legislators', 0.5563080310821533)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.similar_by_vector(word2vec.wv['congressional'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = np.asarray(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.zeros(shape=(contents.shape[0], contents.shape[1], max_seq_length, word_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(contents.shape[0]):\n",
    "    for j in range(contents.shape[1]):\n",
    "        for k in range(max_seq_length):\n",
    "            samples[i, j, k, :] = word2vec.wv[contents[i, j][k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO use closest cosine distance to find output word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciton Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squareError(xTrue, xPred):\n",
    "    return K.square(xTrue - xPred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstructionLoss(sample, encoder, decoder, f_w, weight): # (L_1 from the paper)\n",
    "    return K.mean(squareError(sample, decoder(encoder(sample)))) + K.mean(weight*K.log(f_w(encoder(sample))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divergenceLoss(f_w, encoder, sample, z_j, n_j): # Mean of log f_w(E_theta_i(x_j)) + log (1-f_w(z_j, n_j)) from the paper (L_2).\n",
    "    return K.mean(K.log(f_w(encoder(sample)))) + K.mean(K.log(1 - f_w([z_j, n_j])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(data, domain, num_samples):\n",
    "    N = data.shape[1]\n",
    "    return tf.convert_to_tensor(data[domain, np.random.choice(N, num_samples, replace=True),:,:], dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently just doing a restriction to the last z variables, might want to do a matrix multiplication?\n",
    "# pi_Z from the paper. projects a latent distribution in (z, n) to z\n",
    "def projectZ(encoded):\n",
    "    return encoded[0] # take zs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projectN(encoded):\n",
    "    return encoded[1] # taek Ns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in two inputs, n and z, and outputs samples.\n",
    "def createDecoder(z_dims, n_dims, time_steps, output_dims):\n",
    "    # TODO MAYBE: Add in more regularization or different than dropout?\n",
    "\n",
    "    z_inputs = Input(shape=(z_dims,))\n",
    "    n_inputs = Input(shape=(n_dims,))\n",
    "    inputs = concatenate([z_inputs, n_inputs])\n",
    "#     # 150 is arbitrary rn...\n",
    "#     dense = Dense(150)(inputs)\n",
    "    dense = Dense(time_steps*output_dims)(inputs)\n",
    "    reshape = Reshape((time_steps, output_dims))(dense)\n",
    "    # TODO Reshape to enforce time_steps?\n",
    "    bilstm = Bidirectional(LSTM(32, activation='tanh', return_sequences=True))(reshape)\n",
    "    bilstm = Dropout(0.2)(bilstm)\n",
    "    bilstm = Bidirectional(LSTM(32, activation='tanh', return_sequences=False))(bilstm)\n",
    "    bilstm = Dropout(0.2)(bilstm)\n",
    "    \n",
    "    dense = Dense(time_steps*output_dims, activation='linear')(bilstm)\n",
    "    outputs = Reshape((time_steps, output_dims))(dense)\n",
    "    \n",
    "    model = Model(inputs=[z_inputs, n_inputs], outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEncoder(time_steps, input_num, z_dims, n_dims):\n",
    "    # TODO MAYBE: Add in more regularization or different than dropout?\n",
    "    inputs = Input(shape=(time_steps, input_num,))\n",
    "    bilstm = Bidirectional(LSTM(32, activation='tanh', return_sequences=True))(inputs)\n",
    "    bilstm = Dropout(0.2)(bilstm)\n",
    "    dense = Bidirectional(LSTM(32, activation='tanh', return_sequences=False))(bilstm)\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    z_output = Dense(z_dims, activation='linear')(dense)\n",
    "    n_output = Dense(n_dims, activation='linear')(dense)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=[z_output, n_output])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDiscriminator(z_dims, n_dims):\n",
    "    z_inputs = Input(shape=(z_dims,))\n",
    "    n_inputs = Input(shape=(n_dims,))\n",
    "    inputs = concatenate([z_inputs, n_inputs])\n",
    "    \n",
    "    # 150, 100 is arbitrary rn...\n",
    "    dense = Dense(150, activation='relu')(inputs)\n",
    "    dense = Dense(100, activation='relu')(dense)\n",
    "    output = Dense(1, activation='sigmoid')(dense)\n",
    "    \n",
    "    model = Model(inputs=[z_inputs, n_inputs], outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-4\n",
    "enc_optimizer = tf.keras.optimizers.Adam(lr)\n",
    "dec_optimizer = tf.keras.optimizers.Adam(lr)\n",
    "disc_optimizer = tf.keras.optimizers.Adam(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When $P_Z$ is known... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# k is num of domains.\n",
    "# encoders is a list of encoders.\n",
    "# decoders is list of decoders.\n",
    "# samples is a K x N x Timesteps x dim, array of samples, where the 0th index is the domain,\n",
    "# the 1th index is the # of the sample in that domain, 2th index is the # timesteps per sequence, 3th index is the #\n",
    "# of dimensions at each timestep\n",
    "# original_domains is a list of the original domains P_z was derived from.\n",
    "\n",
    "# Currently assuming P_Z is known. Must approximate P_Z first.\n",
    "def trainAutoencodersWithPz(samples, encoders, decoders, discriminator, num_samples, original_domains, epochs=10, weight=1.0):\n",
    "    N = samples.shape[1]\n",
    "    k = samples.shape[0]\n",
    "        \n",
    "    \n",
    "    for i in range(k):\n",
    "        if i not in original_domains:\n",
    "            original_domain = np.random.choice(original_domains)\n",
    "            encoder = encoders[i]\n",
    "            decoder = decoders[i]\n",
    "            original_encoder = encoders[original_domain]\n",
    "            epoch = 0\n",
    "            while(epoch < epochs): # TOOD: could also do until some convergence criteria.\n",
    "                p_Xi_samples = sample(samples, i, num_samples)\n",
    "                p_Z_samples = projectZ(original_encoder(sample(samples, original_domain, num_samples)))\n",
    "                p_Ni_samples = projectN(encoder(sample(samples, i, num_samples)))\n",
    "\n",
    "                with tf.GradientTape() as enc_tape, tf.GradientTape() as dec_tape, tf.GradientTape() as disc_tape:\n",
    "\n",
    "                    reconstruction_loss = reconstructionLoss(p_Xi_samples, encoder, decoder, discriminator, weight)\n",
    "\n",
    "                    # negative b/c gradient ascent.\n",
    "                    divergence_loss = -1 * divergenceLoss(discriminator, encoder, p_Xi_samples, p_Z_samples, p_Ni_samples)\n",
    "\n",
    "                gradients_of_encoder = enc_tape.gradient(reconstruction_loss, encoder.trainable_variables)\n",
    "                gradients_of_decoder = dec_tape.gradient(reconstruction_loss, decoder.trainable_variables)\n",
    "                gradients_of_discriminator = disc_tape.gradient(divergence_loss, discriminator.trainable_variables)\n",
    "\n",
    "\n",
    "                enc_optimizer.apply_gradients(zip(gradients_of_encoder, encoder.trainable_variables))\n",
    "                dec_optimizer.apply_gradients(zip(gradients_of_decoder, decoder.trainable_variables))\n",
    "                disc_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "                \n",
    "                print('Domain {}, Epoch {}:\\n\\tReconstruction Loss: {}\\n\\tDivergence Loss: {}'.format(i, epoch+1, reconstruction_loss, divergence_loss))\n",
    "                epoch+=1\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When $P_Z$ is unknown...\n",
    "\"A straight-forward approach for learning the latent distribution PZ is to train a regularized autoencoder on data from a\n",
    "single representative domain. However, such a representation could potentially capture variability that is specific to\n",
    "that one domain. To learn a more invariant latent representation, we propose the following extension of our autoencoder\n",
    "framework. The basic idea is to alternate between training\n",
    "multiple autoencoders until they agree on a latent representation that is effective for their respective domains. This is\n",
    "particularly relevant for applications to biology; for example, often one is interested in learning a latent representation\n",
    "that integrates all of the data modalities.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# k is num of domains.\n",
    "# encoders is a list of encoders.\n",
    "# decoders is list of decoders.\n",
    "# samples is a K x N x Timesteps x dim, array of samples, where the 0th index is the domain,\n",
    "# the 1th index is the # of the sample in that domain, 2th index is the # timesteps per sequence, 3th index is the #\n",
    "# of dimensions at each timestep\n",
    "# domains is a list of the domains we are currently training over.\n",
    "\n",
    "def trainAutoencodersInitial(samples, encoders, decoders, discriminator, num_samples, domains, epochs=10, weight=1.0):\n",
    "    N = samples.shape[1]\n",
    "    k = samples.shape[0]\n",
    "    \n",
    "    for i in domains:\n",
    "        encoder = encoders[i]\n",
    "        decoder = decoders[i]\n",
    "        for j in domains:\n",
    "            if i != j:\n",
    "                j_encoder = encoders[j]\n",
    "                epoch = 0\n",
    "                while(epoch < epochs): # TOOD: could also do until some convergence criteria.\n",
    "                    p_Xi_samples = sample(samples, i, num_samples)\n",
    "                    p_Zj_samples = projectZ(j_encoder(sample(samples, j, num_samples)))\n",
    "                    p_Ni_samples = projectN(encoder(sample(samples, i, num_samples)))\n",
    "\n",
    "                    with tf.GradientTape() as enc_tape, tf.GradientTape() as dec_tape, tf.GradientTape() as disc_tape:\n",
    "\n",
    "                        reconstruction_loss = reconstructionLoss(p_Xi_samples, encoder, decoder, discriminator, weight)\n",
    "#                         print(p_Xi_samples)\n",
    "\n",
    "                        # negative b/c gradient ascent.\n",
    "                        divergence_loss = -1 * divergenceLoss(discriminator, encoder, p_Xi_samples, p_Zj_samples, p_Ni_samples)\n",
    "#                         print(p_Zj_samples)\n",
    "#                         print(p_Ni_samples)\n",
    "                        \n",
    "                    gradients_of_encoder = enc_tape.gradient(reconstruction_loss, encoder.trainable_variables)\n",
    "                    gradients_of_decoder = dec_tape.gradient(reconstruction_loss, decoder.trainable_variables)\n",
    "                    gradients_of_discriminator = disc_tape.gradient(divergence_loss, discriminator.trainable_variables)\n",
    "            \n",
    "\n",
    "                    enc_optimizer.apply_gradients(zip(gradients_of_encoder, encoder.trainable_variables))\n",
    "                    dec_optimizer.apply_gradients(zip(gradients_of_decoder, decoder.trainable_variables))\n",
    "                    disc_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "                    \n",
    "                    print('Domain {}, Epoch {}:\\n\\tReconstruction Loss: {}\\n\\tDivergence Loss: {}'.format(i, epoch+1, reconstruction_loss, divergence_loss))\n",
    "                    epoch+=1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples is a K x N x Timesteps x dim, array of samples, where the 0th index is the domain,\n",
    "# the 1th index is the # of the sample in that domain, 2th index is the # timesteps per sequence, 3th index is the #\n",
    "# of dimensions at each timestep\n",
    "\n",
    "def initModel(samples, z_dims, n_dims):\n",
    "    \n",
    "    k = samples.shape[0]\n",
    "    N = samples.shape[1]\n",
    "    time_steps = samples.shape[2]\n",
    "    dim = samples.shape[3]\n",
    "    \n",
    "    \n",
    "    \n",
    "    discriminator = createDiscriminator(z_dims, n_dims)\n",
    "    \n",
    "    encoders = []\n",
    "    decoders = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        encoders.append(createEncoder(time_steps, dim, z_dims, n_dims))\n",
    "        decoders.append(createDecoder(z_dims, n_dims, time_steps, dim))\n",
    "    \n",
    "    return encoders, decoders, discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(start_sequences, samples, encoders, decoders, start_domain, end_domain):\n",
    "    N = samples.shape[1]\n",
    "    print(start_sequences.shape)\n",
    "    num_samples = start_sequences.shape[0]\n",
    "    \n",
    "    start_encoder = encoders[start_domain]\n",
    "    end_encoder = encoders[end_domain]\n",
    "    end_decoder = decoders[end_domain]\n",
    "    \n",
    "    z = projectZ(start_encoder(start_sequences))\n",
    "    n = projectN(end_encoder(sample(samples, end_domain, num_samples)))\n",
    "    \n",
    "    end_sequences = end_decoder([z, n])\n",
    "    return end_sequences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vecSeqToSentence(sequence):\n",
    "    sequence = K.eval(sequence)\n",
    "    sentence = []\n",
    "    for i in range(sequence.shape[0]):\n",
    "        word = sequence[i,:]\n",
    "#         print(word)\n",
    "#         print(word2vec.wv.similar_by_vector(word))\n",
    "        sentence.append(word2vec.wv.similar_by_vector(word)[0][0])\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dims = 20 # len(n)\n",
    "z_dims = 80 # len(Z)\n",
    "\n",
    "num_epochs = 100\n",
    "num_samples = 128\n",
    "\n",
    "original_domains = [0, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = tf.convert_to_tensor(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders, decoders, discriminator = initModel(samples, z_dims, n_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original First Sentence from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'on tuesday ’ s broadcast of cnn ’ s “ situation room , ” cnn senior washington correspondent jeff zeleny stated that chelsea manning ’ s transition from a man to a woman “ certainly played into ” president obama ’ s decision to commute manning ’ s sentence , and “ without that , it ’ s hard to imagine , i think , this president would have done that . ” zeleny said , “ i think a question that this president the white house will have to answer here — will answer , and i think it ’ s an important one , if — how much was the personal story of chelsea manning involved in this , because the outcry from the left was so strong on this . and she ’ s having a difficult time in federal prison , no question . but , to me , that is a central question here . without that , you have to wonder if the outcome would be the same . i think it might not be . ” he added , “ [ b ] ecause she transitioned from a man to a woman , i think all of that certainly played into this . ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(contents[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 307, 100)\n"
     ]
    }
   ],
   "source": [
    "seq = tf.convert_to_tensor(np.asarray([samples[0, 0, :, :]]), dtype=tf.float32)\n",
    "translation = translate(seq, samples, encoders, decoders, original_domains[0], original_domains[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original First Sentence from 0 translated to 1 before Training (Random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hard-line', 'spoil', 'ex-no', '672', 'droneship', 'borchelt', 'hyping', 'unexploited', '5se', 'ramstein', 'linguists', 'corse', 'aceveo', 'goos.', 'purchaser', '10.85m', \"'throughout\", 'atomics', 'conservatism', 'feel-good', 'detach', 'casual', 'fritsche', 'stelios', 'riling', 'contributed', 'nigerian-owned', 'dematha', 'rewrote', 'hotspots', 'mccririck', 'bershawn', 'comeygate', 'obsessives', 'concurrent', 'vital.', 'disclosure', 'bffs.', 'threat', 'huzzah', 'criminalist', '1707', 'scrummaging', 'well', 'abstinence', 'ukraine.', 'héberger', 'brolin', \"'gordon\", '105,000', 'sociability', 'unpacked', 'impudence', \"'step\", 'unevenly', 'moneybags', 'via', 'nadiya', 'vel', 'dameon', 'upticks', 'athwart', 'pakistan', 'yet', 'ta', 'rickrolled', 'carnations', \"'zero\", 'vogell', 'folks.', 'predetermined', 'cut-rate', 'mdf', 'irma', 'vast', 'forged', 'dramatic', 'mmqb', 'bride.', 'scrummaging', 'jeez', 'intramural', 'pavithra', 'shredders', 'boudreaux', 'promulgate', 'kirschner', 'guinn', 'upstairs.', 'farther', '35‐minute', '55+', 'serious', 'capsizing', 'irrationally', 'tlatest', 'schnoz', 'convened', 'rosa', 'reservations', 'decluttering', 'coulee', 'fte', 'wrench', 'gentility', 'ladawn', 'shafi', 'malpensa', 'trumplaw', 'appearance.', 'bed.', 'troyekurovskoye', 'barter', 'acpo', 'sh***y', 'liveuamap', 'thing', 'weblinks', 'signature', 'harvie', 'homilies', 'spell.', 'demoralised', \"'unusual\", 'guaiana', 'globovision', 'chain.', 'warblier', 'fairstein', 'megastars', 'gelobeter', '1150', 'irfu', 'barrage', 'subarctic', \"'pretty\", 'trifling', 'limply', 'matchbox', 'brinker', 'whaddya', 'people', 'marginals', 'naturals', \"'wal-mart\", 'secondary', 'twin-engined', 'okay', 'precaution.', 'politesse', 'binyali', 'zippered', 'mendis', 'sensationalist', 'keizai', 'techtronic', 'affords', 'willing', 'masquerades', 'edwardsville', 'ft', 'level', 'flt', 'head-on', '£1.2bn', '..money', '“', 'page-turners', 'fareri', 'his', 'flipside', 'blondes.', 'gibney', 'non-specialist', 'arguable', 'delude', 'front.', 'cronycapitalism.', 'households', 'trans', 'saidpreyed', '10-minute', 'gunnell', 'brotherhood', 'fabello', 'reupholstered', 'enchilada', 'recess', 'hangout', 'tatman', 'furiani', 'churchy', 'caliphates', 'springwatch', 'suvs.', 'stopgunviolence', 'gangly', 'orientalis', 'parliament', 'arguably', '79p', 'profiles.', 'abbottadad', 'springwood', 'cilvegozu', 'cultivate', '152\\u200b', 'u', 'eye-popping', \"'last\", 'garcia', 'guff', 'lunching', 'errs', 'murk', 'peregrine', ':', 'broughton', 'module', 'radicchio', 'unalterably', 'koalas', 'wrrc', 'wide.', 'tebbit', '3.4bn', 'supernumerary', 'superhost', 'denote', 'multisyllabic', 'incalculable', 'hiphop', 'overpromising', 'evertrump', 'umbilically', 'ajit', '4.5m', 'valorous', \"'morally\", \"'otherwise\", 'leftist', 'stringent', 'nanping', 'digest', 'clinics', 'kpho', 'baume', '4.88m', 'hinckley', 'enbridge', 'busier', 'sates', 'look', 'walking.', 'galston', '£200m', 'liveris', 'inappropriatene', 'minutely', 'important', 'whtc', 'bbdo', 'tencentcorp', 'schedule', '升平', 'mr.', 'overspending', 'proceeded', 'sorenson', 'remains', 'monk.', 'oktogon', 'hexel', 'surveys', 'ukraine.', 'smart-card', 'bibi.', 'smdh', 'armey', 'applauds', 'myopia', 'texas', '467', 'mobilisation', 'wootton', 'gaisford', 'guardianships', 'teriyaki', 'hash', 'retina', 'lomax', 'abstinence', 'apoyemos', 'consult', 'pointlessly', 'acheampong', 'de-facto', 'bashings', 'bloodhounds', 'batty', 'clarifications', 'amputee', 'lawmany', 'wtvq', 'became', 'snuffing', 'prime-time']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vecSeqToSentence(translation[0,:,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain 0, Epoch 1:\n",
      "\tReconstruction Loss: 1.1809301376342773\n",
      "\tDivergence Loss: 1.3826854228973389\n",
      "Domain 0, Epoch 2:\n",
      "\tReconstruction Loss: 1.151749610900879\n",
      "\tDivergence Loss: 1.3782031536102295\n",
      "Domain 0, Epoch 3:\n",
      "\tReconstruction Loss: 1.1627626419067383\n",
      "\tDivergence Loss: 1.3760323524475098\n",
      "Domain 0, Epoch 4:\n",
      "\tReconstruction Loss: 1.1594337224960327\n",
      "\tDivergence Loss: 1.370973825454712\n",
      "Domain 0, Epoch 5:\n",
      "\tReconstruction Loss: 1.1269513368606567\n",
      "\tDivergence Loss: 1.3562631607055664\n",
      "Domain 0, Epoch 6:\n",
      "\tReconstruction Loss: 1.1555417776107788\n",
      "\tDivergence Loss: 1.3459144830703735\n",
      "Domain 0, Epoch 7:\n",
      "\tReconstruction Loss: 1.1088639497756958\n",
      "\tDivergence Loss: 1.3323426246643066\n",
      "Domain 0, Epoch 8:\n",
      "\tReconstruction Loss: 1.1226344108581543\n",
      "\tDivergence Loss: 1.319995403289795\n",
      "Domain 0, Epoch 9:\n",
      "\tReconstruction Loss: 1.130688190460205\n",
      "\tDivergence Loss: 1.298516035079956\n",
      "Domain 0, Epoch 10:\n",
      "\tReconstruction Loss: 1.114145278930664\n",
      "\tDivergence Loss: 1.274233102798462\n",
      "Domain 0, Epoch 11:\n",
      "\tReconstruction Loss: 1.100257158279419\n",
      "\tDivergence Loss: 1.259047031402588\n"
     ]
    }
   ],
   "source": [
    "trainAutoencodersInitial(samples, encoders, decoders, discriminator, num_samples, original_domains, epochs=num_epochs, weight=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original First Sentence from 0 translated to 1 after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = translate(seq, samples, encoders, decoders, original_domains[0], original_domains[1])\n",
    "vecSeqToSentence(translation[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAutoencodersWithPz(samples, encoders, decoders, discriminator, num_samples, original_domains, epochs=num_epochs, weight=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original First Sentence from 0 translated to 2 after Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = translate(seq, samples, encoders, decoders, 0, 2)\n",
    "vecSeqToSentence(translation[0,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/evaluation.pkl')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = []\n",
    "for pub in publications:\n",
    "    contents.append(np.asarray(df[df['publication'] == pub]['content']))\n",
    "    \n",
    "for content in contents:\n",
    "    for seq in content:\n",
    "        seq.extend([end_token] * (max_seq_length - len(seq)))\n",
    "    \n",
    "contents = np.asarray(contents)\n",
    "samples = np.zeros(shape=(contents.shape[0], contents.shape[1], max_seq_length, word_dim))\n",
    "\n",
    "for i in range(contents.shape[0]):\n",
    "    for j in range(contents.shape[1]):\n",
    "        for k in range(max_seq_length):\n",
    "            try:\n",
    "                samples[i, j, k, :] = word2vec.wv[contents[i, j][k]]\n",
    "            except KeyError as e:\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detok = TreebankWordDetokenizer()\n",
    "\n",
    "evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
    "                        max_n=4,\n",
    "                        limit_length=True,\n",
    "                        length_limit=100,\n",
    "                        length_limit_type='words',\n",
    "                        apply_avg=False,\n",
    "                        apply_best=True,\n",
    "                        alpha=0.5, # Default F1_score\n",
    "                        weight_factor=1.2,\n",
    "                        stemming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateOnArticles(articles, encoder, decoder):\n",
    "    translated = decoder(encoder(tf.convert_to_tensor(articles, dtype=tf.float32)))\n",
    "       \n",
    "    original_sentences = [vecSeqToSentence(tokens) for tokens in articles]\n",
    "    \n",
    "    translated_sentences = [vecSeqToSentence(tokens) for tokens in translated]\n",
    "    \n",
    "    scores = evaluator.get_scores(translated_sentences, original_sentences)\n",
    "    \n",
    "    for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
    "        print('\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * results['p'], 'R', 100.0 * results['r'], 'F1', 100.0 * results['f']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(articles, encoders, decoders):\n",
    "    \n",
    "    for i in range(len(selected_publications)):\n",
    "        for j in range(len(selected_publications)):\n",
    "            if (i != j):\n",
    "                pub1=publications[i]\n",
    "                pub2=publications[j]\n",
    "                #source_articles = articles_df.loc[articles_df['publication'] == pub1]['content'].tolist()\n",
    "                source_articles = articles[i]\n",
    "                \n",
    "                print(pub1,\"to\",pub2)\n",
    "                evaluateOnArticles(source_articles, encoders[i], decoders[j])\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(samples, encoders, decoders)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
