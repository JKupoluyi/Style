{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import pandas\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "import rouge\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LSTM, Activation, Bidirectional, Dropout, Input, concatenate, Reshape, TimeDistributed, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow.keras.backend as K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>publication</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[WASHINGTON, —, Congressional, Republicans, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[After, the, bullet, shells, get, counted, ,, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[When, Walt, Disney, ’, s, “, Bambi, ”, opened...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[Death, may, be, the, great, equalizer, ,, but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[SEOUL, ,, South, Korea, —, North, Korea, ’, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47220</th>\n",
       "      <td>47220</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[BT, is, introducing, two, initiatives, to, he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47221</th>\n",
       "      <td>47221</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[Computer, users, across, the, world, continue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47222</th>\n",
       "      <td>47222</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[A, new, European, directive, could, put, soft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47223</th>\n",
       "      <td>47223</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[The, man, making, sure, US, computer, network...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47224</th>\n",
       "      <td>47224</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[Online, role, playing, games, are, time-consu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47225 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0     publication  \\\n",
       "0               0  New York Times   \n",
       "1               1  New York Times   \n",
       "2               2  New York Times   \n",
       "3               3  New York Times   \n",
       "4               4  New York Times   \n",
       "...           ...             ...   \n",
       "47220       47220        BBC_tech   \n",
       "47221       47221        BBC_tech   \n",
       "47222       47222        BBC_tech   \n",
       "47223       47223        BBC_tech   \n",
       "47224       47224        BBC_tech   \n",
       "\n",
       "                                                 content  \n",
       "0      [WASHINGTON, —, Congressional, Republicans, ha...  \n",
       "1      [After, the, bullet, shells, get, counted, ,, ...  \n",
       "2      [When, Walt, Disney, ’, s, “, Bambi, ”, opened...  \n",
       "3      [Death, may, be, the, great, equalizer, ,, but...  \n",
       "4      [SEOUL, ,, South, Korea, —, North, Korea, ’, s...  \n",
       "...                                                  ...  \n",
       "47220  [BT, is, introducing, two, initiatives, to, he...  \n",
       "47221  [Computer, users, across, the, world, continue...  \n",
       "47222  [A, new, European, directive, could, put, soft...  \n",
       "47223  [The, man, making, sure, US, computer, network...  \n",
       "47224  [Online, role, playing, games, are, time-consu...  \n",
       "\n",
       "[47225 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('data/tokenized.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = list(data['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Relevant publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_publications = [\n",
    "#  'Breitbart',\n",
    "#  'CNN',\n",
    "#  'New York Times',\n",
    "#  'NPR',\n",
    "#  'Fox News',\n",
    "#  'Reuters']\n",
    "selected_publications = [\n",
    " 'Breitbart',\n",
    " 'CNN',\n",
    " 'New York Times']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Atlantic',\n",
       " 'Talking Points Memo',\n",
       " 'Buzzfeed News',\n",
       " 'New York Times',\n",
       " 'National Review',\n",
       " 'BBC_sport',\n",
       " 'BBC_tech',\n",
       " 'Vox',\n",
       " 'Washington Post',\n",
       " 'CNN',\n",
       " 'BBC_entertainment',\n",
       " 'Business Insider',\n",
       " 'Breitbart',\n",
       " 'New York Post',\n",
       " 'Fox News',\n",
       " 'Guardian',\n",
       " 'BBC_politics',\n",
       " 'NPR',\n",
       " 'BBC_business',\n",
       " 'Reuters']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_publications = list(set(data['publication']))\n",
    "all_publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['New York Times', 'CNN', 'Breitbart']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take only the contents from publications with >= 3000 samples.\n",
    "publications = [pub for pub in all_publications if len(data[data['publication'] == pub]) >= 3000 and pub in selected_publications]\n",
    "publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = []\n",
    "for pub in publications:\n",
    "    contents.append(np.asarray(data[data['publication'] == pub]['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding with special Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = max([len(seq) for content in contents for seq in content])\n",
    "max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_token = '~?@_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for content in contents:\n",
    "    for seq in content:\n",
    "        seq.extend([end_token] * (max_seq_length - len(seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = max([len(seq) for content in contents for seq in content])\n",
    "max_seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word2vec = gensim.models.Word2Vec(all_sentences, min_count = 1,  \n",
    "                              size = word_dim, window = 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'congress' and 'senate' - CBOW :  0.5585541\n",
      "Cosine similarity between 'congress' and 'house' - CBOW :  0.20196319\n"
     ]
    }
   ],
   "source": [
    "print(\"Cosine similarity between 'congress' \" + \n",
    "               \"and 'senate' - CBOW : \", \n",
    "    word2vec.wv.similarity('congress', 'senate')) \n",
    "      \n",
    "print(\"Cosine similarity between 'congress' \" +\n",
    "                 \"and 'house' - CBOW : \", \n",
    "    word2vec.wv.similarity('congress', 'house')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.81409   , -1.8259698 , -0.5213424 , -1.0039812 , -1.1219344 ,\n",
       "       -1.1313971 ,  1.053734  , -2.0641692 ,  0.08805798, -1.3682878 ,\n",
       "       -1.0811073 , -1.4631051 , -0.7301839 ,  0.65787923,  0.07922222,\n",
       "        0.11877885, -1.6522729 , -1.4045465 , -1.2127597 ,  1.4468809 ,\n",
       "       -0.9213013 , -0.24367768, -2.4355125 ,  1.3575794 ,  0.0561852 ,\n",
       "       -0.24700265,  0.4540977 ,  2.4848862 ,  1.3756186 , -0.47934413,\n",
       "       -1.4292005 , -0.05118076,  0.5859316 , -1.1900172 , -0.52885246,\n",
       "       -1.2477894 ,  1.9266349 ,  1.2263732 ,  0.91029304,  1.9951388 ,\n",
       "        1.9347837 ,  1.3460112 , -1.2425102 ,  0.5068862 , -0.25383335,\n",
       "       -0.6194678 ,  2.034936  ,  1.4890639 ,  0.3475387 ,  1.3723586 ,\n",
       "       -1.1544163 ,  0.41810307, -0.36372223,  3.263224  ,  0.8378256 ,\n",
       "        0.03856661,  1.5172393 , -0.5618742 , -0.01595489, -0.18518563,\n",
       "       -0.19397275,  0.8147149 , -0.46721897, -1.6012917 ,  0.53174263,\n",
       "        1.0769796 , -0.46443713, -2.0040765 ,  0.23908596, -1.564936  ,\n",
       "        0.7723342 , -0.35974205,  0.9332044 ,  0.76417357,  0.7028127 ,\n",
       "        0.13883607,  0.1149272 , -0.01184198,  1.7328638 ,  1.2702023 ,\n",
       "       -0.7359761 , -0.49439394, -1.4111449 ,  3.504295  , -1.243134  ,\n",
       "        1.0180775 , -1.0962378 , -0.25298807,  0.91563785, -0.35120478,\n",
       "       -1.7345085 , -0.82282627, -0.3624143 , -0.4747685 ,  0.7673772 ,\n",
       "        0.71842307,  1.4597864 , -2.0088522 ,  0.9894665 , -0.8360592 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv['Congressional']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Congressional', 0.9999998807907104),\n",
       " ('Budget', 0.8419904708862305),\n",
       " ('Ethics', 0.7599501609802246),\n",
       " ('Government', 0.7324299812316895),\n",
       " ('Fairness', 0.7036246657371521),\n",
       " ('Accountability', 0.6974054574966431),\n",
       " ('Information', 0.6918414831161499),\n",
       " ('Freedom', 0.6913105249404907),\n",
       " ('Services', 0.6477600932121277),\n",
       " ('Relations', 0.6472127437591553)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.similar_by_vector(word2vec.wv['Congressional'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = np.asarray(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.zeros(shape=(contents.shape[0], contents.shape[1], max_seq_length, word_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(contents.shape[0]):\n",
    "    for j in range(contents.shape[1]):\n",
    "        for k in range(max_seq_length):\n",
    "            samples[i, j, k, :] = word2vec.wv[contents[i, j][k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO use closest cosine distance to find output word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciton Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squareError(xTrue, xPred):\n",
    "    return K.square(xTrue - xPred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstructionLoss(sample, encoder, decoder, f_w, weight): # (L_1 from the paper)\n",
    "    return K.mean(squareError(sample, decoder(encoder(sample)))) + K.mean(weight*K.log(f_w(encoder(sample))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divergenceLoss(f_w, encoder, sample, z_j, n_j): # Mean of log f_w(E_theta_i(x_j)) + log (1-f_w(z_j, n_j)) from the paper (L_2).\n",
    "    return K.mean(K.log(f_w(encoder(sample)))) + K.mean(K.log(1 - f_w([z_j, n_j])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(data, domain, num_samples):\n",
    "    N = data.shape[1]\n",
    "    return tf.convert_to_tensor(data[domain, np.random.choice(N, num_samples, replace=True),:,:], dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently just doing a restriction to the last z variables, might want to do a matrix multiplication?\n",
    "# pi_Z from the paper. projects a latent distribution in (z, n) to z\n",
    "def projectZ(encoded):\n",
    "    return encoded[0] # take zs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projectN(encoded):\n",
    "    return encoded[1] # taek Ns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in two inputs, n and z, and outputs samples.\n",
    "def createDecoder(z_dims, n_dims, time_steps, output_dims):\n",
    "    # TODO MAYBE: Add in more regularization or different than dropout?\n",
    "\n",
    "    z_inputs = Input(shape=(z_dims,))\n",
    "    n_inputs = Input(shape=(n_dims,))\n",
    "    inputs = concatenate([z_inputs, n_inputs])\n",
    "#     # 150 is arbitrary rn...\n",
    "#     dense = Dense(150)(inputs)\n",
    "    dense = Dense(time_steps*output_dims)(inputs)\n",
    "    reshape = Reshape((time_steps, output_dims))(dense)\n",
    "    # TODO Reshape to enforce time_steps?\n",
    "    bilstm = Bidirectional(LSTM(32, activation='tanh', return_sequences=True))(reshape)\n",
    "    bilstm = Dropout(0.2)(bilstm)\n",
    "    bilstm = Bidirectional(LSTM(32, activation='tanh', return_sequences=False))(bilstm)\n",
    "    \n",
    "    dense = Dense(time_steps*output_dims, activation='linear')(bilstm)\n",
    "    outputs = Reshape((time_steps, output_dims))(dense)\n",
    "    \n",
    "    model = Model(inputs=[z_inputs, n_inputs], outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEncoder(time_steps, input_num, z_dims, n_dims):\n",
    "    # TODO MAYBE: Add in more regularization or different than dropout?\n",
    "    inputs = Input(shape=(time_steps, input_num,))\n",
    "    bilstm = Bidirectional(LSTM(32, activation='tanh', return_sequences=True))(inputs)\n",
    "    bilstm = Dropout(0.2)(bilstm)\n",
    "    dense = Bidirectional(LSTM(32, activation='tanh', return_sequences=False))(bilstm)\n",
    "    dense = Dropout(0.5)(dense)\n",
    "    z_output = Dense(z_dims, activation='linear')(dense)\n",
    "    n_output = Dense(n_dims, activation='linear')(dense)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=[z_output, n_output])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDiscriminator(z_dims, n_dims):\n",
    "    z_inputs = Input(shape=(z_dims,))\n",
    "    n_inputs = Input(shape=(n_dims,))\n",
    "    inputs = concatenate([z_inputs, n_inputs])\n",
    "    \n",
    "    # 150, 100 is arbitrary rn...\n",
    "    dense = Dense(150, activation='relu')(inputs)\n",
    "    dense = Dense(100, activation='relu')(dense)\n",
    "    output = Dense(1, activation='sigmoid')(dense)\n",
    "    \n",
    "    model = Model(inputs=[z_inputs, n_inputs], outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-4\n",
    "enc_optimizer = tf.keras.optimizers.Adam(lr)\n",
    "dec_optimizer = tf.keras.optimizers.Adam(lr)\n",
    "disc_optimizer = tf.keras.optimizers.Adam(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When $P_Z$ is known... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# k is num of domains.\n",
    "# encoders is a list of encoders.\n",
    "# decoders is list of decoders.\n",
    "# samples is a K x N x Timesteps x dim, array of samples, where the 0th index is the domain,\n",
    "# the 1th index is the # of the sample in that domain, 2th index is the # timesteps per sequence, 3th index is the #\n",
    "# of dimensions at each timestep\n",
    "# original_domains is a list of the original domains P_z was derived from.\n",
    "\n",
    "# Currently assuming P_Z is known. Must approximate P_Z first.\n",
    "def trainAutoencodersWithPz(samples, encoders, decoders, discriminator, num_samples, original_domains, epochs, weight=1.0):\n",
    "    N = samples.shape[1]\n",
    "    k = samples.shape[0]\n",
    "        \n",
    "    \n",
    "    for i in range(k):\n",
    "        if i not in original_domains:\n",
    "            original_domain = np.random.choice(original_domains)\n",
    "            encoder = encoders[i]\n",
    "            decoder = decoders[i]\n",
    "            original_encoder = encoders[original_domain]\n",
    "            epoch = 0\n",
    "            while(epoch < epochs): # TOOD: could also do until some convergence criteria.\n",
    "                p_Xi_samples = sample(samples, i, num_samples)\n",
    "                p_Z_samples = projectZ(original_encoder(sample(samples, original_domain, num_samples)))\n",
    "                p_Ni_samples = projectN(encoder(sample(samples, i, num_samples)))\n",
    "\n",
    "                with tf.GradientTape() as enc_tape, tf.GradientTape() as dec_tape, tf.GradientTape() as disc_tape:\n",
    "\n",
    "                    reconstruction_loss = reconstructionLoss(p_Xi_samples, encoder, decoder, discriminator, weight)\n",
    "\n",
    "                    # negative b/c gradient ascent.\n",
    "                    divergence_loss = -1 * divergenceLoss(discriminator, encoder, p_Xi_samples, p_Z_samples, p_Ni_samples)\n",
    "\n",
    "                gradients_of_encoder = enc_tape.gradient(reconstruction_loss, encoder.trainable_variables)\n",
    "                gradients_of_decoder = dec_tape.gradient(reconstruction_loss, decoder.trainable_variables)\n",
    "                gradients_of_discriminator = disc_tape.gradient(divergence_loss, discriminator.trainable_variables)\n",
    "\n",
    "\n",
    "                enc_optimizer.apply_gradients(zip(gradients_of_encoder, encoder.trainable_variables))\n",
    "                dec_optimizer.apply_gradients(zip(gradients_of_decoder, decoder.trainable_variables))\n",
    "                disc_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "                \n",
    "                print('Domain {}, Epoch {}:\\n\\tReconstruction Loss: {}\\n\\tDivergence Loss: {}'.format(i, epoch+1, reconstruction_loss, divergence_loss))\n",
    "                epoch+=1\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When $P_Z$ is unknown...\n",
    "\"A straight-forward approach for learning the latent distribution PZ is to train a regularized autoencoder on data from a\n",
    "single representative domain. However, such a representation could potentially capture variability that is specific to\n",
    "that one domain. To learn a more invariant latent representation, we propose the following extension of our autoencoder\n",
    "framework. The basic idea is to alternate between training\n",
    "multiple autoencoders until they agree on a latent representation that is effective for their respective domains. This is\n",
    "particularly relevant for applications to biology; for example, often one is interested in learning a latent representation\n",
    "that integrates all of the data modalities.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# k is num of domains.\n",
    "# encoders is a list of encoders.\n",
    "# decoders is list of decoders.\n",
    "# samples is a K x N x Timesteps x dim, array of samples, where the 0th index is the domain,\n",
    "# the 1th index is the # of the sample in that domain, 2th index is the # timesteps per sequence, 3th index is the #\n",
    "# of dimensions at each timestep\n",
    "# domains is a list of the domains we are currently training over.\n",
    "\n",
    "def trainAutoencodersInitial(samples, encoders, decoders, discriminator, num_samples, domains, epochs, weight=1.0):\n",
    "    N = samples.shape[1]\n",
    "    k = samples.shape[0]\n",
    "    \n",
    "    for i in domains:\n",
    "        encoder = encoders[i]\n",
    "        decoder = decoders[i]\n",
    "        for j in domains:\n",
    "            if i != j:\n",
    "                j_encoder = encoders[j]\n",
    "                epoch = 0\n",
    "                while(epoch < epochs): # TOOD: could also do until some convergence criteria.\n",
    "                    p_Xi_samples = sample(samples, i, num_samples)\n",
    "                    p_Zj_samples = projectZ(j_encoder(sample(samples, j, num_samples)))\n",
    "                    p_Ni_samples = projectN(encoder(sample(samples, i, num_samples)))\n",
    "\n",
    "                    with tf.GradientTape() as enc_tape, tf.GradientTape() as dec_tape, tf.GradientTape() as disc_tape:\n",
    "\n",
    "                        reconstruction_loss = reconstructionLoss(p_Xi_samples, encoder, decoder, discriminator, weight)\n",
    "#                         print(p_Xi_samples)\n",
    "\n",
    "                        # negative b/c gradient ascent.\n",
    "                        divergence_loss = -1 * divergenceLoss(discriminator, encoder, p_Xi_samples, p_Zj_samples, p_Ni_samples)\n",
    "#                         print(p_Zj_samples)\n",
    "#                         print(p_Ni_samples)\n",
    "                        \n",
    "                    gradients_of_encoder = enc_tape.gradient(reconstruction_loss, encoder.trainable_variables)\n",
    "                    gradients_of_decoder = dec_tape.gradient(reconstruction_loss, decoder.trainable_variables)\n",
    "                    gradients_of_discriminator = disc_tape.gradient(divergence_loss, discriminator.trainable_variables)\n",
    "            \n",
    "\n",
    "                    enc_optimizer.apply_gradients(zip(gradients_of_encoder, encoder.trainable_variables))\n",
    "                    dec_optimizer.apply_gradients(zip(gradients_of_decoder, decoder.trainable_variables))\n",
    "                    disc_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "                    \n",
    "                    print('Domain {}, Epoch {}:\\n\\tReconstruction Loss: {}\\n\\tDivergence Loss: {}'.format(i, epoch+1, reconstruction_loss, divergence_loss))\n",
    "                    epoch+=1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples is a K x N x Timesteps x dim, array of samples, where the 0th index is the domain,\n",
    "# the 1th index is the # of the sample in that domain, 2th index is the # timesteps per sequence, 3th index is the #\n",
    "# of dimensions at each timestep\n",
    "\n",
    "def initModel(samples, z_dims, n_dims):\n",
    "    \n",
    "    k = samples.shape[0]\n",
    "    N = samples.shape[1]\n",
    "    time_steps = samples.shape[2]\n",
    "    dim = samples.shape[3]\n",
    "    \n",
    "    \n",
    "    \n",
    "    discriminator = createDiscriminator(z_dims, n_dims)\n",
    "    \n",
    "    encoders = []\n",
    "    decoders = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        encoders.append(createEncoder(time_steps, dim, z_dims, n_dims))\n",
    "        decoders.append(createDecoder(z_dims, n_dims, time_steps, dim))\n",
    "    \n",
    "    return encoders, decoders, discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(start_sequences, samples, encoders, decoders, start_domain, end_domain):\n",
    "    N = samples.shape[1]\n",
    "    print(start_sequences.shape)\n",
    "    num_samples = start_sequences.shape[0]\n",
    "    \n",
    "    start_encoder = encoders[start_domain]\n",
    "    end_encoder = encoders[end_domain]\n",
    "    end_decoder = decoders[end_domain]\n",
    "    \n",
    "    z = projectZ(start_encoder(start_sequences))\n",
    "    n = projectN(end_encoder(sample(samples, end_domain, num_samples)))\n",
    "    \n",
    "    end_sequences = end_decoder([z, n])\n",
    "    return end_sequences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vecSeqToSentence(sequence):\n",
    "    sequence = K.eval(sequence)\n",
    "    sentence = []\n",
    "    for i in range(sequence.shape[0]):\n",
    "        word = sequence[i,:]\n",
    "#         print(word)\n",
    "#         print(word2vec.wv.similar_by_vector(word))\n",
    "        sentence.append(word2vec.wv.similar_by_vector(word)[0][0])\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dims = 20 # len(n)\n",
    "z_dims = 80 # len(Z)\n",
    "\n",
    "num_epochs = 1\n",
    "num_samples = 128\n",
    "\n",
    "original_domains = [0, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = tf.convert_to_tensor(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders, decoders, discriminator = initModel(samples, z_dims, n_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original First Sentence from NYT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WASHINGTON — Congressional Republicans have a new fear when it comes to their health care lawsuit against the Obama administration : They might win . The incoming Trump administration could choose to no longer defend the executive branch against the suit , which challenges the administration ’ s authority to spend billions of dollars on health insurance subsidies for and Americans , handing House Republicans a big victory on issues . But a sudden loss of the disputed subsidies could conceivably cause the health care program to implode , leaving millions of people without access to health insurance before Republicans have prepared a replacement . That could lead to chaos in the insurance market and spur a political backlash just as Republicans gain full control of the government . To stave off that outcome , Republicans could find themselves in the awkward position of appropriating huge sums to temporarily prop up the Obama health care law , angering conservative voters who have been demanding an end to the law for years . In another twist , Donald J. Trump ’ s administration , worried about preserving executive branch prerogatives , could choose to fight its Republican allies in the House on some central questions in the dispute . ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(contents[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 307, 100)\n"
     ]
    }
   ],
   "source": [
    "seq = tf.convert_to_tensor(np.asarray([samples[0, 0, :, :]]), dtype=tf.float32)\n",
    "translation = translate(seq, samples, encoders, decoders, original_domains[0], original_domains[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original First Sentence from NYT translated to CNN before Training (Random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bid.', 'stiches', 'woke', 'riverboat', 'ensued.', 'hemorrhaged', '112mph', 'Unfettered', 'accomodate', 'Kindergarten', 'Understood', 'modernizer', 'this✌🏻our', 'classfied', 'into', 'Filipinos', '351', '£1', 'Haggadah', 'criticism', '103.05', 'importance', 'Trumpileaks', 'KCTV', 'Stallion', '3099', 'cataclysms', 'little', 'Limit', '4.89', 'barstool', 'Succumbing', 'USICH', 'incongrous', 'Belgium.', 'smokehouse', 'treasury', 'PVDA', 'Heartbroken', 'Headboards', 'Neave', 'Bangko', 'serious', 'tMe', 'soon-to-retire', 'first', 'hideouts', 'dependability', 'Dimes', 'junta', 'cathartically', 'Hildebrandt', 'CYO', 'particular', 'Employing', 'eyeshadow', 'vital', 'taking', 'fences.', 'LAVC', 'McQueary', 'TamirRice', 'Firman', 'preliminary', 'healthspan', 'hex', 'psychotropic', 'Seaport', '19:15', 'Lonnergan', 'Downplaying', 'domestically', 'multi-billion', 'beseeching', 'fables', \"'50/50\", 'LeBlanc', 'homicide', 'Wass', 'sky.', 'Asmussen', 'resumed', 'abstinence', 'readouts', 'Chicago.', 'tire', 'Bannon', 'Sohinki', 'Schmoozing', 'Floriston', 'KoolkidsKlanKkk', 'unprecedented.', 'industrialised', 'readmission', 'expound', 'Peelean', 'Kept', 'chortled', 'Indiewire', 'Davidians', 'talentedon', 'Xharefi.', 'rectified', 'underhand', 'Bartoli', 'Coonelly', 'washwoman', 'ziplock', 'jackieevancho', 'Chetham', 'Junkin', 'kidnapped.', 'conveying', 'SFII', 'markup', 'chooses.', 'Yonca', 'Gebauer', 'Extraditing', 'adjustment', 'Reckons', 'Lasorda', 'workmanship', 'Besancon', 'Iranian-born', 'COMPUTER', '30F', 'frequenting', 'tight-head', 'Radio', 'whacker', 'passing', 'had', 'Almontaser', 'Bertie', 'ohhhhh', \"'unfiltered\", 'Multiply', 'As', 'Kasler', 'Reckons', 'untried', 'Obama17', 'Leys', 'manipulator', 'mold', 'Callaway', 'dinnertime', 'Valois', 'very', 'jatropha', 'TheHill', 'Safety', 'Sonntag', 'misdemeanors.', '36-page', 'ringing.', 'teens.', 'Gaelynn', 'DaMaris', 'finalization', 'Projecting', 'crickets', 'putting', 'anthology', 'handmaid', 'nuptial', 'Montgomerie', 'Judicial', 'theater.', 'snowshoes', 'Comm', 'fixedly', 'Loftis', 'FailingAnthony', 'non-Opec', 'HouseGOP', 'Hallett', 'stuporous', 'POTUS', 'bantering', 'vilifed', 'Olongapo', 'screengrab', 'porkie', 'tenures', 'Monster.', 'Profession', 'SB54', 'allocating', 'cane', 'bubs', 'Muraat', 'conducts', 'Others', '649', 'Marisol', 'Star', 'warmers', 'blackmarket', 'Tranquillity', 'shriek', 'Pottery', 'WNCT', 'Qataris', 'DolarToday', 'Gwenda', 'inheriting', 'Jazayeri', 'highfalutin', 'dissociating', 'Porridge', 'Jayceon', 'Wimberly', 'Lyttelton', 'strolling', 'lampposts', 'worshiped', 'Djukanovic', 'Chilling', 'predecessors.', 'roof.', 'Mbita', 'sentiment', 'underpin', 'ditch.', 'Boix-Vives', 'startups.', \"'cheated\", 'courtiers', 'interbreeding', 'Solicitation', 'washingtonpost', 'psychotherapists', 'Leshi', 'senators', 'Kemba', \"'Revolution\", 'Street', 'toothsome', '161-strong', 'sauces', 'evalution', 'clemency', 'perverts', 'polarized', 'Fukushima.', 'eBay-type', '127m', 'passenger', 'Merely', 'UNLV', 'Eurogamer', 'dégagisme', '0.4', 'paid', 'collie', 'saturate', \"'look\", 'Emerson\\u200f', 'regrouping', '2.77', 'Handling', 'Daulat', 'disabuse', 'investigation', 'cratering', 'TripAdvisor', 'regular', 'Unseasonably', 'Scientologist', 'city-dwellers', 'textalyzer', 'algorithmic', 'maximalism', '786', 'SkyBridge', 'Obermann', 'Europeanist', 'owls', 'Gardot', 'stubbornly-high', '2026.', 'characterises', 'Bronzefield', '£11.5m', 'Turbie', 'condemnation', 'hots', 'Marchak', 'benefitted', 'Kamiyama', 'livetweet', 'Cleverly', 'glibness', 'temperature.', 'lobotomized', 'left-wing', 'marijauna', 'Houmam', 'Mossack', 'efforts', \"'unfiltered\", 'ROCKFORD', 'MindBodyGreen', 'Lyndale', 'librettist']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vecSeqToSentence(translation[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain 0, Epoch 1:\n",
      "\tReconstruction Loss: 0.8598251342773438\n",
      "\tDivergence Loss: 1.355478048324585\n"
     ]
    }
   ],
   "source": [
    "trainAutoencodersInitial(samples, encoders, decoders, discriminator, num_samples, original_domains, epochs=10, weight=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original First Sentence from NYT translated to CNN after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 307, 100)\n",
      "['Corsica', 'harbours', 'Assurance', 'not', 'Hoerbranz', 'cohered', '6,000-strong', 'ini', 'klansman', 'dudgeon', 'Seltzer', 'Splitting', 'WKMG', 'incarcerating', 'Suchitra', 'givemecookies', 'losses.', 'internists', 'intranet', 'recalled', 'Co-operation', 'unsolvable', 'Poms', 'monolingual', 'Vannevar', 'pap', 'Licht', 'guinea', 'butt.', '50mg', 'Montengro', \"'Throughout\", 'Hounds', 'biweekly', 'LAUGHTER', 'Fung', 'Femi', 'pigeonholed', 'secular.', 'sheikhs', 'idle', 'perused', 'Zawia', 'épater', 'City', 'recommitment', '31m', 'pulverizes', '1561', 'SB193', 'Fraternal', 'Manteca', 'Ryen', 'Chiemingo', 'newspapers', 'Chaikin', 'depends', 'hollows', '464,000', 'Purcellville', 'double-digit', 'Haka', '572,900', 'correctly.', 'unmemorable', 'Unprocessed', '1:27:34', '1981.', 'A1C', 'adopter', 'Instagramming', 'Carolinians', 'Photoshopped', 'Cottom', 'gamesmanship', 'arguable', 'glitziest', 'Rapides', 'snowboards', 'unconcern', 'Reidar', 'Hegeman', 'stooping', 'embrace', 'yeah.', 'causation', 'bigwig', 'Betzy', 'Knicks', 'Shuham', 'multigenerational', 'caddies', 'Wilmot', 'ascends', 'bandits', 'empathizing', 'opposing', 'tracker.', 'Liquidation', 'relocates', 'FLEE', 'Bild', 'surplus', 'Birkenstocks', 'Illegitimate', 'endowments', 'Yachty', 'dumbed', 'professing', 'Yaeger', 'GeraldoRivera', 'jetpack', 'Safeguard', 'Braillard', 'Awaits', 'Pestalozzi', 'protectable', 'Flak', 'unrivalled', 'piper', 'Kentlake', 'JUSTICES', 'Glacier', 'centrifuges', 'rebroadcast', 'pupper', 'operationalize', 'Waving', 'Microphones', 'glennkesslerWP', 'republic', '438', 'sight.', 'campaign', 'longer', 'Elects', 'Trailers', 'Beziers', 'itself', 'part', 'Volin', 'LoveAndTacosContest', '2026.', 'WHNT', 'squelching', 'RPV', 'Fluffster', 'ancestries', 'Libery', 'drumming', 'CIVIL', '..lawd', 'Jadwa', 'Doctrinally', 'have', 'editorialized', '1601', \"''We\", 'conditions', 'Hassino', 'infomercials', 'liberal', 'Exchanges', 'Guerdane', 'self‑deprecatingly', 'Scuds', 'Zagreb', 'with', 'circuses', 'Magnolia', 'Ronstadt', 'ParisAccord', 'publicly', 'overfeeding', 'Margolin', 'Trout', 'salute.', 'Conwy', 'Crowdwish', 'Revelations', 'Reznick', 'Yoshino', 'weeps', '●', 'meekest', 'whosoever', 'stance.', 'bumps.', 'outrun', 'equity.', '39t', 'Hershdorfer', 'GCSEs', '2006/07', 'Drops', 'GLOBAL', '\\u200baccused', 'overspill', 'armed', 'weirdos.', 'fences.', '8:01', 'precedented', 'Cybergenetics', 'Kueng', 'factfinding', 'NFP', 'Scotland-England', 'water', 'teens.', 'calisthenics', 'weekend', 'case-by-case', 'Alamos', 'conﬁrmation', 'assumption.', 'misogynists', 'McQuaid', 'Annandale', 'coweringly', 'Disney', 'glennkesslerWP', 'aversion', 'southerly', 'victual', '79.', 'FPI', 'Rejection', 'Horrow', 'LG_Blount', '£1.30', 'Disney-owned', 'privileges', 'bandaged', 'caseload', 'Foodways', 'giant.', 'Hafeedh', 'Pagasa', 'interning', 'thumbnails', 'Alternately', 'Ishak', 'Beermaker', 'RSVP', '£16', 'Fortaleza', 'Dated', 'Charlatan', 'detailees', 'homage', 'Jervis', 'Linus', 'Dionne', 'contingencies', 'Storrs', 'methylprednisolone', 'judges', 'signed.', 'matriculate', 'L*Space', 'tenderly', 'GMP', 'electric', 'renaming', 'compartmentalize', 'tsunamis', 'cyberpolice', 'reflation', 's', 'ginned', 'sector', 'Pragmatists', 'cozied', '455', 'addresss', '£1,200', 'Zucchero', 'Salad.', 'Jumpstart', 'chuffed', 'trolleys', 'coalminers', '.now', 'Epinephrine', 'parenthetical', 'SolarCity.', 'Goshen', 'Sepahnews', 'Biscan', 'signalman', 'Tamaki', 'Spam', 'Trumputingate', 'Justice.', 'lobes', 'sappers', 'portrays', '639m', 'MLPs', 'Excursions', '248', 'tCarly', 'because', 'freelanced', '19.2', 'circumspect']\n"
     ]
    }
   ],
   "source": [
    "translate(seq, samples, encoders, decoders, original_domains[0], original_domains[1])\n",
    "vecSeqToSentence(translation[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain 2, Epoch 1:\n",
      "\tReconstruction Loss: 0.48000383377075195\n",
      "\tDivergence Loss: 1.581259846687317\n",
      "Domain 2, Epoch 2:\n",
      "\tReconstruction Loss: 0.4359170198440552\n",
      "\tDivergence Loss: 1.6108452081680298\n",
      "Domain 2, Epoch 3:\n",
      "\tReconstruction Loss: 0.3379688262939453\n",
      "\tDivergence Loss: 1.640911340713501\n"
     ]
    }
   ],
   "source": [
    "trainAutoencodersWithPz(samples, encoders, decoders, discriminator, num_samples, original_domains, epochs=10, weight=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original First Sentence from NYT translated to Breitbart after Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detok = TreebankWordDetokenizer()\n",
    "\n",
    "evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
    "                        max_n=4,\n",
    "                        limit_length=True,\n",
    "                        length_limit=100,\n",
    "                        length_limit_type='words',\n",
    "                        apply_avg=False,\n",
    "                        apply_best=True,\n",
    "                        alpha=0.5, # Default F1_score\n",
    "                        weight_factor=1.2,\n",
    "                        stemming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateOnArticles(articles, encoder, decoder):\n",
    "    translated = decoder(encoder(articles))\n",
    "       \n",
    "    original_sentences = [detok.detokenize(tokens) for tokens in articles]\n",
    "    \n",
    "    translated_sentences = [vecSeqToSentence(tokens) for tokens in translated]\n",
    "    \n",
    "    scores = evaluator.get_scores(translated_sentences, original_sentences)\n",
    "    \n",
    "    for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
    "        print('\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * results['p'], 'R', 100.0 * results['r'], 'F1', 100.0 * results['f']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(articles_df, encoders, decoders):\n",
    "    \n",
    "    publications = articles_df.publication.unique()\n",
    "    for i in range(len(publications)):\n",
    "        for j in range(len(publications)):\n",
    "            if (i != j):\n",
    "                pub1=publications[i]\n",
    "                pub2=publications[j]\n",
    "                source_articles = articles_df.loc[articles_df['publication'] == pub1]['content'].tolist()\n",
    "\n",
    "                print(pub1,\"to\",pub2)\n",
    "                evaluateOnArticles(source_articles, encoders[i], decoders[j])\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
