{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import pandas\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "import rouge\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LSTM, Activation, Bidirectional, Dropout, Input, concatenate, Reshape, TimeDistributed, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow.keras.backend as K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>publication</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[washington, —, congressional, republicans, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[after, the, bullet, shells, get, the, south, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[when, walt, disney, ’, s, but, what, they, di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[death, may, be, the, great, equalizer, ,, but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[seoul, ,, south, korea, —, although, north, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47220</th>\n",
       "      <td>47220</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[bt, is, introducing, two, initiatives, from, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47221</th>\n",
       "      <td>47221</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[computer, users, across, the, world, more, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47222</th>\n",
       "      <td>47222</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[a, new, european, directive, could, if, it, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47223</th>\n",
       "      <td>47223</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[the, man, making, sure, us, amit, yoran, was,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47224</th>\n",
       "      <td>47224</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[online, role, playing, games, are, time-consu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47225 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0     publication  \\\n",
       "0               0  New York Times   \n",
       "1               1  New York Times   \n",
       "2               2  New York Times   \n",
       "3               3  New York Times   \n",
       "4               4  New York Times   \n",
       "...           ...             ...   \n",
       "47220       47220        BBC_tech   \n",
       "47221       47221        BBC_tech   \n",
       "47222       47222        BBC_tech   \n",
       "47223       47223        BBC_tech   \n",
       "47224       47224        BBC_tech   \n",
       "\n",
       "                                                 content  \n",
       "0      [washington, —, congressional, republicans, ha...  \n",
       "1      [after, the, bullet, shells, get, the, south, ...  \n",
       "2      [when, walt, disney, ’, s, but, what, they, di...  \n",
       "3      [death, may, be, the, great, equalizer, ,, but...  \n",
       "4      [seoul, ,, south, korea, —, although, north, k...  \n",
       "...                                                  ...  \n",
       "47220  [bt, is, introducing, two, initiatives, from, ...  \n",
       "47221  [computer, users, across, the, world, more, th...  \n",
       "47222  [a, new, european, directive, could, if, it, g...  \n",
       "47223  [the, man, making, sure, us, amit, yoran, was,...  \n",
       "47224  [online, role, playing, games, are, time-consu...  \n",
       "\n",
       "[47225 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('data/tokenized.pkl')\n",
    "eval_df = pd.read_pickle('data/evaluation.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = list(data['content'])\n",
    "all_sentences.extend(list(eval_df['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Relevant publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_publications = [\n",
    "#  'Breitbart',\n",
    "#  'CNN',\n",
    "#  'New York Times',\n",
    "#  'NPR',\n",
    "#  'Fox News',\n",
    "#  'Reuters']\n",
    "selected_publications = [\n",
    " 'Breitbart',\n",
    " 'CNN',\n",
    " 'New York Times']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Atlantic',\n",
       " 'New York Times',\n",
       " 'Vox',\n",
       " 'Reuters',\n",
       " 'Buzzfeed News',\n",
       " 'Business Insider',\n",
       " 'Talking Points Memo',\n",
       " 'New York Post',\n",
       " 'Washington Post',\n",
       " 'BBC_business',\n",
       " 'BBC_politics',\n",
       " 'BBC_tech',\n",
       " 'BBC_sport',\n",
       " 'Breitbart',\n",
       " 'Fox News',\n",
       " 'Guardian',\n",
       " 'NPR',\n",
       " 'CNN',\n",
       " 'National Review',\n",
       " 'BBC_entertainment']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_publications = list(set(data['publication']))\n",
    "all_publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Breitbart', 'CNN', 'New York Times']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take only the contents from publications with >= 3000 samples.\n",
    "publications = [pub for pub in selected_publications if pub in all_publications and len(data[data['publication'] == pub]) >= 3000]\n",
    "publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = []\n",
    "for pub in publications:\n",
    "    contents.append(np.asarray(data[data['publication'] == pub]['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding with special Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = max([len(seq) for content in contents for seq in content])\n",
    "max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_token = '~?@_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for content in contents:\n",
    "    for seq in content:\n",
    "        seq.extend([end_token] * (max_seq_length - len(seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = max([len(seq) for content in contents for seq in content])\n",
    "max_seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word2vec = gensim.models.Word2Vec(all_sentences, min_count = 1,  \n",
    "                              size = word_dim, window = 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'congress' and 'senate' - CBOW :  0.7559486\n",
      "Cosine similarity between 'congress' and 'house' - CBOW :  0.6252873\n"
     ]
    }
   ],
   "source": [
    "print(\"Cosine similarity between 'congress' \" + \n",
    "               \"and 'senate' - CBOW : \", \n",
    "    word2vec.wv.similarity('congress', 'senate')) \n",
    "      \n",
    "print(\"Cosine similarity between 'congress' \" +\n",
    "                 \"and 'house' - CBOW : \", \n",
    "    word2vec.wv.similarity('congress', 'house')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.29592454, -0.8773146 ,  0.76711816, -0.05720106, -0.03461413,\n",
       "       -0.40311787,  0.22946852,  1.0140978 ,  0.8975274 , -0.7267992 ,\n",
       "        1.2324462 , -0.43481615,  0.8528818 , -0.7095517 , -0.07889917,\n",
       "       -0.52104896,  0.2608844 , -0.58594555, -0.59863317, -0.31064552,\n",
       "       -0.11377191, -0.50565964,  0.15798195,  0.26478073,  1.4427081 ,\n",
       "        0.55394846,  0.17723931,  0.7052398 , -0.88249487, -0.5539005 ,\n",
       "        0.2363067 ,  0.06038764, -0.86185366, -0.19701053,  0.14048052,\n",
       "       -0.45728892,  0.2533323 , -0.26327336, -0.8601514 ,  1.0089282 ,\n",
       "        0.14991663, -0.44243005, -1.3007863 , -1.164767  , -1.2460694 ,\n",
       "        0.24592596, -0.501519  , -0.23789279,  0.24771175,  0.13797134,\n",
       "       -0.20122194,  1.2545592 ,  1.3626865 , -0.5414245 ,  0.9415389 ,\n",
       "       -0.27835897, -0.4958755 , -0.01836712,  0.8055421 , -0.56852084,\n",
       "       -0.9491626 , -1.0129623 , -0.07664649, -0.00478025, -1.3793099 ,\n",
       "        0.02242324, -0.07649595, -0.25851354,  0.30401504,  0.07324145,\n",
       "        0.53251505,  0.6323623 ,  0.02162529,  0.14522254, -0.9740372 ,\n",
       "       -0.41615644, -0.21245232,  0.48022053, -0.6016198 ,  0.6512501 ,\n",
       "       -0.30599976,  0.3995207 ,  0.1611771 ,  0.36789933,  0.85954314,\n",
       "        0.0623347 , -0.14163163, -0.5173472 ,  0.6012967 , -0.30832508,\n",
       "        0.5945276 ,  0.5229075 ,  0.15898941,  0.26670974,  0.04463593,\n",
       "        0.69880843, -0.0644099 ,  0.3105393 ,  0.15631242,  0.6915079 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv['congressional']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('congressional', 0.9999998807907104),\n",
       " ('senate', 0.8712809085845947),\n",
       " ('gop', 0.8363369703292847),\n",
       " ('committee', 0.8346565961837769),\n",
       " ('judiciary', 0.8318742513656616),\n",
       " ('lawmakers', 0.8250969052314758),\n",
       " ('chambers', 0.8131620287895203),\n",
       " ('voted', 0.8018012046813965),\n",
       " ('liberal', 0.7954562306404114),\n",
       " ('conservative', 0.7942031621932983)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.similar_by_vector(word2vec.wv['congressional'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = np.asarray(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.zeros(shape=(contents.shape[0], contents.shape[1], max_seq_length, word_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(contents.shape[0]):\n",
    "    for j in range(contents.shape[1]):\n",
    "        for k in range(max_seq_length):\n",
    "            samples[i, j, k, :] = word2vec.wv[contents[i, j][k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciton Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squareError(xTrue, xPred):\n",
    "    return K.square(xTrue - xPred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstructionLoss(sample, encoder, decoder, f_w, weight): # (L_1 from the paper)\n",
    "    return K.mean(squareError(sample, decoder(encoder(sample)))) + K.mean(weight*K.log(f_w(encoder(sample))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divergenceLoss(f_w, encoder, sample, z_j, n_j): # Mean of log f_w(E_theta_i(x_j)) + log (1-f_w(z_j, n_j)) from the paper (L_2).\n",
    "    return K.mean(K.log(f_w(encoder(sample)))) + K.mean(K.log(1 - f_w([z_j, n_j])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(data, domain, num_samples):\n",
    "    N = data.shape[1]\n",
    "    return tf.convert_to_tensor(data[domain, np.random.choice(N, num_samples, replace=True),:,:], dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently just doing a restriction to the last z variables, might want to do a matrix multiplication?\n",
    "# pi_Z from the paper. projects a latent distribution in (z, n) to z\n",
    "def projectZ(encoded):\n",
    "    return encoded[0] # take zs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projectN(encoded):\n",
    "    return encoded[1] # taek Ns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in two inputs, n and z, and outputs samples.\n",
    "def createDecoder(z_dims, n_dims, time_steps, output_dims, lstm_dims):\n",
    "    # TODO MAYBE: Add in more regularization or different than dropout?\n",
    "\n",
    "    z_inputs = Input(shape=(z_dims,))\n",
    "    n_inputs = Input(shape=(n_dims,))\n",
    "    inputs = concatenate([z_inputs, n_inputs])\n",
    "    dense = Dropout(0.2)(inputs)\n",
    "#     # 150 is arbitrary rn...\n",
    "#     dense = Dense(150)(inputs)\n",
    "    dense = Dense(time_steps*output_dims)(dense)\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    reshape = Reshape((time_steps, output_dims))(dense)\n",
    "    # TODO Reshape to enforce time_steps?\n",
    "    bilstm = Bidirectional(LSTM(lstm_dims, activation='tanh', return_sequences=True))(reshape)\n",
    "    bilstm = Dropout(0.2)(bilstm)\n",
    "    bilstm = Bidirectional(LSTM(lstm_dims, activation='tanh', return_sequences=True))(bilstm)\n",
    "    bilstm = Dropout(0.2)(bilstm)\n",
    "    \n",
    "    outputs = TimeDistributed(Dense(output_dims, activation='linear'))(bilstm)\n",
    "    \n",
    "    model = Model(inputs=[z_inputs, n_inputs], outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEncoder(time_steps, input_num, z_dims, n_dims, lstm_dims):\n",
    "    # TODO MAYBE: Add in more regularization or different than dropout?\n",
    "    inputs = Input(shape=(time_steps, input_num,))\n",
    "    dense = TimeDistributed(Dropout(0.2))(inputs)\n",
    "    bilstm = Bidirectional(LSTM(lstm_dims, activation='tanh', return_sequences=True))(dense)\n",
    "    bilstm = Dropout(0.2)(bilstm)\n",
    "    dense = Bidirectional(LSTM(lstm_dims, activation='tanh', return_sequences=False))(bilstm)\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    z_output = Dense(z_dims, activation='linear')(dense)\n",
    "    n_output = Dense(n_dims, activation='linear')(dense)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=[z_output, n_output])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDiscriminator(z_dims, n_dims):\n",
    "    z_inputs = Input(shape=(z_dims,))\n",
    "    n_inputs = Input(shape=(n_dims,))\n",
    "    inputs = concatenate([z_inputs, n_inputs])\n",
    "    \n",
    "    # 150, 100 is arbitrary rn...\n",
    "    dense = Dense(150, activation='relu')(inputs)\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    dense = Dense(100, activation='relu')(dense)\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    output = Dense(1, activation='sigmoid')(dense)\n",
    "    \n",
    "    model = Model(inputs=[z_inputs, n_inputs], outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-3\n",
    "enc_optimizer = tf.keras.optimizers.Adam(lr)\n",
    "dec_optimizer = tf.keras.optimizers.Adam(lr)\n",
    "disc_optimizer = tf.keras.optimizers.Adam(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When $P_Z$ is known... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# k is num of domains.\n",
    "# encoders is a list of encoders.\n",
    "# decoders is list of decoders.\n",
    "# samples is a K x N x Timesteps x dim, array of samples, where the 0th index is the domain,\n",
    "# the 1th index is the # of the sample in that domain, 2th index is the # timesteps per sequence, 3th index is the #\n",
    "# of dimensions at each timestep\n",
    "# original_domains is a list of the original domains P_z was derived from.\n",
    "\n",
    "# Currently assuming P_Z is known. Must approximate P_Z first.\n",
    "def trainAutoencodersWithPz(samples, encoders, decoders, discriminator, num_samples, original_domains, epochs=10, weight=1.0):\n",
    "    N = samples.shape[1]\n",
    "    k = samples.shape[0]\n",
    "        \n",
    "    \n",
    "    for i in range(k):\n",
    "        if i not in original_domains:\n",
    "            original_domain = np.random.choice(original_domains)\n",
    "            encoder = encoders[i]\n",
    "            decoder = decoders[i]\n",
    "            original_encoder = encoders[original_domain]\n",
    "            epoch = 0\n",
    "            while(epoch < epochs): # could also do until some convergence criteria.\n",
    "                p_Xi_samples = sample(samples, i, num_samples)\n",
    "                p_Z_samples = projectZ(original_encoder(sample(samples, original_domain, num_samples)))\n",
    "                p_Ni_samples = projectN(encoder(sample(samples, i, num_samples)))\n",
    "\n",
    "                with tf.GradientTape() as enc_tape, tf.GradientTape() as dec_tape, tf.GradientTape() as disc_tape:\n",
    "\n",
    "                    reconstruction_loss = reconstructionLoss(p_Xi_samples, encoder, decoder, discriminator, weight)\n",
    "\n",
    "                    # negative b/c gradient ascent.\n",
    "                    divergence_loss = -1 * divergenceLoss(discriminator, encoder, p_Xi_samples, p_Z_samples, p_Ni_samples)\n",
    "\n",
    "                gradients_of_encoder = enc_tape.gradient(reconstruction_loss, encoder.trainable_variables)\n",
    "                gradients_of_decoder = dec_tape.gradient(reconstruction_loss, decoder.trainable_variables)\n",
    "                gradients_of_discriminator = disc_tape.gradient(divergence_loss, discriminator.trainable_variables)\n",
    "\n",
    "\n",
    "                enc_optimizer.apply_gradients(zip(gradients_of_encoder, encoder.trainable_variables))\n",
    "                dec_optimizer.apply_gradients(zip(gradients_of_decoder, decoder.trainable_variables))\n",
    "                disc_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "                \n",
    "                print('Domain {}, Epoch {}:\\n\\tReconstruction Loss: {}\\n\\tDivergence Loss: {}'.format(i, epoch+1, reconstruction_loss, divergence_loss))\n",
    "                epoch+=1\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When $P_Z$ is unknown...\n",
    "\"A straight-forward approach for learning the latent distribution PZ is to train a regularized autoencoder on data from a\n",
    "single representative domain. However, such a representation could potentially capture variability that is specific to\n",
    "that one domain. To learn a more invariant latent representation, we propose the following extension of our autoencoder\n",
    "framework. The basic idea is to alternate between training\n",
    "multiple autoencoders until they agree on a latent representation that is effective for their respective domains. This is\n",
    "particularly relevant for applications to biology; for example, often one is interested in learning a latent representation\n",
    "that integrates all of the data modalities.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# k is num of domains.\n",
    "# encoders is a list of encoders.\n",
    "# decoders is list of decoders.\n",
    "# samples is a K x N x Timesteps x dim, array of samples, where the 0th index is the domain,\n",
    "# the 1th index is the # of the sample in that domain, 2th index is the # timesteps per sequence, 3th index is the #\n",
    "# of dimensions at each timestep\n",
    "# domains is a list of the domains we are currently training over.\n",
    "\n",
    "def trainAutoencodersInitial(samples, encoders, decoders, discriminator, num_samples, domains, epochs=10, weight=1.0):\n",
    "    N = samples.shape[1]\n",
    "    k = samples.shape[0]\n",
    "    \n",
    "    for i in domains:\n",
    "        encoder = encoders[i]\n",
    "        decoder = decoders[i]\n",
    "        for j in domains:\n",
    "            if i != j:\n",
    "                j_encoder = encoders[j]\n",
    "                epoch = 0\n",
    "                while(epoch < epochs): # could also do until some convergence criteria.\n",
    "                    p_Xi_samples = sample(samples, i, num_samples)\n",
    "                    p_Zj_samples = projectZ(j_encoder(sample(samples, j, num_samples)))\n",
    "                    p_Ni_samples = projectN(encoder(sample(samples, i, num_samples)))\n",
    "\n",
    "                    with tf.GradientTape() as enc_tape, tf.GradientTape() as dec_tape, tf.GradientTape() as disc_tape:\n",
    "\n",
    "                        reconstruction_loss = reconstructionLoss(p_Xi_samples, encoder, decoder, discriminator, weight)\n",
    "                        #print(p_Xi_samples)\n",
    "\n",
    "                        # negative b/c gradient ascent.\n",
    "                        divergence_loss = -1 * divergenceLoss(discriminator, encoder, p_Xi_samples, p_Zj_samples, p_Ni_samples)\n",
    "                        \n",
    "                    gradients_of_encoder = enc_tape.gradient(reconstruction_loss, encoder.trainable_variables)\n",
    "                    gradients_of_decoder = dec_tape.gradient(reconstruction_loss, decoder.trainable_variables)\n",
    "                    gradients_of_discriminator = disc_tape.gradient(divergence_loss, discriminator.trainable_variables)\n",
    "            \n",
    "\n",
    "                    enc_optimizer.apply_gradients(zip(gradients_of_encoder, encoder.trainable_variables))\n",
    "                    dec_optimizer.apply_gradients(zip(gradients_of_decoder, decoder.trainable_variables))\n",
    "                    disc_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "                    \n",
    "                    print('Domain {}, Epoch {}:\\n\\tReconstruction Loss: {}\\n\\tDivergence Loss: {}'.format(i, epoch+1, reconstruction_loss, divergence_loss))\n",
    "                    epoch+=1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples is a K x N x Timesteps x dim, array of samples, where the 0th index is the domain,\n",
    "# the 1th index is the # of the sample in that domain, 2th index is the # timesteps per sequence, 3th index is the #\n",
    "# of dimensions at each timestep\n",
    "\n",
    "def initModel(samples, z_dims, n_dims, lstm_dims):\n",
    "    \n",
    "    k = samples.shape[0]\n",
    "    N = samples.shape[1]\n",
    "    time_steps = samples.shape[2]\n",
    "    dim = samples.shape[3]\n",
    "    \n",
    "    \n",
    "    \n",
    "    discriminator = createDiscriminator(z_dims, n_dims)\n",
    "    \n",
    "    encoders = []\n",
    "    decoders = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        encoders.append(createEncoder(time_steps, dim, z_dims, n_dims, lstm_dims))\n",
    "        decoders.append(createDecoder(z_dims, n_dims, time_steps, dim, lstm_dims))\n",
    "    \n",
    "    return encoders, decoders, discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(start_sequences, samples, encoders, decoders, start_domain, end_domain):\n",
    "    N = samples.shape[1]\n",
    "    print(start_sequences.shape)\n",
    "    num_samples = start_sequences.shape[0]\n",
    "    \n",
    "    start_encoder = encoders[start_domain]\n",
    "    end_encoder = encoders[end_domain]\n",
    "    end_decoder = decoders[end_domain]\n",
    "    \n",
    "    z = projectZ(start_encoder(start_sequences))\n",
    "    n = projectN(end_encoder(sample(samples, end_domain, num_samples)))\n",
    "    \n",
    "    end_sequences = end_decoder([z, n])\n",
    "    return end_sequences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vecSeqToSentence(sequence):\n",
    "    sequence = K.eval(sequence)\n",
    "    sentence = []\n",
    "    for i in range(sequence.shape[0]):\n",
    "        word = sequence[i,:]\n",
    "        #print(word)\n",
    "        #print(word2vec.wv.similar_by_vector(word))\n",
    "        sentence.append(word2vec.wv.similar_by_vector(word)[0][0])\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dims = 60 # len(n)\n",
    "z_dims = 240 # len(Z)\n",
    "\n",
    "lstm_dims = 256\n",
    "\n",
    "num_epochs = 30\n",
    "num_samples = 128\n",
    "\n",
    "weight = 1\n",
    "\n",
    "original_domains = [0, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = tf.convert_to_tensor(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders, decoders, discriminator = initModel(samples, z_dims, n_dims, lstm_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original First Sentence from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'on tuesday ’ s broadcast ” zeleny said , “ and she ’ s having a difficult time in federal prison , no question . ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(contents[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 100)\n"
     ]
    }
   ],
   "source": [
    "seq = tf.convert_to_tensor(np.asarray([samples[0, 0, :, :]]), dtype=tf.float32)\n",
    "translation = translate(seq, samples, encoders, decoders, original_domains[0], original_domains[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original First Sentence from 0 translated to 1 before Training (Random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['receptions', 'bonobos', 'unmask', 'ambivalent', 'parted', 'parted', 'parted', 'compulsion', 'compulsion', 'devouring', 'devouring', 'therealuw', 'nader', 'nader', 'nader', 'nader', 'svensson', 'svensson', 'julavits', 'shyamalan', 'shyamalan', 'sedaris', 'sedaris', 'sedaris', 'sedaris', 'oakridge', 'history.', \"'podcasting\", \"'podcasting\", \"'podcasting\", 'antagonistic', 'antagonistic', 'cojedes', 'cyclones', 'showboating', '757', '757', 'deferred', 'huddle', 'huddle', 'huddle', \"'new\", \"'new\", 'tempt', 'lutzes', 'semantics', 'semantics', 'semantics', 'recede', 'as']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vecSeqToSentence(translation[0,:,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain 0, Epoch 1:\n",
      "\tReconstruction Loss: 0.10268735885620117\n",
      "\tDivergence Loss: 1.2201030254364014\n",
      "Domain 0, Epoch 2:\n",
      "\tReconstruction Loss: -2.51455020904541\n",
      "\tDivergence Loss: 3.563983201980591\n",
      "Domain 0, Epoch 3:\n",
      "\tReconstruction Loss: -8.458131790161133\n",
      "\tDivergence Loss: 9.23603343963623\n",
      "Domain 0, Epoch 4:\n",
      "\tReconstruction Loss: -12.60092544555664\n",
      "\tDivergence Loss: 13.521685600280762\n",
      "Domain 0, Epoch 5:\n",
      "\tReconstruction Loss: -14.220378875732422\n",
      "\tDivergence Loss: 14.963300704956055\n",
      "Domain 0, Epoch 6:\n",
      "\tReconstruction Loss: -11.735140800476074\n",
      "\tDivergence Loss: 12.387560844421387\n",
      "Domain 0, Epoch 7:\n",
      "\tReconstruction Loss: -5.361825942993164\n",
      "\tDivergence Loss: 6.037952899932861\n",
      "Domain 0, Epoch 8:\n",
      "\tReconstruction Loss: 0.5255743265151978\n",
      "\tDivergence Loss: 0.36627310514450073\n",
      "Domain 0, Epoch 9:\n",
      "\tReconstruction Loss: 0.581428050994873\n",
      "\tDivergence Loss: 0.5158608555793762\n",
      "Domain 0, Epoch 10:\n",
      "\tReconstruction Loss: 0.5827150344848633\n",
      "\tDivergence Loss: 0.18962696194648743\n",
      "Domain 0, Epoch 11:\n",
      "\tReconstruction Loss: 0.5964619517326355\n",
      "\tDivergence Loss: 0.01910790242254734\n",
      "Domain 0, Epoch 12:\n",
      "\tReconstruction Loss: 0.5507987141609192\n",
      "\tDivergence Loss: 0.001348256366327405\n",
      "Domain 0, Epoch 13:\n",
      "\tReconstruction Loss: 0.5426820516586304\n",
      "\tDivergence Loss: 6.0366430261638016e-05\n",
      "Domain 0, Epoch 14:\n",
      "\tReconstruction Loss: 0.5781931281089783\n",
      "\tDivergence Loss: 2.0666070668085013e-06\n",
      "Domain 0, Epoch 15:\n",
      "\tReconstruction Loss: 0.5729058384895325\n",
      "\tDivergence Loss: 8.475035429000854e-08\n",
      "Domain 0, Epoch 16:\n",
      "\tReconstruction Loss: 0.5971402525901794\n",
      "\tDivergence Loss: -0.0\n",
      "Domain 0, Epoch 17:\n",
      "\tReconstruction Loss: 0.5620620846748352\n",
      "\tDivergence Loss: -0.0\n",
      "Domain 0, Epoch 18:\n",
      "\tReconstruction Loss: 0.5534623861312866\n",
      "\tDivergence Loss: -0.0\n",
      "Domain 0, Epoch 19:\n",
      "\tReconstruction Loss: 0.5524135231971741\n",
      "\tDivergence Loss: -0.0\n",
      "Domain 0, Epoch 20:\n",
      "\tReconstruction Loss: 0.5994690656661987\n",
      "\tDivergence Loss: -0.0\n",
      "Domain 0, Epoch 21:\n",
      "\tReconstruction Loss: 0.5864915251731873\n",
      "\tDivergence Loss: -0.0\n",
      "Domain 0, Epoch 22:\n",
      "\tReconstruction Loss: 0.5420562028884888\n",
      "\tDivergence Loss: -0.0\n",
      "Domain 0, Epoch 23:\n",
      "\tReconstruction Loss: 0.5869048237800598\n",
      "\tDivergence Loss: -0.0\n",
      "Domain 0, Epoch 24:\n",
      "\tReconstruction Loss: 0.546943724155426\n",
      "\tDivergence Loss: -0.0\n",
      "Domain 0, Epoch 25:\n",
      "\tReconstruction Loss: 0.5506347417831421\n",
      "\tDivergence Loss: -0.0\n",
      "Domain 0, Epoch 26:\n",
      "\tReconstruction Loss: 0.5632322430610657\n",
      "\tDivergence Loss: -0.0\n",
      "Domain 0, Epoch 27:\n",
      "\tReconstruction Loss: 0.5647971630096436\n",
      "\tDivergence Loss: -0.0\n",
      "Domain 0, Epoch 28:\n",
      "\tReconstruction Loss: 0.5491461157798767\n",
      "\tDivergence Loss: -0.0\n",
      "Domain 0, Epoch 29:\n",
      "\tReconstruction Loss: 0.5629287362098694\n",
      "\tDivergence Loss: -0.0\n"
     ]
    }
   ],
   "source": [
    "trainAutoencodersInitial(samples, encoders, decoders, discriminator, num_samples, original_domains, epochs=num_epochs, weight=weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original First Sentence from 0 translated to 1 after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = translate(seq, samples, encoders, decoders, original_domains[0], original_domains[1])\n",
    "vecSeqToSentence(translation[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAutoencodersWithPz(samples, encoders, decoders, discriminator, num_samples, original_domains, epochs=num_epochs, weight=weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original First Sentence from 0 translated to 2 after Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = translate(seq, samples, encoders, decoders, 0, 2)\n",
    "vecSeqToSentence(translation[0,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>publication</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[it, is, called, the, heimlich, maneuver, —, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[after, all, the, allegations, of, rampant, vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[zsa, zsa, gabor, ,, the, hungarian, actress, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[russia, ’, s, ambassador, to, turkey, ,, andr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[san, francisco, —, following, directions, fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>5995</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[bt, is, introducing, two, initiatives, to, he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>5996</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[computer, users, across, the, world, continue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>5997</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[a, new, european, directive, could, put, soft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>5998</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[the, man, making, sure, us, computer, network...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>5999</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[online, role, playing, games, are, time-consu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0     publication  \\\n",
       "0              0  New York Times   \n",
       "1              1  New York Times   \n",
       "2              2  New York Times   \n",
       "3              3  New York Times   \n",
       "4              4  New York Times   \n",
       "...          ...             ...   \n",
       "5995        5995        BBC_tech   \n",
       "5996        5996        BBC_tech   \n",
       "5997        5997        BBC_tech   \n",
       "5998        5998        BBC_tech   \n",
       "5999        5999        BBC_tech   \n",
       "\n",
       "                                                content  \n",
       "0     [it, is, called, the, heimlich, maneuver, —, s...  \n",
       "1     [after, all, the, allegations, of, rampant, vo...  \n",
       "2     [zsa, zsa, gabor, ,, the, hungarian, actress, ...  \n",
       "3     [russia, ’, s, ambassador, to, turkey, ,, andr...  \n",
       "4     [san, francisco, —, following, directions, fro...  \n",
       "...                                                 ...  \n",
       "5995  [bt, is, introducing, two, initiatives, to, he...  \n",
       "5996  [computer, users, across, the, world, continue...  \n",
       "5997  [a, new, european, directive, could, put, soft...  \n",
       "5998  [the, man, making, sure, us, computer, network...  \n",
       "5999  [online, role, playing, games, are, time-consu...  \n",
       "\n",
       "[6000 rows x 3 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = []\n",
    "for pub in publications:\n",
    "    contents.append(np.asarray(eval_df[eval_df['publication'] == pub]['content']))\n",
    "    \n",
    "for content in contents:\n",
    "    for seq in content:\n",
    "        seq.extend([end_token] * (max_seq_length - len(seq)))\n",
    "    \n",
    "contents = np.asarray(contents)\n",
    "samples = np.zeros(shape=(contents.shape[0], contents.shape[1], max_seq_length, word_dim))\n",
    "\n",
    "for i in range(contents.shape[0]):\n",
    "    for j in range(contents.shape[1]):\n",
    "        for k in range(max_seq_length):\n",
    "            samples[i, j, k, :] = word2vec.wv[contents[i, j][k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detok = TreebankWordDetokenizer()\n",
    "\n",
    "evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
    "                        max_n=4,\n",
    "                        limit_length=True,\n",
    "                        length_limit=100,\n",
    "                        length_limit_type='words',\n",
    "                        apply_avg=False,\n",
    "                        apply_best=True,\n",
    "                        alpha=0.5, # Default F1_score\n",
    "                        weight_factor=1.2,\n",
    "                        stemming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateOnArticles(articles, encoder, decoder):\n",
    "    translated = decoder(encoder(tf.convert_to_tensor(articles, dtype=tf.float32)))\n",
    "       \n",
    "    original_sentences = [detok.detokenize(vecSeqToSentence(tokens)) for tokens in articles]\n",
    "    \n",
    "    translated_sentences = [detok.detokenize(vecSeqToSentence(tokens)) for tokens in translated]\n",
    "    \n",
    "    scores = evaluator.get_scores(translated_sentences, original_sentences)\n",
    "    \n",
    "    for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
    "        print('\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * results['p'], 'R', 100.0 * results['r'], 'F1', 100.0 * results['f']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(articles, encoders, decoders):\n",
    "    \n",
    "    for i in range(len(selected_publications)):\n",
    "        for j in range(len(selected_publications)):\n",
    "            if (i != j):\n",
    "                pub1=publications[i]\n",
    "                pub2=publications[j]\n",
    "                #source_articles = articles_df.loc[articles_df['publication'] == pub1]['content'].tolist()\n",
    "                source_articles = articles[i]\n",
    "                \n",
    "                print(pub1,\"to\",pub2)\n",
    "                evaluateOnArticles(source_articles, encoders[i], decoders[j])\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(samples, encoders, decoders)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
