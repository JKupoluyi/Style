{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Dense, LSTM, Activation, Bidirectional, Dropout\n",
    "from keras.models import Sequential\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squareError(xTrue, xPred):\n",
    "    return K.square(xTrue - xPred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminateEncodedError(f_w, encoder, sample): # log f_w(E_theta_i(x_j)) from the paper.\n",
    "    return K.log(f_w(encoder(sample)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstructionLoss(sample, encoder, decoder, f_w, weight): # (L_1 from the paper)\n",
    "    return K.mean(squareError(sample, decoder(encoder(sample))) + \n",
    "                  weight*discriminateEncodedError(f_w, encoder, sample), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divergenceLoss(f_w, encoder, sample, z_j, n_j): # Mean of log f_w(E_theta_i(x_j)) + log (1-f_w(z_j, n_j)) from the paper (L_2).\n",
    "    return K.mean(discriminateEncodedError(f_w, encoder, sample) + K.log(1 - f_w(z_j, n_j)), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in two inputs, n and z, and outputs samples.\n",
    "def CreateDecoder():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateEncoder(input_num, shared_output_num, remaining_output_num, hyperparams):\n",
    "    # TODO MAYBE: Add in more regularization or different than dropout?\n",
    "    # TODO make two outputs: n and z.\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(32, activation='tanh', return_sequences=True, input_shape=(batch_size, time_steps, features))),\n",
    "        Dropout(0.2),\n",
    "        Bidirectional(LSTM(32, activation='tanh', dropout=0.2, return_sequences=False)),\n",
    "        Dropout(0.5),\n",
    "        Dense(output_num)\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.compile()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_optimizer = tf.keras.optimizers.Adam(5e-4)\n",
    "dec_optimizer = tf.keras.optimizers.Adam(5e-4)\n",
    "disc_optimizer = tf.keras.optimizers.Adam(5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k is num of domains.\n",
    "# encoders is a list of encoders.\n",
    "# decoders is list of decoders.\n",
    "# samples is a K x N array of samples, where the first index is the domain,\n",
    "# the second index is the # of the sample in that domain.\n",
    "\n",
    "# TODO IMPORTANT: Currently assuming P_Z is known, but it is NOT. Must alter algorithm as in (3.2) to support unknown P_Z.\n",
    "def trainAutoencoders(k, encoders, decoders, samples, discriminator, weight=1.0):\n",
    "    N = samples.shape[0]\n",
    "    \n",
    "    for i in range(k):\n",
    "        encoder = encoders[i]\n",
    "        decoder = decoders[i]\n",
    "        while(not isConverged(encoders[i], decoders[i])):\n",
    "            p_Xi_samples = samples[i,:]\n",
    "            p_Z_samples = projectZ(encoders[i](samples[]))\n",
    "            p_Ni_samples = None # TODO Something!\n",
    "            \n",
    "            with tf.GradientTape() as enc_tape, tf.GradientTape() as dec_tape, tf.GradientTape() as disc_tape:\n",
    "                \n",
    "                reconstruction_loss = reconstructionLoss(p_Xi_samples, encoder, decoder, f_w, weight)\n",
    "                \n",
    "                # negative b/c gradient ascent.\n",
    "                divergence_loss = -divergenceLoss(f_w, encoder, p_Xi_samples, p_Z_samples, p_Ni_samples)\n",
    "\n",
    "            gradients_of_encoder = enc_tape.gradient(reconstruction_loss, encoder.trainable_variables)\n",
    "            gradients_of_decoder = dec_tape.gradient(reconstruction_loss, decoder.trainable_variables)\n",
    "            gradients_of_discriminator = disc_tape.gradient(divergence_loss, discriminator.trainable_variables)\n",
    "            \n",
    "\n",
    "            enc_optimizer.apply_gradients(zip(gradients_of_encoder, encoder.trainable_variables))\n",
    "            dec_optimizer.apply_gradients(zip(gradients_of_decoder, decoder.trainable_variables))\n",
    "            disc_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
