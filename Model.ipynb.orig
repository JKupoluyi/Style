{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcusdaly/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/marcusdaly/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/marcusdaly/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/marcusdaly/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/marcusdaly/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/marcusdaly/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/marcusdaly/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/marcusdaly/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/marcusdaly/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/marcusdaly/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/marcusdaly/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/marcusdaly/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Dense, LSTM, Activation, Bidirectional, Dropout, Input, concatenate\n",
    "from keras.models import Sequential\n",
    "from keras import Model\n",
    "import keras.backend as K\n",
<<<<<<< HEAD
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import pandas\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import gensim \n",
    "from gensim.models import Word2Vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>publication</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[WASHINGTON, —, Congressional, Republicans, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[After, the, bullet, shells, get, counted, ,, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[When, Walt, Disney, ’, s, “, Bambi, ”, opened...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[Death, may, be, the, great, equalizer, ,, but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[SEOUL, ,, South, Korea, —, North, Korea, ’, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47220</th>\n",
       "      <td>47220</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[BT, is, introducing, two, initiatives, to, he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47221</th>\n",
       "      <td>47221</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[Computer, users, across, the, world, continue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47222</th>\n",
       "      <td>47222</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[A, new, European, directive, could, put, soft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47223</th>\n",
       "      <td>47223</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[The, man, making, sure, US, computer, network...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47224</th>\n",
       "      <td>47224</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[Online, role, playing, games, are, time-consu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47225 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0     publication  \\\n",
       "0               0  New York Times   \n",
       "1               1  New York Times   \n",
       "2               2  New York Times   \n",
       "3               3  New York Times   \n",
       "4               4  New York Times   \n",
       "...           ...             ...   \n",
       "47220       47220        BBC_tech   \n",
       "47221       47221        BBC_tech   \n",
       "47222       47222        BBC_tech   \n",
       "47223       47223        BBC_tech   \n",
       "47224       47224        BBC_tech   \n",
       "\n",
       "                                                 content  \n",
       "0      [WASHINGTON, —, Congressional, Republicans, ha...  \n",
       "1      [After, the, bullet, shells, get, counted, ,, ...  \n",
       "2      [When, Walt, Disney, ’, s, “, Bambi, ”, opened...  \n",
       "3      [Death, may, be, the, great, equalizer, ,, but...  \n",
       "4      [SEOUL, ,, South, Korea, —, North, Korea, ’, s...  \n",
       "...                                                  ...  \n",
       "47220  [BT, is, introducing, two, initiatives, to, he...  \n",
       "47221  [Computer, users, across, the, world, continue...  \n",
       "47222  [A, new, European, directive, could, put, soft...  \n",
       "47223  [The, man, making, sure, US, computer, network...  \n",
       "47224  [Online, role, playing, games, are, time-consu...  \n",
       "\n",
       "[47225 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('data/tokenized.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = list(data['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Relevant publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_publications = [\n",
    " 'Breitbart',\n",
    " 'CNN',\n",
    " 'New York Times',\n",
    " 'NPR',\n",
    " 'Fox News',\n",
    " 'Reuters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Guardian',\n",
       " 'New York Times',\n",
       " 'National Review',\n",
       " 'New York Post',\n",
       " 'Fox News',\n",
       " 'NPR',\n",
       " 'BBC_business',\n",
       " 'BBC_entertainment',\n",
       " 'BBC_sport',\n",
       " 'CNN',\n",
       " 'Buzzfeed News',\n",
       " 'Washington Post',\n",
       " 'BBC_tech',\n",
       " 'BBC_politics',\n",
       " 'Atlantic',\n",
       " 'Vox',\n",
       " 'Talking Points Memo',\n",
       " 'Breitbart',\n",
       " 'Reuters',\n",
       " 'Business Insider']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_publications = list(set(data['publication']))\n",
    "all_publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['New York Times', 'Fox News', 'NPR', 'CNN', 'Breitbart', 'Reuters']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take only the contents from publications with >= 3000 samples.\n",
    "publications = [pub for pub in all_publications if len(data[data['publication'] == pub]) >= 3000 and pub in selected_publications]\n",
    "publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = []\n",
    "for pub in publications:\n",
    "    contents.append(np.asarray(data[data['publication'] == pub]['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding with special Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = max([len(seq) for content in contents for seq in content])\n",
    "max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_token = '~?@_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for content in contents:\n",
    "    for seq in content:\n",
    "        seq.extend([end_token] * (max_seq_length - len(seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = max([len(seq) for content in contents for seq in content])\n",
    "max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcusdaly/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:33: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word 'alice' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-7ba9f73338bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m print(\"Cosine similarity between 'alice' \" + \n\u001b[1;32m     32\u001b[0m                \u001b[0;34m\"and 'wonderland' - CBOW : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     word2vec.similarity('alice', 'wonderland')) \n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m print(\"Cosine similarity between 'alice' \" +\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m                 )\n\u001b[0;32m-> 1447\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1449\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36msimilarity\u001b[0;34m(self, w1, w2)\u001b[0m\n\u001b[1;32m   1450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m         \"\"\"\n\u001b[0;32m-> 1452\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1454\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method will be removed in 4.0.0, use self.wv.n_similarity() instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36msimilarity\u001b[0;34m(self, w1, w2)\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m         \"\"\"\n\u001b[0;32m--> 974\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mn_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'alice' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "\n",
    "word2vec = gensim.models.Word2Vec(all_sentences, min_count = 1,  \n",
    "                              size = 100, window = 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'congress' and 'senate' - CBOW :  0.6639341\n",
      "Cosine similarity between 'congress' and 'house' - CBOW :  0.32222375\n"
     ]
    }
   ],
   "source": [
    "print(\"Cosine similarity between 'congress' \" + \n",
    "               \"and 'senate' - CBOW : \", \n",
    "    word2vec.wv.similarity('congress', 'senate')) \n",
    "      \n",
    "print(\"Cosine similarity between 'congress' \" +\n",
    "                 \"and 'house' - CBOW : \", \n",
    "    word2vec.wv.similarity('congress', 'house')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.88724875,  2.0721283 ,  1.9907529 , -0.33965224,  1.550582  ,\n",
       "       -0.91516715, -0.16540274, -0.14593564,  0.67978156, -1.0644202 ,\n",
       "       -0.6942183 , -1.9598311 , -2.5440912 ,  0.12225136,  1.7198706 ,\n",
       "       -1.6258346 , -1.9540172 ,  1.0887271 ,  0.01184045, -1.0149872 ,\n",
       "        1.0683494 , -2.5246124 , -0.90472686, -0.17703725,  0.6866368 ,\n",
       "        0.00935501, -0.5150477 ,  0.3313275 , -0.5463659 ,  1.2370754 ,\n",
       "        2.4271355 , -0.8467633 ,  1.8974649 ,  0.45820168, -0.14888582,\n",
       "       -0.37252513, -0.22020008,  0.37884572, -1.1070328 , -2.112793  ,\n",
       "       -0.7216101 , -0.6546414 ,  2.1453204 , -0.28742695,  0.4864001 ,\n",
       "       -0.63576955, -0.5250879 ,  0.6972952 ,  0.7093611 , -0.8911131 ,\n",
       "       -0.8603011 ,  0.65346074,  0.01996263, -1.1019238 , -0.14523515,\n",
       "        1.8654116 ,  1.2572564 , -0.60106176,  0.8332406 , -0.07214488,\n",
       "        0.441226  , -0.96152943,  1.7457057 , -0.9876988 ,  0.30688086,\n",
       "        0.26345903, -0.65250355,  0.56314176, -1.1433494 , -1.4191875 ,\n",
       "        0.4386535 , -0.1479898 , -0.53408605,  0.5628094 , -1.1924504 ,\n",
       "       -0.9634857 ,  0.5213135 , -1.3021666 ,  1.5808469 , -0.7836676 ,\n",
       "       -1.0451542 , -1.3739574 , -1.45878   ,  1.0441276 , -0.9688255 ,\n",
       "        1.2545426 , -0.99958116, -0.89101315, -1.3165215 ,  0.47103944,\n",
       "       -0.08539072,  1.4524695 ,  1.0267353 , -1.2247316 , -0.23099366,\n",
       "       -1.1827637 ,  0.7591937 , -0.8611215 , -4.3418136 , -3.1022315 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv['Congressional']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 3000)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(contents).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO use closest cosine distance to find output word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciton Definitions"
=======
    "import rouge\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
>>>>>>> b25830477dcee83188689bbfbb8f37001e59ee05
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squareError(xTrue, xPred):\n",
    "    return K.square(xTrue - xPred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstructionLoss(sample, encoder, decoder, f_w, weight): # (L_1 from the paper)\n",
    "    return K.mean(squareError(sample, decoder(*encoder(sample))) + \n",
    "                  weight*K.log(f_w(*encoder(sample))), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divergenceLoss(f_w, encoder, sample, z_j, n_j): # Mean of log f_w(E_theta_i(x_j)) + log (1-f_w(z_j, n_j)) from the paper (L_2).\n",
    "    return K.mean(K.log(f_w(*encoder(sample))) + \n",
    "                  K.log(1 - f_w(z_j, n_j)), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently just doing a restriction to the last z variables, might want to do a matrix multiplication?\n",
    "# pi_Z from the paper. projects a latent distribution in (z, n) to z\n",
    "def projectZ(encoded):\n",
    "    return encoded[0] # take zs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projectN(encoded):\n",
    "    return encoded[1] # taek Ns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in two inputs, n and z, and outputs samples.\n",
    "def createDecoder(z_dims, n_dims, time_steps, output_dims):\n",
    "    # TODO MAYBE: Add in more regularization or different than dropout?\n",
    "\n",
    "    z_inputs = Input(shape=(z_dims))\n",
    "    n_inputs = Input(shape=(n_dims))\n",
    "    inputs = concatenate([z_inputs, n_inputs])\n",
    "    # 150 is arbitrary rn...\n",
    "    dense = Dense(150)(inputs)\n",
    "    # TODO Reshape to enforce time_steps?\n",
    "    bilstm = Bidirectional(LSTM(32, activation='tanh', return_sequences=True))(dense)\n",
    "    bilstm = Dropout(0.2)(bilstm)\n",
    "    outputs = Bidirectional(LSTM(32, activation='tanh', dropout=0.2, return_sequences=True))(inputs)\n",
    "    \n",
    "    model = Model(inputs=[z_inputs, n_inputs], outputs=outputs)\n",
    "    \n",
    "    model.compile()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEncoder(time_steps, input_num, z_dims, n_dims):\n",
    "    # TODO MAYBE: Add in more regularization or different than dropout?\n",
    "    inputs = Input(shape=(time_steps, input_num))\n",
    "    bilstm = Bidirectional(LSTM(32, activation='tanh', return_sequences=True))(bilstm)\n",
    "    bilstm = Dropout(0.2)(bilstm)\n",
    "    dense = Bidirectional(LSTM(32, activation='tanh', dropout=0.2, return_sequences=False))(bilstm)\n",
    "    dense = Dropout(0.5)(dense)\n",
    "    z_output = Dense(z_dims)(dense)\n",
    "    n_output = Dense(n_dims)(dense)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=[z_output, n_output])\n",
    "    \n",
    "    model.compile()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDiscriminator(z_dims, n_dims):\n",
    "    z_inputs = Input(shape=(z_dims))\n",
    "    n_inputs = Input(shape=(n_dims))\n",
    "    inputs = concatenate([z_inputs, n_inputs])\n",
    "    \n",
    "    # 150, 100 is arbitrary rn...\n",
    "    dense = Dense(150)(inputs)\n",
    "    dense = Dense(100)(dense)\n",
    "    output = Dense(1)\n",
    "    \n",
    "    model = Model(inputs=[z_inputs, n_inputs], outputs=output)\n",
    "    \n",
    "    model.compile()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_optimizer = tf.keras.optimizers.Adam(5e-4)\n",
    "dec_optimizer = tf.keras.optimizers.Adam(5e-4)\n",
    "disc_optimizer = tf.keras.optimizers.Adam(5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleZFrom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When $P_Z$ is known... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# k is num of domains.\n",
    "# encoders is a list of encoders.\n",
    "# decoders is list of decoders.\n",
    "# samples is a K x N x Timesteps x dim, array of samples, where the 0th index is the domain,\n",
    "# the 1th index is the # of the sample in that domain, 2th index is the # timesteps per sequence, 3th index is the #\n",
    "# of dimensions at each timestep\n",
    "# original_domains is a list of the original domains P_z was derived from.\n",
    "\n",
    "# Currently assuming P_Z is known. Must approximate P_Z first.\n",
    "def trainAutoencodersWithPz(samples, encoders, decoders, discriminator, num_samples, original_domains, weight=1.0):\n",
    "    N = samples.shape[1]\n",
    "    k = samples.shape[0]\n",
    "    \n",
    "    for i in range(k):\n",
    "        if i not in original_domains:\n",
    "            original_domain = np.random.choice(original_domains)\n",
    "            encoder = encoders[i]\n",
    "            decoder = decoders[i]\n",
    "            original_encoder = encoders[original_domain]\n",
    "            while(not isConverged(encoder, decoder)):\n",
    "                p_Xi_samples = samples[i, np.random.choice(N, num_samples, replace=False),:,:]\n",
    "                p_Z_samples = projectZ(original_encoder(samples[original_domain, np.random.choice(N, num_samples, replace=False),:,:]))\n",
    "                p_Ni_samples = projectN(encoder(samples[i, np.random.choice(N, num_samples, replace=False),:,:]))\n",
    "\n",
    "                with tf.GradientTape() as enc_tape, tf.GradientTape() as dec_tape, tf.GradientTape() as disc_tape:\n",
    "\n",
    "                    reconstruction_loss = reconstructionLoss(p_Xi_samples, encoder, decoder, f_w, weight)\n",
    "\n",
    "                    # negative b/c gradient ascent.\n",
    "                    divergence_loss = -divergenceLoss(f_w, encoder, p_Xi_samples, p_Z_samples, p_Ni_samples)\n",
    "\n",
    "                gradients_of_encoder = enc_tape.gradient(reconstruction_loss, encoder.trainable_variables)\n",
    "                gradients_of_decoder = dec_tape.gradient(reconstruction_loss, decoder.trainable_variables)\n",
    "                gradients_of_discriminator = disc_tape.gradient(divergence_loss, discriminator.trainable_variables)\n",
    "\n",
    "\n",
    "                enc_optimizer.apply_gradients(zip(gradients_of_encoder, encoder.trainable_variables))\n",
    "                dec_optimizer.apply_gradients(zip(gradients_of_decoder, decoder.trainable_variables))\n",
    "                disc_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When $P_Z$ is unknown...\n",
    "\"A straight-forward approach for learning the latent distribution PZ is to train a regularized autoencoder on data from a\n",
    "single representative domain. However, such a representation could potentially capture variability that is specific to\n",
    "that one domain. To learn a more invariant latent representation, we propose the following extension of our autoencoder\n",
    "framework. The basic idea is to alternate between training\n",
    "multiple autoencoders until they agree on a latent representation that is effective for their respective domains. This is\n",
    "particularly relevant for applications to biology; for example, often one is interested in learning a latent representation\n",
    "that integrates all of the data modalities.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# k is num of domains.\n",
    "# encoders is a list of encoders.\n",
    "# decoders is list of decoders.\n",
    "# samples is a K x N x Timesteps x dim, array of samples, where the 0th index is the domain,\n",
    "# the 1th index is the # of the sample in that domain, 2th index is the # timesteps per sequence, 3th index is the #\n",
    "# of dimensions at each timestep\n",
    "# domains is a list of the domains we are currently training over.\n",
    "\n",
    "def trainAutoencodersInitial(samples, encoders, decoders, discriminator, num_samples, domains, weight=1.0):\n",
    "    N = samples.shape[1]\n",
    "    k = samples.shape[0]\n",
    "    \n",
    "    for i in domains:\n",
    "        encoder = encoders[i]\n",
    "        decoder = decoders[i]\n",
    "        for j in domains:\n",
    "            if i != j:\n",
    "                j_encoder = encoders[j]\n",
    "                while(not isConverged(encoder, decoder)):\n",
    "                    p_Xi_samples = samples[i, np.random.choice(N, num_samples, replace=False),:,:]\n",
    "                    p_Zj_samples = projectZ(j_encoder(samples[j, np.random.choice(N, num_samples, replace=False),:,:]))\n",
    "                    p_Ni_samples = projectN(encoder(samples[i, np.random.choice(N, num_samples, replace=False),:,:]))\n",
    "\n",
    "                    with tf.GradientTape() as enc_tape, tf.GradientTape() as dec_tape, tf.GradientTape() as disc_tape:\n",
    "\n",
    "                        reconstruction_loss = reconstructionLoss(p_Xi_samples, encoder, decoder, f_w, weight)\n",
    "\n",
    "                        # negative b/c gradient ascent.\n",
    "                        divergence_loss = -divergenceLoss(f_w, encoder, p_Xi_samples, p_Zj_samples, p_Ni_samples)\n",
    "\n",
    "                    gradients_of_encoder = enc_tape.gradient(reconstruction_loss, encoder.trainable_variables)\n",
    "                    gradients_of_decoder = dec_tape.gradient(reconstruction_loss, decoder.trainable_variables)\n",
    "                    gradients_of_discriminator = disc_tape.gradient(divergence_loss, discriminator.trainable_variables)\n",
    "\n",
    "\n",
    "                    enc_optimizer.apply_gradients(zip(gradients_of_encoder, encoder.trainable_variables))\n",
    "                    dec_optimizer.apply_gradients(zip(gradients_of_decoder, decoder.trainable_variables))\n",
    "                    disc_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples is a K x N x Timesteps x dim, array of samples, where the 0th index is the domain,\n",
    "# the 1th index is the # of the sample in that domain, 2th index is the # timesteps per sequence, 3th index is the #\n",
    "# of dimensions at each timestep\n",
    "\n",
    "def initModel(samples, z_dims, n_dims):\n",
    "    \n",
    "    k = samples.shape[0]\n",
    "    N = samples.shape[1]\n",
    "    time_steps = samples.shape[2]\n",
    "    dim = samples.shape[3]\n",
    "    \n",
    "    \n",
    "    \n",
    "    discriminator = createDiscriminator(z_dims, n_dims)\n",
    "    \n",
    "    encoders = []\n",
    "    decoders = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        encoders.append(createEncoder(dim, z_dims, n_dims))\n",
    "        decoders.append(createDecoder(z_dims, n_dims, time_steps, dim))\n",
    "    \n",
    "    return encoders, decoders, discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(start_sequences, samples, encoders, decoders, start_domain, end_domain):\n",
    "    N = samples.shape[1]\n",
    "    num_samples = start_sequences.shape[0]\n",
    "    \n",
    "    start_encoder = encoders[start_domain]\n",
    "    end_encoder = encoders[end_domain]\n",
    "    end_decoder = decoders[end_domain]\n",
    "    \n",
    "    end_sequences = end_decoder(projectZ(start_encoder(samples)), projectN(end_encoder(samples[end_domain, np.random.choice(N, num_samples, replace=False),:,:])))\n",
    "    return end_sequences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dims = 50 # len(n)\n",
    "z_dims = 50 # len(Z)\n",
    "\n",
    "num_epochs = 1\n",
    "num_samples = 100\n",
    "\n",
    "original_domains = [0, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders, decoders, discriminator = initModel(samples, z_dims, n_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(start_sequences, samples, encoders, decoders, original_domains[0], original_domains[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    trainAutoencodersInitial(samples, encoders, decoders, discriminator, num_samples, original_domains, weight=1.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "translate(start_sequences, samples, encoders, decoders, original_domains[0], original_domains[1])"
=======
    "detok = TreebankWordDetokenizer()\n",
    "\n",
    "evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
    "                        max_n=4,\n",
    "                        limit_length=True,\n",
    "                        length_limit=100,\n",
    "                        length_limit_type='words',\n",
    "                        apply_avg=False,\n",
    "                        apply_best=True,\n",
    "                        alpha=0.5, # Default F1_score\n",
    "                        weight_factor=1.2,\n",
    "                        stemming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateOnArticles(articles, encoder, decoder):\n",
    "    translated = decoder(encoder(articles))\n",
    "       \n",
    "    original_sentences = [detok.detokenize(tokens) for tokens in articles]\n",
    "    \n",
    "    translated_sentences = [vecSeqToSentence(tokens) for tokens in translated]\n",
    "    \n",
    "    scores = evaluator.get_scores(translated_sentences, original_sentences)\n",
    "    \n",
    "    for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
    "        print('\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * results['p'], 'R', 100.0 * results['r'], 'F1', 100.0 * results['f']))"
>>>>>>> b25830477dcee83188689bbfbb8f37001e59ee05
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "\n",
    "for i in range(num_epochs):\n",
    "    trainAutoencodersWithPz(samples, encoders, decoders, discriminator, num_samples, original_domains, weight=1.0)"
=======
    "def evaluate(articles_df, encoders, decoders):\n",
    "    \n",
    "    publications = articles_df.publication.unique()\n",
    "    for i in range(len(publications)):\n",
    "        for j in range(len(publications)):\n",
    "            if (i != j):\n",
    "                pub1=publications[i]\n",
    "                pub2=publications[j]\n",
    "                source_articles = articles_df.loc[articles_df['publication'] == pub1]['content'].tolist()\n",
    "\n",
    "                print(pub1,\"to\",pub2)\n",
    "                evaluateOnArticles(source_articles, encoders[i], decoders[j])\n",
    "                print()"
>>>>>>> b25830477dcee83188689bbfbb8f37001e59ee05
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
