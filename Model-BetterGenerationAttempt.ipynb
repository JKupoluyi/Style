{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import pandas\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "import rouge\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LSTM, Activation, Bidirectional, Dropout, Input, concatenate, Reshape, TimeDistributed, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow.keras.backend as K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>publication</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[washington, —, congressional, republicans, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[after, the, bullet, shells, get, the, south, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[when, walt, disney, ’, s, but, what, they, di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[death, may, be, the, great, equalizer, ,, but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>[seoul, ,, south, korea, —, although, north, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47220</th>\n",
       "      <td>47220</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[bt, is, introducing, two, initiatives, from, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47221</th>\n",
       "      <td>47221</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[computer, users, across, the, world, more, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47222</th>\n",
       "      <td>47222</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[a, new, european, directive, could, if, it, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47223</th>\n",
       "      <td>47223</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[the, man, making, sure, us, amit, yoran, was,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47224</th>\n",
       "      <td>47224</td>\n",
       "      <td>BBC_tech</td>\n",
       "      <td>[online, role, playing, games, are, time-consu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47225 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0     publication  \\\n",
       "0               0  New York Times   \n",
       "1               1  New York Times   \n",
       "2               2  New York Times   \n",
       "3               3  New York Times   \n",
       "4               4  New York Times   \n",
       "...           ...             ...   \n",
       "47220       47220        BBC_tech   \n",
       "47221       47221        BBC_tech   \n",
       "47222       47222        BBC_tech   \n",
       "47223       47223        BBC_tech   \n",
       "47224       47224        BBC_tech   \n",
       "\n",
       "                                                 content  \n",
       "0      [washington, —, congressional, republicans, ha...  \n",
       "1      [after, the, bullet, shells, get, the, south, ...  \n",
       "2      [when, walt, disney, ’, s, but, what, they, di...  \n",
       "3      [death, may, be, the, great, equalizer, ,, but...  \n",
       "4      [seoul, ,, south, korea, —, although, north, k...  \n",
       "...                                                  ...  \n",
       "47220  [bt, is, introducing, two, initiatives, from, ...  \n",
       "47221  [computer, users, across, the, world, more, th...  \n",
       "47222  [a, new, european, directive, could, if, it, g...  \n",
       "47223  [the, man, making, sure, us, amit, yoran, was,...  \n",
       "47224  [online, role, playing, games, are, time-consu...  \n",
       "\n",
       "[47225 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('data/tokenized.pkl')\n",
    "eval_df = pd.read_pickle('data/evaluation.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = list(data['content'])\n",
    "all_sentences.extend(list(eval_df['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Relevant publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_publications = [\n",
    "#  'Breitbart',\n",
    "#  'CNN',\n",
    "#  'New York Times',\n",
    "#  'NPR',\n",
    "#  'Fox News',\n",
    "#  'Reuters']\n",
    "selected_publications = [\n",
    " 'Breitbart',\n",
    " 'CNN',\n",
    " 'New York Times']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BBC_business',\n",
       " 'Atlantic',\n",
       " 'BBC_entertainment',\n",
       " 'Buzzfeed News',\n",
       " 'BBC_sport',\n",
       " 'Guardian',\n",
       " 'BBC_tech',\n",
       " 'Breitbart',\n",
       " 'Business Insider',\n",
       " 'National Review',\n",
       " 'New York Post',\n",
       " 'Reuters',\n",
       " 'Fox News',\n",
       " 'New York Times',\n",
       " 'Vox',\n",
       " 'Washington Post',\n",
       " 'Talking Points Memo',\n",
       " 'NPR',\n",
       " 'CNN',\n",
       " 'BBC_politics']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_publications = list(set(data['publication']))\n",
    "all_publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Breitbart', 'CNN', 'New York Times']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take only the contents from publications with >= 3000 samples.\n",
    "publications = [pub for pub in selected_publications if pub in all_publications and len(data[data['publication'] == pub]) >= 3000]\n",
    "publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = []\n",
    "for pub in publications:\n",
    "    contents.append(np.asarray(data[data['publication'] == pub]['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding with special Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = max([len(seq) for content in contents for seq in content])\n",
    "max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_token = '~?@_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for content in contents:\n",
    "    for seq in content:\n",
    "        seq.extend([end_token] * (max_seq_length - len(seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = max([len(seq) for content in contents for seq in content])\n",
    "max_seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word2vec = gensim.models.Word2Vec(all_sentences, min_count = 1,  \n",
    "                              size = word_dim, window = 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'congress' and 'senate' - CBOW :  0.7506351\n",
      "Cosine similarity between 'congress' and 'house' - CBOW :  0.62278795\n"
     ]
    }
   ],
   "source": [
    "print(\"Cosine similarity between 'congress' \" + \n",
    "               \"and 'senate' - CBOW : \", \n",
    "    word2vec.wv.similarity('congress', 'senate')) \n",
    "      \n",
    "print(\"Cosine similarity between 'congress' \" +\n",
    "                 \"and 'house' - CBOW : \", \n",
    "    word2vec.wv.similarity('congress', 'house')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.2737055 , -0.0815997 , -0.40515476,  0.10907958, -0.10488369,\n",
       "       -0.46514127,  0.3546361 , -0.49898246, -0.13679565, -0.95021194,\n",
       "        0.11053801,  0.28043687,  0.39044622,  0.18986215,  0.31135228,\n",
       "        0.65644604, -0.53040445,  0.09304625, -0.33855414,  1.5273463 ,\n",
       "       -1.550524  ,  0.8062989 ,  0.27642545, -0.655483  ,  0.59252894,\n",
       "       -0.29950106, -0.10941707, -1.2045021 , -0.7059632 ,  0.05883586,\n",
       "        0.27209347, -0.7489524 , -0.28490993, -0.5156418 ,  0.11626618,\n",
       "       -0.3572677 ,  0.03211268,  0.07818181, -0.51545733, -0.13020003,\n",
       "        0.1085882 , -1.5130123 ,  0.297752  , -0.5476296 ,  0.37076122,\n",
       "        0.8633128 , -0.20785286, -0.68780774,  0.73181003, -0.8301219 ,\n",
       "        0.05571607, -0.42777744, -0.12656578,  0.7455623 ,  0.36209732,\n",
       "       -0.32334244, -0.31440073,  0.4286173 , -1.4040293 ,  0.34348148,\n",
       "        0.05970663, -0.36402565, -0.15024659,  0.07058263,  0.5764989 ,\n",
       "        0.985047  ,  0.57990754,  1.5233004 ,  1.2298111 , -0.22801913,\n",
       "       -0.4920899 , -0.24267499,  0.58051795,  0.69041365, -0.21854842,\n",
       "        0.02109715,  0.7593805 ,  0.29873502, -0.08461803,  1.1381595 ,\n",
       "        0.3673395 ,  0.36424753,  0.57372564,  0.15777108,  0.20087793,\n",
       "        0.41798577,  0.66633093, -0.67897594, -0.91672003, -0.8039592 ,\n",
       "        0.16521865, -0.45108184,  1.1792547 , -0.18628389,  0.49541008,\n",
       "       -0.6159321 ,  0.3487106 ,  0.06702092, -0.42541516,  0.29356983],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv['congressional']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('congressional', 1.0000001192092896),\n",
       " ('senate', 0.8786221146583557),\n",
       " ('judiciary', 0.8456677198410034),\n",
       " ('committee', 0.8444260954856873),\n",
       " ('conservative', 0.8296844959259033),\n",
       " ('gop', 0.821047842502594),\n",
       " ('liberal', 0.8202149868011475),\n",
       " ('lawmakers', 0.8164830803871155),\n",
       " ('272-219', 0.8096722364425659),\n",
       " ('governors', 0.8080213665962219)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.similar_by_vector(word2vec.wv['congressional'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = np.asarray(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.zeros(shape=(contents.shape[0], contents.shape[1], max_seq_length, word_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(contents.shape[0]):\n",
    "    for j in range(contents.shape[1]):\n",
    "        for k in range(max_seq_length):\n",
    "            samples[i, j, k, :] = word2vec.wv[contents[i, j][k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO use closest cosine distance to find output word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciton Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squareError(xTrue, xPred):\n",
    "    return K.square(xTrue - xPred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstructionLoss(sample, encoder, decoder, f_w, weight): # (L_1 from the paper)\n",
    "    return K.mean(squareError(sample, decode_sequence(encoder(sample), decoder, sample.shape[1], sample.shape[2]))) + K.mean(weight*K.log(f_w(encoder(sample))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divergenceLoss(f_w, encoder, sample, z_j, n_j): # Mean of log f_w(E_theta_i(x_j)) + log (1-f_w(z_j, n_j)) from the paper (L_2).\n",
    "    return K.mean(K.log(f_w(encoder(sample)))) + K.mean(K.log(1 - f_w(z_j + n_j)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(data, domain, num_samples):\n",
    "    N = data.shape[1]\n",
    "    return tf.convert_to_tensor(data[domain, np.random.choice(N, num_samples, replace=True),:,:], dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently just doing a restriction to the last z variables, might want to do a matrix multiplication?\n",
    "# pi_Z from the paper. projects a latent distribution in (z, n) to z\n",
    "def projectZ(encoded):\n",
    "    return encoded[0:2] # take zs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projectN(encoded):\n",
    "    return encoded[2:4] # take ns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # takes in two inputs, n and z, and outputs samples.\n",
    "# def createDecoder(z_dims, n_dims, time_steps, output_dims):\n",
    "#     # TODO MAYBE: Add in more regularization or different than dropout?\n",
    "\n",
    "#     z_inputs = Input(shape=(z_dims,))\n",
    "#     n_inputs = Input(shape=(n_dims,))\n",
    "#     inputs = concatenate([z_inputs, n_inputs])\n",
    "# #     # 150 is arbitrary rn...\n",
    "# #     dense = Dense(150)(inputs)\n",
    "#     dense = Dense(time_steps*output_dims)(inputs)\n",
    "#     reshape = Reshape((time_steps, output_dims))(dense)\n",
    "#     # TODO Reshape to enforce time_steps?\n",
    "#     bilstm = Bidirectional(LSTM(64, activation='tanh', return_sequences=True))(reshape)\n",
    "#     bilstm = Dropout(0.2)(bilstm)\n",
    "#     bilstm = Bidirectional(LSTM(64, activation='tanh', return_sequences=False))(bilstm)\n",
    "#     bilstm = Dropout(0.2)(bilstm)\n",
    "    \n",
    "#     dense = Dense(time_steps*output_dims, activation='linear')(bilstm)\n",
    "#     outputs = Reshape((time_steps, output_dims))(dense)\n",
    "    \n",
    "#     model = Model(inputs=[z_inputs, n_inputs], outputs=outputs)\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEncoder(time_steps, input_num, z_dims, n_dims):\n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = Input(shape=(time_steps, input_num))\n",
    "    z_encoder = LSTM(z_dims, return_state=True)\n",
    "    n_encoder = LSTM(n_dims, return_state=True)\n",
    "    \n",
    "    z_encoder_outputs, z_state_h, z_state_c = z_encoder(encoder_inputs)\n",
    "    n_encoder_outputs, n_state_h, n_state_c = n_encoder(encoder_inputs)\n",
    "    \n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    z_encoder_states = [z_state_h, z_state_c]\n",
    "    n_encoder_states = [n_state_h, n_state_c]\n",
    "    \n",
    "    model = Model(inputs=encoder_inputs, outputs=z_encoder_states + n_encoder_states)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDecoder(z_dims, n_dims, time_steps, output_dims):\n",
    "    # Set up the decoder, using `encoder_states` as initial state.\n",
    "    decoder_inputs = Input(shape=(1, output_dims,))\n",
    "    \n",
    "    z_state_h = Input(shape=(z_dims,))\n",
    "    z_state_c = Input(shape=(z_dims,))\n",
    "    n_state_h = Input(shape=(n_dims,))\n",
    "    n_state_c = Input(shape=(n_dims,))\n",
    "    \n",
    "    z_encoder_states = [z_state_h, z_state_c]\n",
    "    n_encoder_states = [n_state_h, n_state_c]\n",
    "    \n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the \n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    z_decoder_lstm = LSTM(z_dims, return_sequences=True, return_state=True)\n",
    "    n_decoder_lstm = LSTM(n_dims, return_sequences=True, return_state=True)\n",
    "    z_decoder_outputs, z_state_h, z_state_c = z_decoder_lstm(decoder_inputs,\n",
    "                                         initial_state=z_encoder_states)\n",
    "    n_decoder_outputs, n_state_h, n_state_c = n_decoder_lstm(decoder_inputs,\n",
    "                                         initial_state=n_encoder_states)\n",
    "    \n",
    "    decoder_outputs = concatenate([z_decoder_outputs, n_decoder_outputs])\n",
    "    \n",
    "    z_decoder_states = [z_state_h, z_state_c]\n",
    "    n_decoder_states = [n_state_h, n_state_c]\n",
    "    \n",
    "    decoder_dense = Dense(output_dims, activation='linear')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    model = Model(inputs=[decoder_inputs] + z_encoder_states + n_encoder_states,\n",
    "                  outputs=[decoder_outputs] + z_decoder_states + n_decoder_states)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEncDecModel(encoder, decoder, time_steps, dims):\n",
    "    encoder_inputs = Input(shape=(time_steps, dims))\n",
    "    decoder_inputs = Input(shape=(1, dims,))\n",
    "    \n",
    "    enc = encoder(encoder_inputs)\n",
    "    dec = createDecoder(decoder_inputs + enc)\n",
    "    model = Model(inputs = [encoder_inputs, decoder_inputs], outputs = dec)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(encoder_outputs, decoder, time_steps, dims):\n",
    "    batch_size = encoder_outputs[0].shape[0]\n",
    "    \n",
    "    states_value = encoder_outputs\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((batch_size, 1, dims))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "#     target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = np.zeros((batch_size, time_steps, dims))\n",
    "    \n",
    "    index = 0\n",
    "    while index < time_steps:\n",
    "        output_vec, z_state_h, z_state_c, n_state_h, n_state_c = decoder.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        decoded_sentence[:, index, :] = output_vec.reshape((batch_size, dims))\n",
    "\n",
    "        \n",
    "            \n",
    "            \n",
    "        target_seq = np.zeros((batch_size, 1, dims))\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            # Update the target sequence (of length 1).\n",
    "            word = word2vec.wv.similar_by_vector(output_vec[i, 0, :])[0][0]\n",
    "\n",
    "            target_seq[i, 0, :] = word2vec.wv[word]\n",
    "\n",
    "        # Update states\n",
    "        states_value = [z_state_h, z_state_c, n_state_h, n_state_c]\n",
    "        index += 1\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def createEncoder(time_steps, input_num, z_dims, n_dims):\n",
    "#     # TODO MAYBE: Add in more regularization or different than dropout?\n",
    "#     inputs = Input(shape=(time_steps, input_num,))\n",
    "#     bilstm = Bidirectional(LSTM(64, activation='tanh', return_sequences=True))(inputs)\n",
    "#     bilstm = Dropout(0.2)(bilstm)\n",
    "#     dense = Bidirectional(LSTM(64, activation='tanh', return_sequences=False))(bilstm)\n",
    "#     dense = Dropout(0.2)(dense)\n",
    "#     z_output = Dense(z_dims, activation='linear')(dense)\n",
    "#     n_output = Dense(n_dims, activation='linear')(dense)\n",
    "    \n",
    "#     model = Model(inputs=inputs, outputs=[z_output, n_output])\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDiscriminator(z_dims, n_dims):\n",
    "    \n",
    "    z_inputs_h = Input(shape=(z_dims,))\n",
    "    z_inputs_c = Input(shape=(z_dims,))\n",
    "    n_inputs_h = Input(shape=(n_dims,))\n",
    "    n_inputs_c = Input(shape=(n_dims,))\n",
    "    \n",
    "    inputs = concatenate([z_inputs_h, z_inputs_c, n_inputs_h, n_inputs_c])\n",
    "    \n",
    "    # 150, 100 is arbitrary rn...\n",
    "    dense = Dense(150, activation='relu')(inputs)\n",
    "    dense = Dense(100, activation='relu')(dense)\n",
    "    output = Dense(1, activation='sigmoid')(dense)\n",
    "    \n",
    "    model = Model(inputs=[z_inputs_h, z_inputs_c, n_inputs_h, n_inputs_c], outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-4\n",
    "enc_optimizer = tf.keras.optimizers.Adam(lr)\n",
    "dec_optimizer = tf.keras.optimizers.Adam(lr)\n",
    "disc_optimizer = tf.keras.optimizers.Adam(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When $P_Z$ is known... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# k is num of domains.\n",
    "# encoders is a list of encoders.\n",
    "# decoders is list of decoders.\n",
    "# samples is a K x N x Timesteps x dim, array of samples, where the 0th index is the domain,\n",
    "# the 1th index is the # of the sample in that domain, 2th index is the # timesteps per sequence, 3th index is the #\n",
    "# of dimensions at each timestep\n",
    "# original_domains is a list of the original domains P_z was derived from.\n",
    "\n",
    "# Currently assuming P_Z is known. Must approximate P_Z first.\n",
    "def trainAutoencodersWithPz(samples, encoders, decoders, discriminator, num_samples, original_domains, epochs=10, weight=1.0):\n",
    "    N = samples.shape[1]\n",
    "    k = samples.shape[0]\n",
    "        \n",
    "    \n",
    "    for i in range(k):\n",
    "        if i not in original_domains:\n",
    "            original_domain = np.random.choice(original_domains)\n",
    "            encoder = encoders[i]\n",
    "            decoder = decoders[i]\n",
    "            original_encoder = encoders[original_domain]\n",
    "            epoch = 0\n",
    "            while(epoch < epochs): # TOOD: could also do until some convergence criteria.\n",
    "                p_Xi_samples = sample(samples, i, num_samples)\n",
    "                p_Z_samples = projectZ(original_encoder(sample(samples, original_domain, num_samples)))\n",
    "                p_Ni_samples = projectN(encoder(sample(samples, i, num_samples)))\n",
    "\n",
    "                with tf.GradientTape() as enc_tape, tf.GradientTape() as dec_tape, tf.GradientTape() as disc_tape:\n",
    "\n",
    "                    reconstruction_loss = reconstructionLoss(p_Xi_samples, encoder, decoder, discriminator, weight)\n",
    "\n",
    "                    # negative b/c gradient ascent.\n",
    "                    divergence_loss = -1 * divergenceLoss(discriminator, encoder, p_Xi_samples, p_Z_samples, p_Ni_samples)\n",
    "\n",
    "                gradients_of_encoder = enc_tape.gradient(reconstruction_loss, encoder.trainable_variables)\n",
    "                gradients_of_decoder = dec_tape.gradient(reconstruction_loss, decoder.trainable_variables)\n",
    "                gradients_of_discriminator = disc_tape.gradient(divergence_loss, discriminator.trainable_variables)\n",
    "\n",
    "\n",
    "                enc_optimizer.apply_gradients(zip(gradients_of_encoder, encoder.trainable_variables))\n",
    "                dec_optimizer.apply_gradients(zip(gradients_of_decoder, decoder.trainable_variables))\n",
    "                disc_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "                \n",
    "                print('Domain {}, Epoch {}:\\n\\tReconstruction Loss: {}\\n\\tDivergence Loss: {}'.format(i, epoch+1, reconstruction_loss, divergence_loss))\n",
    "                epoch+=1\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When $P_Z$ is unknown...\n",
    "\"A straight-forward approach for learning the latent distribution PZ is to train a regularized autoencoder on data from a\n",
    "single representative domain. However, such a representation could potentially capture variability that is specific to\n",
    "that one domain. To learn a more invariant latent representation, we propose the following extension of our autoencoder\n",
    "framework. The basic idea is to alternate between training\n",
    "multiple autoencoders until they agree on a latent representation that is effective for their respective domains. This is\n",
    "particularly relevant for applications to biology; for example, often one is interested in learning a latent representation\n",
    "that integrates all of the data modalities.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# k is num of domains.\n",
    "# encoders is a list of encoders.\n",
    "# decoders is list of decoders.\n",
    "# samples is a K x N x Timesteps x dim, array of samples, where the 0th index is the domain,\n",
    "# the 1th index is the # of the sample in that domain, 2th index is the # timesteps per sequence, 3th index is the #\n",
    "# of dimensions at each timestep\n",
    "# domains is a list of the domains we are currently training over.\n",
    "\n",
    "def trainAutoencodersInitial(samples, encoders, decoders, discriminator, num_samples, domains, epochs=10, weight=1.0):\n",
    "    N = samples.shape[1]\n",
    "    k = samples.shape[0]\n",
    "    \n",
    "    for i in domains:\n",
    "        encoder = encoders[i]\n",
    "        decoder = decoders[i]\n",
    "        for j in domains:\n",
    "            if i != j:\n",
    "                j_encoder = encoders[j]\n",
    "                epoch = 0\n",
    "                while(epoch < epochs): # TOOD: could also do until some convergence criteria.\n",
    "                    p_Xi_samples = sample(samples, i, num_samples)\n",
    "                    p_Zj_samples = projectZ(j_encoder(sample(samples, j, num_samples)))\n",
    "                    p_Ni_samples = projectN(encoder(sample(samples, i, num_samples)))\n",
    "\n",
    "                    with tf.GradientTape() as enc_tape, tf.GradientTape() as dec_tape, tf.GradientTape() as disc_tape:\n",
    "\n",
    "                        reconstruction_loss = reconstructionLoss(p_Xi_samples, encoder, decoder, discriminator, weight)\n",
    "#                         print(p_Xi_samples)\n",
    "\n",
    "                        # negative b/c gradient ascent.\n",
    "                        divergence_loss = -1 * divergenceLoss(discriminator, encoder, p_Xi_samples, p_Zj_samples, p_Ni_samples)\n",
    "#                         print(p_Zj_samples)\n",
    "#                         print(p_Ni_samples)\n",
    "                        \n",
    "                    gradients_of_encoder = enc_tape.gradient(reconstruction_loss, encoder.trainable_variables)\n",
    "                    gradients_of_decoder = dec_tape.gradient(reconstruction_loss, decoder.trainable_variables)\n",
    "                    gradients_of_discriminator = disc_tape.gradient(divergence_loss, discriminator.trainable_variables)\n",
    "            \n",
    "\n",
    "                    enc_optimizer.apply_gradients(zip(gradients_of_encoder, encoder.trainable_variables))\n",
    "                    dec_optimizer.apply_gradients(zip(gradients_of_decoder, decoder.trainable_variables))\n",
    "                    disc_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "                    \n",
    "                    print('Domain {}, Epoch {}:\\n\\tReconstruction Loss: {}\\n\\tDivergence Loss: {}'.format(i, epoch+1, reconstruction_loss, divergence_loss))\n",
    "                    epoch+=1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples is a K x N x Timesteps x dim, array of samples, where the 0th index is the domain,\n",
    "# the 1th index is the # of the sample in that domain, 2th index is the # timesteps per sequence, 3th index is the #\n",
    "# of dimensions at each timestep\n",
    "\n",
    "def initModel(samples, z_dims, n_dims):\n",
    "    \n",
    "    k = samples.shape[0]\n",
    "    N = samples.shape[1]\n",
    "    time_steps = samples.shape[2]\n",
    "    dim = samples.shape[3]\n",
    "    \n",
    "    \n",
    "    \n",
    "    discriminator = createDiscriminator(z_dims, n_dims)\n",
    "    \n",
    "    encoders = []\n",
    "    decoders = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        encoders.append(createEncoder(time_steps, dim, z_dims, n_dims))\n",
    "        decoders.append(createDecoder(z_dims, n_dims, time_steps, dim))\n",
    "    \n",
    "    return encoders, decoders, discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(start_sequences, samples, encoders, decoders, start_domain, end_domain):\n",
    "    time_steps = start_sequences.shape[1]\n",
    "    dims = start_sequences.shape[2]\n",
    "    \n",
    "    N = samples.shape[1]\n",
    "    print(start_sequences.shape)\n",
    "    num_samples = start_sequences.shape[0]\n",
    "    \n",
    "    start_encoder = encoders[start_domain]\n",
    "    end_encoder = encoders[end_domain]\n",
    "    end_decoder = decoders[end_domain]\n",
    "    \n",
    "    z = projectZ(start_encoder(start_sequences))\n",
    "    n = projectN(end_encoder(sample(samples, end_domain, num_samples)))\n",
    "    \n",
    "    end_sequences = decode_sequence(z+n, end_decoder, time_steps, dims)\n",
    "    return end_sequences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vecSeqToSentence(sequence):\n",
    "    sequence = K.eval(sequence)\n",
    "    sentence = []\n",
    "    for i in range(sequence.shape[0]):\n",
    "        word = sequence[i,:]\n",
    "#         print(word)\n",
    "#         print(word2vec.wv.similar_by_vector(word))\n",
    "        sentence.append(word2vec.wv.similar_by_vector(word)[0][0])\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dims = 64 # len(n)\n",
    "z_dims = 64 # len(Z)\n",
    "\n",
    "num_epochs = 2\n",
    "num_samples = 128\n",
    "\n",
    "weight = 1\n",
    "\n",
    "original_domains = [0, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = tf.convert_to_tensor(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders, decoders, discriminator = initModel(samples, z_dims, n_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_52\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_169 (InputLayer)          [(None, 50, 100)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_88 (LSTM)                  [(None, 64), (None,  42240       input_169[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_89 (LSTM)                  [(None, 64), (None,  42240       input_169[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 84,480\n",
      "Trainable params: 84,480\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoders[0].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original First Sentence from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'on tuesday ’ s broadcast ” zeleny said , “ and she ’ s having a difficult time in federal prison , no question . ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_ ~?@_'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(contents[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 100)\n"
     ]
    }
   ],
   "source": [
    "seq = tf.convert_to_tensor(np.asarray([samples[0, 0, :, :]]), dtype=tf.float32)\n",
    "translation = translate(seq, samples, encoders, decoders, original_domains[0], original_domains[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original First Sentence from 0 translated to 1 before Training (Random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['faring', 'faring', 'faring', 'faring', 'faring', 'faring', 'antinuclear', 'antinuclear', 'antinuclear', 'antinuclear', 'jails', 'jails', 'jails', 'jails', 'jails', 'jails', 'specimens', 'specimens', 'buckfield', 'buckfield', 'buckfield', 'cementing', 'cementing', 'cementing', 'bookmark', 'cementing', 'bookmark', 'cementing', 'cementing', 'bookmark', 'cementing', 'cementing', 'cementing', 'ahsha', 'cementing', 'terrorist.', 'spartan', 'annihilation', 'animate', 'hollow', 'animate', 'hollow', 'shipworm', 'gasp', 'canes', 'symptomatic', 'symptomatic', 'symptomatic', 'continual', 'dehumanizing']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vecSeqToSentence(translation[0,:,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: ['lstm_90/kernel:0', 'lstm_90/recurrent_kernel:0', 'lstm_90/bias:0', 'lstm_91/kernel:0', 'lstm_91/recurrent_kernel:0', 'lstm_91/bias:0', 'dense_48/kernel:0', 'dense_48/bias:0'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-171-bd9778458573>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainAutoencodersInitial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_domains\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-160-db1d014bb6e7>\u001b[0m in \u001b[0;36mtrainAutoencodersInitial\u001b[0;34m(samples, encoders, decoders, discriminator, num_samples, domains, epochs, weight)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0menc_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients_of_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                     \u001b[0mdec_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients_of_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                     \u001b[0mdisc_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients_of_discriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[1;32m    425\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mnone\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvariables\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     \"\"\"\n\u001b[0;32m--> 427\u001b[0;31m     \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_filter_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_filter_grads\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m   1023\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m     raise ValueError(\"No gradients provided for any variable: %s.\" %\n\u001b[0;32m-> 1025\u001b[0;31m                      ([v.name for _, v in grads_and_vars],))\n\u001b[0m\u001b[1;32m   1026\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mvars_with_empty_grads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m     logging.warning(\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable: ['lstm_90/kernel:0', 'lstm_90/recurrent_kernel:0', 'lstm_90/bias:0', 'lstm_91/kernel:0', 'lstm_91/recurrent_kernel:0', 'lstm_91/bias:0', 'dense_48/kernel:0', 'dense_48/bias:0']."
     ]
    }
   ],
   "source": [
    "trainAutoencodersInitial(samples, encoders, decoders, discriminator, num_samples, original_domains, epochs=num_epochs, weight=weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original First Sentence from 0 translated to 1 after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = translate(seq, samples, encoders, decoders, original_domains[0], original_domains[1])\n",
    "vecSeqToSentence(translation[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAutoencodersWithPz(samples, encoders, decoders, discriminator, num_samples, original_domains, epochs=num_epochs, weight=weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original First Sentence from 0 translated to 2 after Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = translate(seq, samples, encoders, decoders, 0, 2)\n",
    "vecSeqToSentence(translation[0,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = []\n",
    "for pub in publications:\n",
    "    contents.append(np.asarray(eval_df[eval_df['publication'] == pub]['content']))\n",
    "    \n",
    "for content in contents:\n",
    "    for seq in content:\n",
    "        seq.extend([end_token] * (max_seq_length - len(seq)))\n",
    "    \n",
    "contents = np.asarray(contents)\n",
    "samples = np.zeros(shape=(contents.shape[0], contents.shape[1], max_seq_length, word_dim))\n",
    "\n",
    "for i in range(contents.shape[0]):\n",
    "    for j in range(contents.shape[1]):\n",
    "        for k in range(max_seq_length):\n",
    "            samples[i, j, k, :] = word2vec.wv[contents[i, j][k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detok = TreebankWordDetokenizer()\n",
    "\n",
    "evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
    "                        max_n=4,\n",
    "                        limit_length=True,\n",
    "                        length_limit=100,\n",
    "                        length_limit_type='words',\n",
    "                        apply_avg=False,\n",
    "                        apply_best=True,\n",
    "                        alpha=0.5, # Default F1_score\n",
    "                        weight_factor=1.2,\n",
    "                        stemming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateOnArticles(articles, encoder, decoder):\n",
    "    translated = decode_sequence(encoder(tf.convert_to_tensor(articles, dtype=tf.float32)), decoder, articles.shape[1], articles.shape[2])\n",
    "       \n",
    "    original_sentences = [vecSeqToSentence(tokens) for tokens in articles]\n",
    "    \n",
    "    translated_sentences = [vecSeqToSentence(tokens) for tokens in translated]\n",
    "    \n",
    "    scores = evaluator.get_scores(translated_sentences, original_sentences)\n",
    "    \n",
    "    for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
    "        print('\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * results['p'], 'R', 100.0 * results['r'], 'F1', 100.0 * results['f']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(articles, encoders, decoders):\n",
    "    \n",
    "    for i in range(len(selected_publications)):\n",
    "        for j in range(len(selected_publications)):\n",
    "            if (i != j):\n",
    "                pub1=publications[i]\n",
    "                pub2=publications[j]\n",
    "                #source_articles = articles_df.loc[articles_df['publication'] == pub1]['content'].tolist()\n",
    "                source_articles = articles[i]\n",
    "                \n",
    "                print(pub1,\"to\",pub2)\n",
    "                evaluateOnArticles(source_articles, encoders[i], decoders[j])\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(samples, encoders, decoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a485853c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses1 = [10, 9, 6, 3, 2, 2, 2, 1, 1, 1, 1]\n",
    "losses2 = [11, 10, 7, 6, 5, 5, 3, 3, 2, 1, 1]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(losses1)), losses1, losses2, range(len(losses2)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
